{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching SPY: 48068rows [00:00, 77752.28rows/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing MSFT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching MSFT: 38094rows [00:02, 16268.63rows/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing trading parameters for MSFT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-17 12:59:50,741] A new study created in memory with name: no-name-aed55e7c-c529-48c1-af7a-d7b777d97a3e\n",
      "/opt/conda/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Essential Imports\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import mplfinance as mpf\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "import ta\n",
    "import logging\n",
    "import gc\n",
    "import traceback\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional, Dict, Tuple\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import matplotlib.gridspec as gridspec\n",
    "from ta.volatility import BollingerBands\n",
    "from ta.trend import MACD\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "from scipy.stats import percentileofscore\n",
    "import optuna\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "# Suppress mplfinance warnings for too much data\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"mplfinance\")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    filename='checklstm2_trading_model.log',\n",
    "    filemode='a',\n",
    "    format='%(asctime)s:%(levelname)s:%(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# API Configuration\n",
    "API_KEY = os.getenv(\"POLYGON_API_KEY\")\n",
    "if not API_KEY:\n",
    "    raise ValueError(\"Missing Polygon API KEY. Please check your .env file.\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingHistory:\n",
    "    \"\"\"Tracks training metrics during model training.\"\"\"\n",
    "    loss_history: List[float] = field(default_factory=list)\n",
    "    validation_loss_history: List[float] = field(default_factory=list)\n",
    "    direction_accuracy_history: List[float] = field(default_factory=list)\n",
    "    long_accuracy_history: List[float] = field(default_factory=list)\n",
    "    short_accuracy_history: List[float] = field(default_factory=list)\n",
    "\n",
    "    def update(self, epoch: int, loss: float, val_loss: float, \n",
    "              direction_accuracy: float = None, \n",
    "              long_accuracy: float = None, \n",
    "              short_accuracy: float = None):\n",
    "        \"\"\"Update training history with new metrics.\"\"\"\n",
    "        self.loss_history.append(loss)\n",
    "        self.validation_loss_history.append(val_loss)\n",
    "        if direction_accuracy is not None:\n",
    "            self.direction_accuracy_history.append(direction_accuracy)\n",
    "        if long_accuracy is not None:\n",
    "            self.long_accuracy_history.append(long_accuracy)\n",
    "        if short_accuracy is not None:\n",
    "            self.short_accuracy_history.append(short_accuracy)\n",
    "\n",
    "class ModelEvaluator:\n",
    "    def __init__(self, prediction_threshold: float = 0.001):\n",
    "        self.prediction_threshold = prediction_threshold\n",
    "        self.mse_scores = []\n",
    "        self.mae_scores = []\n",
    "        self.seasonal_errors = []\n",
    "        # Track prediction errors for adaptive correction\n",
    "        self.error_history = defaultdict(list)\n",
    "        self.max_history_size = 100\n",
    "\n",
    "    def calculate_metrics(self, y_true: np.ndarray, y_pred: np.ndarray, \n",
    "                         current_prices: np.ndarray = None,\n",
    "                         ticker: str = None) -> Dict[str, float]:\n",
    "        \"\"\"Calculate various error metrics based on predictions with direction-specific analysis.\"\"\"\n",
    "        metrics = {}\n",
    "        \n",
    "        # Ensure inputs are 1-D and matching length\n",
    "        y_true = y_true.flatten()\n",
    "        y_pred = y_pred.flatten()\n",
    "        \n",
    "        if len(y_true) != len(y_pred):\n",
    "            raise ValueError(\"y_true and y_pred must have the same length\")\n",
    "            \n",
    "        # Handle invalid values\n",
    "        if np.isnan(y_true).any() or np.isnan(y_pred).any():\n",
    "            raise ValueError(\"Inputs contain NaN values\")\n",
    "        if np.isinf(y_true).any() or np.isinf(y_pred).any():\n",
    "            raise ValueError(\"Inputs contain infinite values\")\n",
    "        \n",
    "        # Track errors for this ticker if provided\n",
    "        if ticker:\n",
    "            errors = y_pred - y_true\n",
    "            self.error_history[ticker].extend(errors.tolist())\n",
    "            # Trim history if too long\n",
    "            if len(self.error_history[ticker]) > self.max_history_size:\n",
    "                self.error_history[ticker] = self.error_history[ticker][-self.max_history_size:]\n",
    "            \n",
    "            metrics['mean_error'] = np.mean(errors)\n",
    "            metrics['error_std'] = np.std(errors)\n",
    "            \n",
    "        try:\n",
    "            # Direction Accuracy (overall)\n",
    "            direction_true = np.sign(y_true[1:] - y_true[:-1])\n",
    "            direction_pred = np.sign(y_pred[1:] - y_pred[:-1])\n",
    "            direction_accuracy = np.mean(direction_true == direction_pred) * 100\n",
    "            metrics['direction_accuracy'] = direction_accuracy\n",
    "            \n",
    "            # Direction Accuracy (separated by up/down)\n",
    "            up_indices = np.where(direction_true > 0)[0]\n",
    "            down_indices = np.where(direction_true < 0)[0]\n",
    "            \n",
    "            if len(up_indices) > 0:\n",
    "                up_direction_accuracy = np.mean(direction_pred[up_indices] > 0) * 100\n",
    "                metrics['up_direction_accuracy'] = up_direction_accuracy\n",
    "            else:\n",
    "                metrics['up_direction_accuracy'] = 0\n",
    "                \n",
    "            if len(down_indices) > 0:\n",
    "                down_direction_accuracy = np.mean(direction_pred[down_indices] < 0) * 100\n",
    "                metrics['down_direction_accuracy'] = down_direction_accuracy\n",
    "            else:\n",
    "                metrics['down_direction_accuracy'] = 0\n",
    "            \n",
    "            # Directional Bias (higher means bias toward predicting up moves)\n",
    "            if current_prices is not None:\n",
    "                pred_direction_vs_current = np.sign(y_pred - current_prices)\n",
    "                up_pred_pct = np.mean(pred_direction_vs_current > 0) * 100\n",
    "                down_pred_pct = np.mean(pred_direction_vs_current < 0) * 100\n",
    "                metrics['up_prediction_pct'] = up_pred_pct\n",
    "                metrics['down_prediction_pct'] = down_pred_pct\n",
    "                metrics['direction_bias'] = up_pred_pct - down_pred_pct\n",
    "            \n",
    "            # Calculate prediction range metrics\n",
    "            metrics['true_price_range'] = np.max(y_true) - np.min(y_true)\n",
    "            metrics['pred_price_range'] = np.max(y_pred) - np.min(y_pred)\n",
    "            metrics['range_ratio'] = metrics['pred_price_range'] / metrics['true_price_range'] if metrics['true_price_range'] > 0 else 0\n",
    "            \n",
    "            # Standard error metrics\n",
    "            metrics['mse'] = mean_squared_error(y_true, y_pred)\n",
    "            metrics['mae'] = mean_absolute_error(y_true, y_pred)\n",
    "            metrics['rmse'] = np.sqrt(metrics['mse'])\n",
    "            \n",
    "            # Magnitude error by direction\n",
    "            if len(up_indices) > 0:\n",
    "                metrics['up_move_mae'] = mean_absolute_error(\n",
    "                    y_true[1:][up_indices], y_pred[1:][up_indices])\n",
    "            if len(down_indices) > 0:\n",
    "                metrics['down_move_mae'] = mean_absolute_error(\n",
    "                    y_true[1:][down_indices], y_pred[1:][down_indices])\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in calculate_metrics: {e}\")\n",
    "            raise\n",
    "            \n",
    "        return metrics\n",
    "\n",
    "    def get_adaptive_correction(self, ticker: str) -> float:\n",
    "        \"\"\"Get adaptive correction factor based on historical errors for this ticker.\"\"\"\n",
    "        if ticker in self.error_history and len(self.error_history[ticker]) > 10:\n",
    "            # Use recent history for correction\n",
    "            recent_errors = self.error_history[ticker][-10:]\n",
    "            return np.mean(recent_errors) * 0.5  # Partial correction to avoid overcompensation\n",
    "        return 0.0\n",
    "\n",
    "    def calculate_trade_performance_metrics(self, trades: List[Dict], \n",
    "                                           initial_capital: float = 10000.0,\n",
    "                                           include_transaction_costs: bool = True) -> Dict[str, float]:\n",
    "        \"\"\"Calculate comprehensive performance metrics for trades with realistic costs.\"\"\"\n",
    "        if not trades:\n",
    "            return {\n",
    "                'total_trades': 0,\n",
    "                'total_profit': 0.0,\n",
    "                'win_rate': 0.0,\n",
    "                'loss_rate': 0.0,\n",
    "                'average_profit_per_trade': 0.0,\n",
    "                'maximum_drawdown': 0.0,\n",
    "                'profit_factor': 0.0,\n",
    "                'sharpe_ratio': 0.0\n",
    "            }\n",
    "\n",
    "        # Initialize metrics tracking\n",
    "        equity_curve = [initial_capital]\n",
    "        daily_returns = defaultdict(float)\n",
    "        long_profits = []\n",
    "        short_profits = []\n",
    "        trade_durations = []\n",
    "\n",
    "        # Process each trade\n",
    "        for trade in trades:\n",
    "            # Calculate trade profit (should already include transaction costs if include_transaction_costs is True)\n",
    "            if 'profit' in trade:\n",
    "                profit = trade['profit']\n",
    "            else:\n",
    "                # Calculate from entry/exit prices if profit not provided\n",
    "                if trade['position'] == 'enter_long':\n",
    "                    profit = trade['exit_price'] - trade['entry_price']\n",
    "                else:  # enter_short\n",
    "                    profit = trade['entry_price'] - trade['exit_price']\n",
    "                \n",
    "                # Apply transaction costs if requested\n",
    "                if include_transaction_costs and 'transaction_costs' in trade:\n",
    "                    profit -= trade['transaction_costs']\n",
    "            \n",
    "            # Update equity curve\n",
    "            equity_curve.append(equity_curve[-1] + profit)\n",
    "            \n",
    "            # Track daily returns\n",
    "            trade_date = trade['exit_time'].date()\n",
    "            daily_returns[trade_date] += profit\n",
    "            \n",
    "            # Track by position type\n",
    "            if trade['position'] == 'enter_long':\n",
    "                long_profits.append(profit)\n",
    "            else:\n",
    "                short_profits.append(profit)\n",
    "                \n",
    "            # Track trade duration\n",
    "            if 'duration' in trade:\n",
    "                trade_durations.append(trade['duration'])\n",
    "\n",
    "        # Calculate overall metrics\n",
    "        profits = long_profits + short_profits\n",
    "        total_profit = sum(profits)\n",
    "        winning_trades = [p for p in profits if p > 0]\n",
    "        losing_trades = [p for p in profits if p < 0]\n",
    "        \n",
    "        win_rate = len(winning_trades) / len(profits) * 100 if profits else 0\n",
    "        loss_rate = len(losing_trades) / len(profits) * 100 if profits else 0\n",
    "        \n",
    "        # Calculate separate metrics for long and short positions\n",
    "        long_win_rate = len([p for p in long_profits if p > 0]) / len(long_profits) * 100 if long_profits else 0\n",
    "        short_win_rate = len([p for p in short_profits if p > 0]) / len(short_profits) * 100 if short_profits else 0\n",
    "        \n",
    "        # Calculate drawdown\n",
    "        drawdowns = [equity_curve[i] - max(equity_curve[:i+1]) for i in range(len(equity_curve))]\n",
    "        max_drawdown = abs(min(drawdowns)) if drawdowns else 0\n",
    "        \n",
    "        # Calculate Sharpe ratio from daily returns\n",
    "        daily_returns_list = list(daily_returns.values())\n",
    "        avg_daily_return = np.mean(daily_returns_list) if daily_returns_list else 0\n",
    "        std_daily_return = np.std(daily_returns_list) if len(daily_returns_list) > 1 else 1e-6\n",
    "        sharpe_ratio = (avg_daily_return / std_daily_return * np.sqrt(252)\n",
    "                       if std_daily_return > 0 else 0)\n",
    "\n",
    "        return {\n",
    "            'total_trades': len(trades),\n",
    "            'total_profit': round(total_profit, 2),\n",
    "            'win_rate': round(win_rate, 2),\n",
    "            'loss_rate': round(loss_rate, 2),\n",
    "            'long_trades': len(long_profits),\n",
    "            'short_trades': len(short_profits),\n",
    "            'long_win_rate': round(long_win_rate, 2),\n",
    "            'short_win_rate': round(short_win_rate, 2),\n",
    "            'average_profit_per_trade': round(np.mean(profits), 2) if profits else 0,\n",
    "            'average_trade_duration': round(np.mean(trade_durations), 2) if trade_durations else 0,\n",
    "            'maximum_drawdown': round(max_drawdown, 2),\n",
    "            'profit_factor': round(sum(winning_trades) / abs(sum(losing_trades)) if losing_trades and sum(losing_trades) != 0 else float('inf'), 2),\n",
    "            'sharpe_ratio': round(sharpe_ratio, 2)\n",
    "        }\n",
    "\n",
    "class AdaptiveSignalGenerator:\n",
    "    def __init__(self, ticker: str):\n",
    "        self.ticker = ticker\n",
    "        self.stock_profiles = {\n",
    "            'MSFT': {\n",
    "                'base_entry_threshold': 0.0008,  # Increased from 0.0006\n",
    "                'short_entry_threshold_factor': 0.9,  # Slightly favor shorts (was 1.0)\n",
    "                'base_exit_threshold': 0.0006,  # Increased from 0.0005\n",
    "                'base_stop_loss': 0.0012,\n",
    "                'atr_multiplier': 1.1,  # Increased from 1.0\n",
    "                'min_hold_time': 2,\n",
    "                'max_daily_trades': 250,\n",
    "                'position_size': 0.2,\n",
    "                'volatility_threshold': 0.7,\n",
    "                'volume_threshold': 0.6,\n",
    "                'trend_threshold': 0.015\n",
    "            },\n",
    "            'GOOGL': {\n",
    "                'base_entry_threshold': 0.0010,  # Increased from 0.0008\n",
    "                'short_entry_threshold_factor': 0.9,  # Slightly favor shorts (was 1.0)\n",
    "                'base_exit_threshold': 0.0007,  # Increased from 0.0006\n",
    "                'base_stop_loss': 0.0018,\n",
    "                'atr_multiplier': 1.2,  # Increased from 1.1\n",
    "                'min_hold_time': 2,\n",
    "                'max_daily_trades': 280,\n",
    "                'position_size': 0.2,\n",
    "                'volatility_threshold': 0.8,\n",
    "                'volume_threshold': 0.5,\n",
    "                'trend_threshold': 0.018\n",
    "            },\n",
    "            'TSLA': {\n",
    "                'base_entry_threshold': 0.0030,  # Increased from 0.0025\n",
    "                'short_entry_threshold_factor': 0.9,  # Slightly favor shorts (was 1.0)\n",
    "                'base_exit_threshold': 0.0023,  # Increased from 0.002\n",
    "                'base_stop_loss': 0.0035,\n",
    "                'atr_multiplier': 1.8,\n",
    "                'min_hold_time': 1,\n",
    "                'max_daily_trades': 180,\n",
    "                'position_size': 0.1,\n",
    "                'volatility_threshold': 1.3,\n",
    "                'volume_threshold': 0.7,\n",
    "                'trend_threshold': 0.025\n",
    "            },\n",
    "            'NVDA': {\n",
    "                'base_entry_threshold': 0.0025,  # Increased from 0.002\n",
    "                'short_entry_threshold_factor': 0.9,  # Slightly favor shorts (was 1.0)\n",
    "                'base_exit_threshold': 0.0018,  # Increased from 0.0015\n",
    "                'base_stop_loss': 0.003,\n",
    "                'atr_multiplier': 1.6,  # Increased from 1.5\n",
    "                'min_hold_time': 1,\n",
    "                'max_daily_trades': 220,\n",
    "                'position_size': 0.12,\n",
    "                'volatility_threshold': 1.1,\n",
    "                'volume_threshold': 0.6,\n",
    "                'trend_threshold': 0.02\n",
    "            },\n",
    "            'TQQQ': {\n",
    "                'base_entry_threshold': 0.0045,  # Increased from 0.004\n",
    "                'short_entry_threshold_factor': 0.85,  # More strongly favor shorts (was 1.0)\n",
    "                'base_exit_threshold': 0.0035,  # Increased from 0.003\n",
    "                'base_stop_loss': 0.006,\n",
    "                'atr_multiplier': 2.5,\n",
    "                'min_hold_time': 3,\n",
    "                'max_daily_trades': 120,\n",
    "                'position_size': 0.06,\n",
    "                'volatility_threshold': 2.0,\n",
    "                'volume_threshold': 0.8,\n",
    "                'trend_threshold': 0.04\n",
    "            },\n",
    "            'SQQQ': {\n",
    "                'base_entry_threshold': 0.0045,  # Increased from 0.004\n",
    "                'short_entry_threshold_factor': 0.85,  # More strongly favor shorts (was 1.0)\n",
    "                'base_exit_threshold': 0.0035,  # Increased from 0.003\n",
    "                'base_stop_loss': 0.006,\n",
    "                'atr_multiplier': 2.5,\n",
    "                'min_hold_time': 2,\n",
    "                'max_daily_trades': 100,\n",
    "                'position_size': 0.06,\n",
    "                'volatility_threshold': 2.0,\n",
    "                'volume_threshold': 0.8,\n",
    "                'trend_threshold': 0.04\n",
    "            },\n",
    "            'QLD': {\n",
    "                'base_entry_threshold': 0.0030,  # Increased from 0.0025\n",
    "                'short_entry_threshold_factor': 0.9,  # Slightly favor shorts (was 1.0)\n",
    "                'base_exit_threshold': 0.0022,  # Increased from 0.0018\n",
    "                'base_stop_loss': 0.0038,  # Increased from 0.0035\n",
    "                'atr_multiplier': 1.9,  # Increased from 1.8\n",
    "                'min_hold_time': 1,\n",
    "                'max_daily_trades': 180,\n",
    "                'position_size': 0.10,\n",
    "                'volatility_threshold': 1.5,\n",
    "                'volume_threshold': 0.7,\n",
    "                'trend_threshold': 0.03\n",
    "            },\n",
    "            'PSQ': {\n",
    "                'base_entry_threshold': 0.0030,  # Increased from 0.0025\n",
    "                'short_entry_threshold_factor': 0.9,  # Slightly favor shorts (was 1.0) \n",
    "                'base_exit_threshold': 0.0020,  # Increased from 0.0018\n",
    "                'base_stop_loss': 0.0038,  # Increased from 0.0035\n",
    "                'atr_multiplier': 1.7,  # Increased from 1.6\n",
    "                'min_hold_time': 1,\n",
    "                'max_daily_trades': 180,\n",
    "                'position_size': 0.15,\n",
    "                'volatility_threshold': 1.2,\n",
    "                'volume_threshold': 0.6,\n",
    "                'trend_threshold': 0.025\n",
    "            },\n",
    "            'QQQ': {\n",
    "                'base_entry_threshold': 0.0004,  # Increased from 0.0003\n",
    "                'short_entry_threshold_factor': 0.9,  # Slightly favor shorts (was 1.0)\n",
    "                'base_exit_threshold': 0.0003,  # Increased from 0.0002 \n",
    "                'base_stop_loss': 0.0012,  # Increased from 0.001\n",
    "                'atr_multiplier': 1.0,  # Increased from 0.9\n",
    "                'min_hold_time': 1,\n",
    "                'max_daily_trades': 250,\n",
    "                'position_size': 0.25,\n",
    "                'volatility_threshold': 0.3,\n",
    "                'volume_threshold': 0.0,\n",
    "                'trend_threshold': 0.006\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Default values for any ticker not explicitly listed\n",
    "        self.default_profile = {\n",
    "            'base_entry_threshold': 0.0018,  # Increased from 0.0015\n",
    "            'short_entry_threshold_factor': 0.9,  # Favor shorts (was 1.0)\n",
    "            'base_exit_threshold': 0.0012,  # Increased from 0.001\n",
    "            'base_stop_loss': 0.0028,  # Increased from 0.0025\n",
    "            'atr_multiplier': 1.4,  # Increased from 1.3\n",
    "            'min_hold_time': 2,\n",
    "            'max_daily_trades': 180,\n",
    "            'position_size': 0.15,\n",
    "            'volatility_threshold': 1.0,\n",
    "            'volume_threshold': 1.0,\n",
    "            'trend_threshold': 0.02\n",
    "        }\n",
    "        \n",
    "        # Use stock-specific profile if available, otherwise use default\n",
    "        self.params = self.stock_profiles.get(ticker, self.default_profile)\n",
    "        self._initialize_parameters()\n",
    "        \n",
    "        # For storing prediction confidence levels - equalized for better balance\n",
    "        self.direction_confidence = {'up': 0.75, 'down': 0.75}\n",
    "        \n",
    "    def _initialize_parameters(self):\n",
    "        \"\"\"Initialize trading parameters from stock profile.\"\"\"\n",
    "        self.base_entry_threshold = self.params['base_entry_threshold']\n",
    "        # Use full value for short entry threshold (removed the 0.7 reduction factor)\n",
    "        self.short_entry_threshold_factor = self.params['short_entry_threshold_factor']\n",
    "        self.base_exit_threshold = self.params['base_exit_threshold']\n",
    "        self.base_stop_loss = self.params['base_stop_loss']\n",
    "        self.atr_multiplier = self.params['atr_multiplier']\n",
    "        self.min_hold_time = self.params['min_hold_time']\n",
    "        self.volatility_threshold = self.params['volatility_threshold']\n",
    "        \n",
    "        self.position = None\n",
    "        self.entry_price = None\n",
    "        self.entry_time = None\n",
    "        self.current_atr = None\n",
    "        self.daily_trades = 0\n",
    "        self.last_trade_date = None\n",
    "        \n",
    "    def calculate_atr(self, market_data: pd.DataFrame) -> float:\n",
    "        \"\"\"Calculate Average True Range.\"\"\"\n",
    "        high = market_data['high'].values\n",
    "        low = market_data['low'].values\n",
    "        close = pd.Series(market_data['close'].values)\n",
    "        \n",
    "        tr1 = pd.Series(high - low)\n",
    "        tr2 = pd.Series(abs(high - close.shift()))\n",
    "        tr3 = pd.Series(abs(low - close.shift()))\n",
    "        \n",
    "        tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
    "        atr = tr.rolling(window=14).mean().iloc[-1]\n",
    "        \n",
    "        return atr\n",
    "        \n",
    "    def calculate_thresholds(self, current_volatility: float, \n",
    "                            market_regime: str = 'neutral',\n",
    "                            prediction_confidence: Dict[str, float] = None) -> Dict[str, float]:\n",
    "        \"\"\"Calculate adaptive thresholds based on current volatility, market regime, and prediction confidence.\"\"\"\n",
    "        volatility_factor = current_volatility * self.atr_multiplier\n",
    "        \n",
    "        # Base thresholds - make them equal for long and short entries\n",
    "        long_entry_threshold = self.base_entry_threshold * (1 + volatility_factor)\n",
    "        # NEW: Make short entry threshold slightly lower to counter bullish bias\n",
    "        short_entry_threshold = self.base_entry_threshold * (1 + volatility_factor) * 0.9\n",
    "        exit_threshold = self.base_exit_threshold * (1 + volatility_factor)\n",
    "        stop_loss = self.base_stop_loss * (1 + volatility_factor)\n",
    "        \n",
    "        # Adjust based on prediction confidence if available\n",
    "        if prediction_confidence:\n",
    "            # Scale thresholds inversely by confidence\n",
    "            long_entry_threshold *= (1.2 - prediction_confidence.get('up', 0.5))\n",
    "            short_entry_threshold *= (1.2 - prediction_confidence.get('down', 0.5))\n",
    "        \n",
    "        # NEW: Adjust based on market regime - more balanced approach\n",
    "        if market_regime == 'high_volatility':\n",
    "            long_entry_threshold *= 1.3   # More conservative for longs in high volatility\n",
    "            short_entry_threshold *= 0.7   # More aggressive for shorts in high volatility\n",
    "            stop_loss *= 1.1              # Wider stop loss in high volatility\n",
    "        elif market_regime == 'trending_up':\n",
    "            long_entry_threshold *= 1.0    # Neutral for longs in uptrend\n",
    "            short_entry_threshold *= 0.8   # More aggressive for shorts in uptrend\n",
    "        elif market_regime == 'trending_down':\n",
    "            long_entry_threshold *= 1.3    # More conservative for longs in downtrend\n",
    "            short_entry_threshold *= 0.6   # Much more aggressive for shorts in downtrend\n",
    "        \n",
    "        return {\n",
    "            'long_entry': long_entry_threshold,\n",
    "            'short_entry': short_entry_threshold,\n",
    "            'exit': exit_threshold,\n",
    "            'stop_loss': stop_loss\n",
    "        }\n",
    "        \n",
    "    def detect_market_regime(self, market_data: pd.DataFrame) -> str:\n",
    "        \"\"\"Detect current market regime based on price action and indicators.\"\"\"\n",
    "        if len(market_data) < 20:\n",
    "            return 'neutral'\n",
    "            \n",
    "        # Calculate volatility regime\n",
    "        returns = market_data['close'].pct_change().dropna()\n",
    "        current_volatility = returns.iloc[-20:].std() * np.sqrt(252)  # Annualized\n",
    "        avg_volatility = returns.iloc[-60:].std() * np.sqrt(252)\n",
    "        \n",
    "        # Calculate trend\n",
    "        ma5 = market_data['close'].rolling(5).mean().iloc[-1]\n",
    "        ma20 = market_data['close'].rolling(20).mean().iloc[-1]\n",
    "        trend_strength = (ma5 - ma20) / ma20\n",
    "        \n",
    "        # Determine regime\n",
    "        if current_volatility > avg_volatility * 1.3:\n",
    "            regime = 'high_volatility'\n",
    "        elif trend_strength > 0.02:\n",
    "            regime = 'trending_up'\n",
    "        elif trend_strength < -0.02:\n",
    "            regime = 'trending_down'\n",
    "        else:\n",
    "            regime = 'neutral'\n",
    "            \n",
    "        return regime\n",
    "        \n",
    "    def check_trend_condition(self, market_data: pd.DataFrame) -> Dict[str, bool]:\n",
    "        \"\"\"Check if trend conditions are favorable, return separate conditions for long and short.\"\"\"\n",
    "        if 'MA5' not in market_data.columns or 'SMA_20' not in market_data.columns:\n",
    "            # Calculate if not present\n",
    "            market_data['MA5'] = market_data['close'].rolling(5).mean()\n",
    "            market_data['SMA_20'] = market_data['close'].rolling(20).mean()\n",
    "        \n",
    "        ma5 = market_data['MA5'].iloc[-1]\n",
    "        ma20 = market_data['SMA_20'].iloc[-1]\n",
    "        \n",
    "        trend_strength = abs((ma5 - ma20) / ma20)\n",
    "        trend_direction = np.sign(ma5 - ma20)\n",
    "        \n",
    "        # Different trend conditions for long and short\n",
    "        if self.ticker in ['TSLA', 'NVDA', 'TQQQ', 'SQQQ']:\n",
    "            trend_threshold = self.params['trend_threshold'] * 1.5\n",
    "        else:\n",
    "            trend_threshold = self.params['trend_threshold']\n",
    "            \n",
    "        # More permissive trend check for shorts\n",
    "        long_trend_ok = trend_strength <= trend_threshold\n",
    "        short_trend_ok = True  # Allow shorts regardless of trend strength\n",
    "        \n",
    "        return {'long': long_trend_ok, 'short': short_trend_ok}\n",
    "            \n",
    "    def check_volume_condition(self, market_data: pd.DataFrame) -> bool:\n",
    "        \"\"\"Check if volume conditions are favorable.\"\"\"\n",
    "        current_volume = market_data['volume'].iloc[-1]\n",
    "        avg_volume = market_data['volume'].rolling(20).mean().iloc[-1]\n",
    "        volume_ratio = current_volume / avg_volume\n",
    "        \n",
    "        # If volume_threshold is 0, disable volume check (always return True)\n",
    "        if self.params['volume_threshold'] <= 0:\n",
    "            return True\n",
    "            \n",
    "        return volume_ratio >= self.params['volume_threshold']\n",
    "        \n",
    "    def update_direction_confidence(self, metrics: Dict[str, float]):\n",
    "        \"\"\"Update direction confidence based on recent model metrics.\"\"\"\n",
    "        if 'up_direction_accuracy' in metrics and 'down_direction_accuracy' in metrics:\n",
    "            # Scale between 0.5 and 1.0\n",
    "            self.direction_confidence['up'] = 0.5 + (metrics['up_direction_accuracy'] / 200)\n",
    "            self.direction_confidence['down'] = 0.5 + (metrics['down_direction_accuracy'] / 200)\n",
    "    \n",
    "    def _create_signal(self, ticker: str, timestamp: pd.Timestamp, price: float) -> Dict:\n",
    "        \"\"\"Create a base signal dictionary.\"\"\"\n",
    "        return {\n",
    "            'ticker': ticker,\n",
    "            'timestamp': timestamp,\n",
    "            'price': price,\n",
    "            'action': None,\n",
    "            'position': None,\n",
    "            'position_size': self.params['position_size'],\n",
    "            'thresholds': {\n",
    "                'long_entry': self.base_entry_threshold,\n",
    "                'short_entry': self.base_entry_threshold * self.short_entry_threshold_factor,\n",
    "                'exit': self.base_exit_threshold,\n",
    "                'stop_loss': self.base_stop_loss\n",
    "            },\n",
    "            'market_conditions': {\n",
    "                'atr': self.current_atr,\n",
    "                'daily_trades': self.daily_trades\n",
    "            }\n",
    "        }\n",
    "            \n",
    "    def generate_signals(self, ticker: str, current_price: float, \n",
    "                        predicted_price: float, timestamp: pd.Timestamp,\n",
    "                        market_data: pd.DataFrame,\n",
    "                        order_book_data: Dict = None,\n",
    "                        prediction_confidence: Dict[str, float] = None) -> Dict:\n",
    "        \"\"\"Generate trading signals with balanced thresholds for long/short.\"\"\"\n",
    "        # Reset daily trades if new day\n",
    "        current_date = timestamp.date()\n",
    "        if self.last_trade_date != current_date:\n",
    "            self.daily_trades = 0\n",
    "            self.last_trade_date = current_date\n",
    "            \n",
    "        # Check trade frequency limit\n",
    "        if self.daily_trades >= self.params['max_daily_trades']:\n",
    "            logger.info(f\"Trade rejected due to exceeding max daily trades ({self.daily_trades}/{self.params['max_daily_trades']})\")\n",
    "            return self._create_signal(ticker, timestamp, current_price)\n",
    "            \n",
    "        # Check minimum hold time\n",
    "        if self.position and self.entry_time:\n",
    "            hold_time = (timestamp - self.entry_time).total_seconds() / 60\n",
    "            if hold_time < self.params['min_hold_time']:\n",
    "                logger.info(f\"Trade rejected due to minimum hold time restriction ({hold_time:.1f} min < {self.params['min_hold_time']} min)\")\n",
    "                return self._create_signal(ticker, timestamp, current_price)\n",
    "        \n",
    "        # Variables for trend and volume checks - initialize outside the if blocks\n",
    "        trend_conditions = None\n",
    "        volume_ok = False\n",
    "        \n",
    "        try:\n",
    "            # Calculate ATR and market regime\n",
    "            self.current_atr = self.calculate_atr(market_data)\n",
    "            market_regime = self.detect_market_regime(market_data)\n",
    "            \n",
    "            # Safe logging for ATR\n",
    "            atr_value = 0.0 if self.current_atr is None else self.current_atr\n",
    "            logger.info(f\"Current market regime: {market_regime}, ATR: {atr_value:.6f}\")\n",
    "            \n",
    "            # Calculate recent prediction accuracy to adjust bias correction\n",
    "            price_changes = market_data['close'].pct_change().dropna()\n",
    "            mean_price_change = price_changes[-20:].mean() if len(price_changes) >= 20 else 0\n",
    "            logger.info(f\"Recent mean price change: {mean_price_change*100:.4f}%\")\n",
    "            \n",
    "            # Apply reduced downward bias correction\n",
    "            if market_regime in ['trending_up', 'high_volatility']:\n",
    "                downward_bias_correction = current_price * 0.0025  # Reduced from 0.5% to 0.25%\n",
    "            else:\n",
    "                downward_bias_correction = current_price * 0.001   # Reduced from 0.2% to 0.1%\n",
    "            \n",
    "            # Apply additional adaptive correction based on recent price movement\n",
    "            if mean_price_change > 0:\n",
    "                # If prices have been rising, apply reduced correction\n",
    "                adaptive_factor = min(0.0015, mean_price_change)  # Reduced from 0.3% to 0.15%\n",
    "                downward_bias_correction += current_price * adaptive_factor\n",
    "            \n",
    "            logger.info(f\"Applied downward bias correction: {downward_bias_correction:.4f} ({(downward_bias_correction/current_price)*100:.4f}%)\")\n",
    "            \n",
    "            # Use two different predictions for different directions\n",
    "            original_prediction = predicted_price\n",
    "            corrected_prediction = predicted_price - downward_bias_correction\n",
    "            \n",
    "            logger.info(f\"Original prediction: {original_prediction:.4f}, Corrected: {corrected_prediction:.4f}\")\n",
    "            \n",
    "            # Use provided prediction confidence or default to internal values\n",
    "            confidence = prediction_confidence or self.direction_confidence\n",
    "            \n",
    "            # Safe confidence values\n",
    "            up_conf = confidence.get('up', 0.5) if confidence else 0.5\n",
    "            down_conf = confidence.get('down', 0.5) if confidence else 0.5\n",
    "            logger.info(f\"Direction confidence - Up: {up_conf:.2f}, Down: {down_conf:.2f}\")\n",
    "            \n",
    "            # Get adaptive thresholds based on current conditions\n",
    "            thresholds = self.calculate_thresholds(\n",
    "                atr_value, \n",
    "                market_regime,\n",
    "                confidence\n",
    "            )\n",
    "            \n",
    "            # Safe logging for thresholds\n",
    "            if thresholds:\n",
    "                logger.info(f\"Thresholds - Long entry: {thresholds.get('long_entry', 0)*100:.4f}%, \" + \n",
    "                        f\"Short entry: {thresholds.get('short_entry', 0)*100:.4f}%\")\n",
    "            else:\n",
    "                logger.info(\"Thresholds calculation failed, using defaults\")\n",
    "                thresholds = {\n",
    "                    'long_entry': 0.001,\n",
    "                    'short_entry': 0.001,\n",
    "                    'exit': 0.001,\n",
    "                    'stop_loss': 0.002\n",
    "                }\n",
    "            \n",
    "            # Calculate price change for LONG - using corrected prediction\n",
    "            long_price_change_pct = (corrected_prediction - current_price) / current_price\n",
    "            \n",
    "            # Calculate price change for SHORT - using original prediction\n",
    "            short_price_change_pct = (original_prediction - current_price) / current_price\n",
    "            \n",
    "            logger.info(f\"Price change % - Long: {long_price_change_pct*100:.4f}%, Short: {short_price_change_pct*100:.4f}%\")\n",
    "            \n",
    "            # Create base signal\n",
    "            signal = self._create_signal(ticker, timestamp, current_price)\n",
    "            signal['market_regime'] = market_regime\n",
    "            signal['thresholds'] = thresholds\n",
    "            \n",
    "            # Pre-calculate trend and volume conditions once\n",
    "            trend_conditions = self.check_trend_condition(market_data)\n",
    "            volume_ok = self.check_volume_condition(market_data)\n",
    "            \n",
    "            # Safe logging for trend conditions\n",
    "            if trend_conditions:\n",
    "                logger.info(f\"Trend conditions - Long: {trend_conditions.get('long', False)}, Short: {trend_conditions.get('short', False)}\")\n",
    "            else:\n",
    "                logger.info(\"Trend conditions calculation failed\")\n",
    "                trend_conditions = {'long': False, 'short': False}\n",
    "                \n",
    "            logger.info(f\"Volume condition: {volume_ok}\")\n",
    "            \n",
    "            # Handle entry signals with balanced thresholds and detailed diagnostics\n",
    "            if self.position is None:\n",
    "                # Check and log long entry conditions\n",
    "                meets_long_threshold = long_price_change_pct > thresholds['long_entry']\n",
    "                meets_long_trend = trend_conditions.get('long', False)\n",
    "                \n",
    "                if not meets_long_threshold:\n",
    "                    logger.info(f\"LONG signal rejected - Price change {long_price_change_pct*100:.4f}% below threshold {thresholds['long_entry']*100:.4f}%\")\n",
    "                elif not meets_long_trend:\n",
    "                    logger.info(f\"LONG signal rejected - Failed trend condition\")\n",
    "                elif not volume_ok:\n",
    "                    logger.info(f\"LONG signal rejected - Failed volume condition\")\n",
    "                    \n",
    "                # Check and log short entry conditions\n",
    "                meets_short_threshold = short_price_change_pct < -thresholds['short_entry']\n",
    "                meets_short_trend = trend_conditions.get('short', False)\n",
    "                \n",
    "                if not meets_short_threshold:\n",
    "                    logger.info(f\"SHORT signal rejected - Price change {short_price_change_pct*100:.4f}% not below threshold -{thresholds['short_entry']*100:.4f}%\")\n",
    "                elif not meets_short_trend:\n",
    "                    logger.info(f\"SHORT signal rejected - Failed trend condition\")\n",
    "                elif not volume_ok:\n",
    "                    logger.info(f\"SHORT signal rejected - Failed volume condition\")\n",
    "                \n",
    "                # Long entry\n",
    "                if meets_long_threshold and meets_long_trend and volume_ok:\n",
    "                    self.position = 'long'\n",
    "                    self.entry_price = current_price\n",
    "                    self.entry_time = timestamp\n",
    "                    signal['action'] = 'enter_long'\n",
    "                    signal['position'] = 'long'\n",
    "                    signal['position_size'] = self.params['position_size'] * confidence.get('up', 0.5) * 2\n",
    "                    self.daily_trades += 1\n",
    "                    \n",
    "                    # Safe logging with try/except\n",
    "                    try:\n",
    "                        logger.info(f\"LONG SIGNAL: {ticker} at {timestamp}, price={current_price:.2f}, \" +\n",
    "                            f\"predicted_change={long_price_change_pct*100:.2f}%, \" +\n",
    "                            f\"threshold={thresholds['long_entry']*100:.2f}%\")\n",
    "                    except (TypeError, ValueError) as e:\n",
    "                        logger.info(f\"LONG SIGNAL: {ticker} at {timestamp}, price={current_price}, \" +\n",
    "                            f\"predicted_change={long_price_change_pct*100}%, \" +\n",
    "                            f\"threshold={thresholds['long_entry']*100}%\")\n",
    "                \n",
    "                # Short entry - using original prediction without bias correction\n",
    "                elif meets_short_threshold and meets_short_trend and volume_ok:\n",
    "                    self.position = 'short'\n",
    "                    self.entry_price = current_price\n",
    "                    self.entry_time = timestamp\n",
    "                    signal['action'] = 'enter_short'\n",
    "                    signal['position'] = 'short'\n",
    "                    signal['position_size'] = self.params['position_size'] * confidence.get('down', 0.5) * 2\n",
    "                    self.daily_trades += 1\n",
    "                    \n",
    "                    # Safe logging with try/except\n",
    "                    try:\n",
    "                        logger.info(f\"SHORT SIGNAL: {ticker} at {timestamp}, price={current_price:.2f}, \" +\n",
    "                            f\"predicted_change={short_price_change_pct*100:.2f}%, \" +\n",
    "                            f\"threshold={thresholds['short_entry']*100:.2f}%\")\n",
    "                    except (TypeError, ValueError) as e:\n",
    "                        logger.info(f\"SHORT SIGNAL: {ticker} at {timestamp}, price={current_price}, \" +\n",
    "                            f\"predicted_change={short_price_change_pct*100}%, \" +\n",
    "                            f\"threshold={thresholds['short_entry']*100}%\")\n",
    "                    \n",
    "                # Enhanced criteria for short entries based on technical indicators\n",
    "                elif ('RSI' in market_data.columns and 'MACD_Histogram' in market_data.columns):\n",
    "                    rsi_value = market_data['RSI'].iloc[-1]\n",
    "                    macd_hist = market_data['MACD_Histogram'].iloc[-1]\n",
    "                    \n",
    "                    # FIXED: Proper way to handle conditional formatting for logging\n",
    "                    rsi_str = f\"{rsi_value:.2f}\" if pd.notna(rsi_value) else \"N/A\"\n",
    "                    macd_str = f\"{macd_hist:.6f}\" if pd.notna(macd_hist) else \"N/A\"\n",
    "                    logger.info(f\"RSI: {rsi_str}, MACD Histogram: {macd_str}\")\n",
    "                    \n",
    "                    if (pd.notna(rsi_value) and pd.notna(macd_hist) and \n",
    "                        rsi_value > 65 and macd_hist < 0 and volume_ok and trend_conditions.get('short', False)):\n",
    "                        # RSI approaching overbought and MACD bearish - potential short opportunity\n",
    "                        self.position = 'short'\n",
    "                        self.entry_price = current_price\n",
    "                        self.entry_time = timestamp\n",
    "                        signal['action'] = 'enter_short'\n",
    "                        signal['position'] = 'short'\n",
    "                        signal['position_size'] = self.params['position_size'] * 1.0\n",
    "                        signal['forced_signal'] = True\n",
    "                        self.daily_trades += 1\n",
    "                        \n",
    "                        # Safe logging with try/except\n",
    "                        try:\n",
    "                            logger.info(f\"FORCED SHORT SIGNAL: {ticker} at {timestamp}, price={current_price:.2f}, \" +\n",
    "                                f\"RSI={rsi_value:.1f}, MACD_Hist={macd_hist:.4f}\")\n",
    "                        except (TypeError, ValueError) as e:\n",
    "                            logger.info(f\"FORCED SHORT SIGNAL: {ticker} at {timestamp}, price={current_price}, \" +\n",
    "                                f\"RSI={rsi_value}, MACD_Hist={macd_hist}\")\n",
    "                    else:\n",
    "                        logger.info(f\"Technical indicator SHORT condition not met - need RSI>65 and MACD_Hist<0\")\n",
    "                    \n",
    "                    if (pd.notna(rsi_value) and pd.notna(macd_hist) and \n",
    "                        rsi_value < 35 and macd_hist > 0 and volume_ok and trend_conditions.get('long', False)):\n",
    "                        # RSI approaching oversold and MACD bullish - potential long opportunity\n",
    "                        self.position = 'long'\n",
    "                        self.entry_price = current_price\n",
    "                        self.entry_time = timestamp\n",
    "                        signal['action'] = 'enter_long'\n",
    "                        signal['position'] = 'long'\n",
    "                        signal['position_size'] = self.params['position_size'] * 1.0\n",
    "                        signal['forced_signal'] = True\n",
    "                        self.daily_trades += 1\n",
    "                        \n",
    "                        # Safe logging with try/except\n",
    "                        try:\n",
    "                            logger.info(f\"FORCED LONG SIGNAL: {ticker} at {timestamp}, price={current_price:.2f}, \" +\n",
    "                                f\"RSI={rsi_value:.1f}, MACD_Hist={macd_hist:.4f}\")\n",
    "                        except (TypeError, ValueError) as e:\n",
    "                            logger.info(f\"FORCED LONG SIGNAL: {ticker} at {timestamp}, price={current_price}, \" +\n",
    "                                f\"RSI={rsi_value}, MACD_Hist={macd_hist}\")\n",
    "                    else:\n",
    "                        logger.info(f\"Technical indicator LONG condition not met - need RSI<35 and MACD_Hist>0\")\n",
    "                \n",
    "                # Log if no signals were generated\n",
    "                if not signal['action']:\n",
    "                    logger.info(f\"No trading signal generated - conditions not met\")\n",
    "            \n",
    "            # Handle exit signals\n",
    "            else:\n",
    "                if self.position == 'long':\n",
    "                    # Store entry_price before it gets reset\n",
    "                    stored_entry_price = self.entry_price\n",
    "                    \n",
    "                    # Calculate price change safely\n",
    "                    if stored_entry_price is not None:\n",
    "                        price_change_from_entry = (current_price - stored_entry_price) / stored_entry_price\n",
    "                        logger.info(f\"LONG position - Price change from entry: {price_change_from_entry*100:.4f}%\")\n",
    "                        logger.info(f\"Exit thresholds - Stop loss: -{thresholds['stop_loss']*100:.4f}%, Take profit: {thresholds['exit']*100:.4f}%\")\n",
    "                        \n",
    "                        if price_change_from_entry <= -thresholds['stop_loss']:\n",
    "                            logger.info(f\"LONG position - Stop loss triggered\")\n",
    "                            # Set signal data before resetting position variables\n",
    "                            signal['action'] = 'exit_long'\n",
    "                            signal['position'] = 'long'\n",
    "                            signal['exit_reason'] = 'stop_loss'\n",
    "                            \n",
    "                            # Safe logging with try/except - capture values before reset\n",
    "                            try:\n",
    "                                logger.info(f\"EXIT LONG (STOP_LOSS): {ticker} at {timestamp}, \" +\n",
    "                                    f\"entry={stored_entry_price:.2f}, exit={current_price:.2f}, \" +\n",
    "                                    f\"profit={price_change_from_entry*100:.2f}%\")\n",
    "                            except (TypeError, ValueError) as e:\n",
    "                                logger.info(f\"EXIT LONG (STOP_LOSS): {ticker} at {timestamp}, \" +\n",
    "                                    f\"entry={stored_entry_price}, exit={current_price}, \" +\n",
    "                                    f\"profit={price_change_from_entry*100}%\")\n",
    "                            \n",
    "                            # Reset position variables after logging\n",
    "                            self.position = None\n",
    "                            self.entry_price = None\n",
    "                            self.entry_time = None\n",
    "                            \n",
    "                        elif price_change_from_entry >= thresholds['exit']:\n",
    "                            logger.info(f\"LONG position - Take profit triggered\")\n",
    "                            # Set signal data before resetting position variables\n",
    "                            signal['action'] = 'exit_long'\n",
    "                            signal['position'] = 'long'\n",
    "                            signal['exit_reason'] = 'take_profit'\n",
    "                            \n",
    "                            # Safe logging with try/except - capture values before reset\n",
    "                            try:\n",
    "                                logger.info(f\"EXIT LONG (TAKE_PROFIT): {ticker} at {timestamp}, \" +\n",
    "                                    f\"entry={stored_entry_price:.2f}, exit={current_price:.2f}, \" +\n",
    "                                    f\"profit={price_change_from_entry*100:.2f}%\")\n",
    "                            except (TypeError, ValueError) as e:\n",
    "                                logger.info(f\"EXIT LONG (TAKE_PROFIT): {ticker} at {timestamp}, \" +\n",
    "                                    f\"entry={stored_entry_price}, exit={current_price}, \" +\n",
    "                                    f\"profit={price_change_from_entry*100}%\")\n",
    "                            \n",
    "                            # Reset position variables after logging\n",
    "                            self.position = None\n",
    "                            self.entry_price = None\n",
    "                            self.entry_time = None\n",
    "                            \n",
    "                        else:\n",
    "                            logger.info(f\"LONG position - Holding (no exit signal)\")\n",
    "                    else:\n",
    "                        logger.warning(f\"LONG position has invalid entry price. Resetting position.\")\n",
    "                        self.position = None\n",
    "                        self.entry_price = None\n",
    "                        self.entry_time = None\n",
    "                \n",
    "                elif self.position == 'short':\n",
    "                    # Store entry_price before it gets reset\n",
    "                    stored_entry_price = self.entry_price\n",
    "                    \n",
    "                    # Calculate price change safely\n",
    "                    if stored_entry_price is not None:\n",
    "                        price_change_from_entry = (stored_entry_price - current_price) / stored_entry_price\n",
    "                        logger.info(f\"SHORT position - Price change from entry: {price_change_from_entry*100:.4f}%\")\n",
    "                        logger.info(f\"Exit thresholds - Stop loss: -{thresholds['stop_loss']*100:.4f}%, Take profit: {thresholds['exit']*100:.4f}%\")\n",
    "                        \n",
    "                        if price_change_from_entry <= -thresholds['stop_loss']:\n",
    "                            logger.info(f\"SHORT position - Stop loss triggered\")\n",
    "                            # Set signal data before resetting position variables\n",
    "                            signal['action'] = 'exit_short'\n",
    "                            signal['position'] = 'short'\n",
    "                            signal['exit_reason'] = 'stop_loss'\n",
    "                            \n",
    "                            # Safe logging with try/except - capture values before reset\n",
    "                            try:\n",
    "                                logger.info(f\"EXIT SHORT (STOP_LOSS): {ticker} at {timestamp}, \" +\n",
    "                                    f\"entry={stored_entry_price:.2f}, exit={current_price:.2f}, \" +\n",
    "                                    f\"profit={price_change_from_entry*100:.2f}%\")\n",
    "                            except (TypeError, ValueError) as e:\n",
    "                                logger.info(f\"EXIT SHORT (STOP_LOSS): {ticker} at {timestamp}, \" +\n",
    "                                    f\"entry={stored_entry_price}, exit={current_price}, \" +\n",
    "                                    f\"profit={price_change_from_entry*100}%\")\n",
    "                            \n",
    "                            # Reset position variables after logging\n",
    "                            self.position = None\n",
    "                            self.entry_price = None\n",
    "                            self.entry_time = None\n",
    "                            \n",
    "                        elif price_change_from_entry >= thresholds['exit']:\n",
    "                            logger.info(f\"SHORT position - Take profit triggered\")\n",
    "                            # Set signal data before resetting position variables\n",
    "                            signal['action'] = 'exit_short'\n",
    "                            signal['position'] = 'short'\n",
    "                            signal['exit_reason'] = 'take_profit'\n",
    "                            \n",
    "                            # Safe logging with try/except - capture values before reset\n",
    "                            try:\n",
    "                                logger.info(f\"EXIT SHORT (TAKE_PROFIT): {ticker} at {timestamp}, \" +\n",
    "                                    f\"entry={stored_entry_price:.2f}, exit={current_price:.2f}, \" +\n",
    "                                    f\"profit={price_change_from_entry*100:.2f}%\")\n",
    "                            except (TypeError, ValueError) as e:\n",
    "                                logger.info(f\"EXIT SHORT (TAKE_PROFIT): {ticker} at {timestamp}, \" +\n",
    "                                    f\"entry={stored_entry_price}, exit={current_price}, \" +\n",
    "                                    f\"profit={price_change_from_entry*100}%\")\n",
    "                            \n",
    "                            # Reset position variables after logging\n",
    "                            self.position = None\n",
    "                            self.entry_price = None\n",
    "                            self.entry_time = None\n",
    "                            \n",
    "                        else:\n",
    "                            logger.info(f\"SHORT position - Holding (no exit signal)\")\n",
    "                    else:\n",
    "                        logger.warning(f\"SHORT position has invalid entry price. Resetting position.\")\n",
    "                        self.position = None\n",
    "                        self.entry_price = None\n",
    "                        self.entry_time = None\n",
    "                        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating signals: {str(e)}\")\n",
    "            traceback.print_exc()  # Added stack trace for better debugging\n",
    "            return self._create_signal(ticker, timestamp, current_price)\n",
    "        \n",
    "        return signal\n",
    "        \n",
    "\n",
    "\n",
    "class PolygonDataFetcher:\n",
    "    \"\"\"Handles data retrieval from the Polygon.io API with robust error handling and rate limiting.\"\"\"\n",
    "    \n",
    "    BASE_URL = \"https://api.polygon.io/v2\"\n",
    "\n",
    "    def __init__(self, api_key: str):\n",
    "        self.api_key = api_key\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\"Authorization\": f\"Bearer {api_key}\"})\n",
    "        self.validation_metrics = []\n",
    "\n",
    "    def _make_request(self, endpoint: str, params: Optional[Dict] = None, retries: int = 3) -> Dict:\n",
    "        \"\"\"Make an API request with retry logic and rate limiting.\"\"\"\n",
    "        url = f\"{self.BASE_URL}/{endpoint}\"\n",
    "        \n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                response = self.session.get(url, params=params, timeout=30)\n",
    "                \n",
    "                if response.status_code == 429:  # Rate limit exceeded\n",
    "                    wait_time = min(60 * (attempt + 1), 300)  # Max 5 minutes\n",
    "                    logger.warning(f\"Rate limit exceeded. Waiting {wait_time} seconds...\")\n",
    "                    time.sleep(wait_time)\n",
    "                    continue\n",
    "                    \n",
    "                response.raise_for_status()\n",
    "                data = response.json()\n",
    "                \n",
    "                return data\n",
    "                \n",
    "            except Exception as e:\n",
    "                if attempt == retries - 1:\n",
    "                    logger.error(f\"Request failed after {retries} attempts: {e}\")\n",
    "                    raise\n",
    "                logger.warning(f\"Request failed (Attempt {attempt + 1}/{retries}): {e}. Retrying in 30 seconds...\")\n",
    "                time.sleep(30)\n",
    "                \n",
    "        raise Exception(f\"Failed after {retries} attempts\")\n",
    "\n",
    "    def fetch_stock_data(self, symbol: str, start_date: str, end_date: str, \n",
    "                        timespan: str = \"minute\", multiplier: int = 1) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Fetch aggregate stock data for a specific ticker and date range.\n",
    "        \n",
    "        Args:\n",
    "            symbol: Stock ticker symbol\n",
    "            start_date: Start date in YYYY-MM-DD format\n",
    "            end_date: End date in YYYY-MM-DD format\n",
    "            timespan: Timespan of the aggregates ('minute', 'hour', 'day')\n",
    "            multiplier: The size of the timespan multiplier\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame containing aggregated stock data\n",
    "        \"\"\"\n",
    "        endpoint = f\"aggs/ticker/{symbol}/range/{multiplier}/{timespan}/{start_date}/{end_date}\"\n",
    "        params = {\"adjusted\": \"true\", \"sort\": \"asc\", \"limit\": 50000}\n",
    "        all_results = []\n",
    "\n",
    "        with tqdm(desc=f\"Fetching {symbol}\", unit='rows') as pbar:\n",
    "            while True:\n",
    "                data = self._make_request(endpoint, params)\n",
    "                \n",
    "                if 'results' in data:\n",
    "                    if len(data['results']) > 0 and 'c' not in data['results'][0]:\n",
    "                        logger.error(f\"Data for {symbol} does not contain 'c' (close) field.\")\n",
    "                        return pd.DataFrame()\n",
    "\n",
    "                    all_results.extend(data['results'])\n",
    "                    pbar.update(len(data['results']))\n",
    "                    \n",
    "                if len(data.get('results', [])) < 50000:\n",
    "                    break\n",
    "                    \n",
    "                # Update start_date for next request\n",
    "                last_timestamp = data['results'][-1]['t']\n",
    "                start_date = datetime.fromtimestamp(last_timestamp / 1000).strftime('%Y-%m-%d')\n",
    "                params['from'] = start_date\n",
    "\n",
    "        # Create DataFrame and process data\n",
    "        if not all_results:\n",
    "            logger.warning(f\"No data retrieved for {symbol}\")\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "        df = pd.DataFrame(all_results)\n",
    "        \n",
    "        if 't' in df.columns:\n",
    "            # Remove duplicate timestamps\n",
    "            duplicates_before = len(df) - df.drop_duplicates(subset=['t'], keep='last').shape[0]\n",
    "            if duplicates_before > 0:\n",
    "                logger.info(f\"Removed {duplicates_before} duplicate timestamps for {symbol}.\")\n",
    "            df = df.drop_duplicates(subset=['t'], keep='last')\n",
    "        else:\n",
    "            logger.error(\"'t' column not found in fetched data.\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        # Rename columns for clarity\n",
    "        df = df.rename(columns={\n",
    "            't': 'timestamp',\n",
    "            'o': 'open',\n",
    "            'h': 'high',\n",
    "            'l': 'low',\n",
    "            'c': 'close',\n",
    "            'v': 'volume',\n",
    "            'n': 'transactions'\n",
    "        })\n",
    "\n",
    "        # Convert timestamp to datetime and set as index\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
    "        df.set_index('timestamp', inplace=True)\n",
    "\n",
    "        return df\n",
    "        \n",
    "    def fetch_market_index_data(self, index_symbol: str = \"SPY\", \n",
    "                             start_date: str = None, end_date: str = None) -> pd.DataFrame:\n",
    "        \"\"\"Fetch market index data to add market context.\"\"\"\n",
    "        if not start_date or not end_date:\n",
    "            end_date = datetime.now().strftime('%Y-%m-%d')\n",
    "            start_date = (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d')\n",
    "            \n",
    "        return self.fetch_stock_data(index_symbol, start_date, end_date)\n",
    "\n",
    "def enhance_features(ticker_df: pd.DataFrame, market_index_df: Optional[pd.DataFrame] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add enhanced features including market regime detection.\n",
    "    \n",
    "    Args:\n",
    "        ticker_df: DataFrame with ticker OHLCV data\n",
    "        market_index_df: Optional DataFrame with market index data for context\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with additional features and indicators\n",
    "    \"\"\"\n",
    "    # Check for required columns\n",
    "    required_cols = ['open', 'high', 'low', 'close', 'volume']\n",
    "    if not all(col in ticker_df.columns for col in required_cols):\n",
    "        missing = [col for col in required_cols if col not in ticker_df.columns]\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "        \n",
    "    enhanced_df = ticker_df.copy()\n",
    "    \n",
    "    # Add basic technical indicators\n",
    "    enhanced_df['returns'] = enhanced_df['close'].pct_change()\n",
    "    enhanced_df['log_returns'] = np.log(enhanced_df['close'] / enhanced_df['close'].shift(1))\n",
    "    \n",
    "    # Price moving averages\n",
    "    enhanced_df['MA5'] = enhanced_df['close'].rolling(window=5).mean()\n",
    "    enhanced_df['SMA_20'] = enhanced_df['close'].rolling(window=20).mean()\n",
    "    enhanced_df['EMA_20'] = enhanced_df['close'].ewm(span=20).mean()\n",
    "    \n",
    "    # Momentum indicators\n",
    "    enhanced_df['RSI'] = ta.momentum.rsi(enhanced_df['close'], window=14)\n",
    "    \n",
    "    # Volatility indicators\n",
    "    bb_indicator = BollingerBands(close=enhanced_df['close'])\n",
    "    enhanced_df['Bollinger_High'] = bb_indicator.bollinger_hband()\n",
    "    enhanced_df['Bollinger_Low'] = bb_indicator.bollinger_lband()\n",
    "    enhanced_df['Bollinger_Width'] = (enhanced_df['Bollinger_High'] - enhanced_df['Bollinger_Low']) / enhanced_df['SMA_20']\n",
    "    \n",
    "    # Trend indicators\n",
    "    macd_indicator = MACD(close=enhanced_df['close'])\n",
    "    enhanced_df['MACD'] = macd_indicator.macd()\n",
    "    enhanced_df['MACD_Signal'] = macd_indicator.macd_signal()\n",
    "    enhanced_df['MACD_Histogram'] = enhanced_df['MACD'] - enhanced_df['MACD_Signal']\n",
    "    \n",
    "    # Price patterns\n",
    "    enhanced_df['daily_range'] = enhanced_df['high'] - enhanced_df['low']\n",
    "    enhanced_df['gap_up'] = (enhanced_df['open'] - enhanced_df['close'].shift(1)) / enhanced_df['close'].shift(1)\n",
    "    enhanced_df['body_size'] = abs(enhanced_df['close'] - enhanced_df['open']) / enhanced_df['open']\n",
    "    \n",
    "    # Volume analysis\n",
    "    enhanced_df['volume_ma'] = enhanced_df['volume'].rolling(window=20).mean()\n",
    "    enhanced_df['volume_ratio'] = enhanced_df['volume'] / enhanced_df['volume_ma']\n",
    "    enhanced_df['price_volume'] = enhanced_df['returns'].abs() * enhanced_df['volume_ratio']\n",
    "\n",
    "    # Add relative features rather than absolute values\n",
    "    enhanced_df['price_acceleration'] = enhanced_df['returns'].diff()\n",
    "    enhanced_df['ma_cross'] = (enhanced_df['MA5'] > enhanced_df['SMA_20']).astype(int)\n",
    "    enhanced_df['bb_position'] = (enhanced_df['close'] - enhanced_df['Bollinger_Low']) / (enhanced_df['Bollinger_High'] - enhanced_df['Bollinger_Low'])\n",
    "    \n",
    "    # Market regime detection\n",
    "    enhanced_df['volatility'] = enhanced_df['returns'].rolling(20).std() * np.sqrt(252)  # Annualized\n",
    "    enhanced_df['volatility_percentile'] = enhanced_df['volatility'].rolling(60).apply(\n",
    "        lambda x: percentileofscore(x, x.iloc[-1]) if len(x) > 0 else 50\n",
    "    )\n",
    "    \n",
    "    # Trend regime\n",
    "    enhanced_df['trend_strength'] = (enhanced_df['MA5'] - enhanced_df['SMA_20']) / enhanced_df['SMA_20']\n",
    "    enhanced_df['trend_regime'] = pd.cut(\n",
    "        enhanced_df['trend_strength'].fillna(0),\n",
    "        bins=[-float('inf'), -0.02, -0.005, 0.005, 0.02, float('inf')],\n",
    "        labels=['strong_downtrend', 'downtrend', 'neutral', 'uptrend', 'strong_uptrend']\n",
    "    )\n",
    "    \n",
    "    # Up/down move features\n",
    "    enhanced_df['up_day'] = (enhanced_df['close'] > enhanced_df['close'].shift(1)).astype(int)\n",
    "    enhanced_df['down_day'] = (enhanced_df['close'] < enhanced_df['close'].shift(1)).astype(int)\n",
    "    enhanced_df['up_volume'] = enhanced_df['volume'] * enhanced_df['up_day']\n",
    "    enhanced_df['down_volume'] = enhanced_df['volume'] * enhanced_df['down_day']\n",
    "\n",
    "    # Enhanced price distance features\n",
    "    enhanced_df['price_dist_from_mean_5d'] = (enhanced_df['close'] - enhanced_df['MA5']) / enhanced_df['close']\n",
    "    enhanced_df['price_dist_from_mean_20d'] = (enhanced_df['close'] - enhanced_df['SMA_20']) / enhanced_df['close']\n",
    "    \n",
    "    # Enhanced momentum features\n",
    "    enhanced_df['price_momentum_5d'] = enhanced_df['returns'].rolling(5).sum()\n",
    "    enhanced_df['price_momentum_20d'] = enhanced_df['returns'].rolling(20).sum()\n",
    "    \n",
    "    # Non-linear transformations\n",
    "    enhanced_df['log_abs_return'] = np.log(np.abs(enhanced_df['returns']) + 1e-6)\n",
    "    enhanced_df['return_sign'] = np.sign(enhanced_df['returns'])\n",
    "    enhanced_df['return_squared'] = enhanced_df['returns'] ** 2\n",
    "    \n",
    "    # Range features\n",
    "    enhanced_df['daily_range_ratio'] = enhanced_df['daily_range'] / enhanced_df['close']\n",
    "    enhanced_df['range_momentum'] = enhanced_df['daily_range_ratio'].rolling(5).mean()\n",
    "    \n",
    "    # Add market index context if available\n",
    "    if market_index_df is not None and not market_index_df.empty:\n",
    "        # Align indices\n",
    "        market_index_df = market_index_df.reindex(enhanced_df.index, method='ffill')\n",
    "        \n",
    "        # Add market return features\n",
    "        enhanced_df['market_return'] = market_index_df['close'].pct_change()\n",
    "        enhanced_df['market_ma20'] = market_index_df['close'].rolling(20).mean()\n",
    "        enhanced_df['market_vol'] = market_index_df['close'].pct_change().rolling(20).std()\n",
    "        \n",
    "        # Correlation metrics\n",
    "        enhanced_df['market_correlation'] = enhanced_df['returns'].rolling(20).corr(enhanced_df['market_return'])\n",
    "        enhanced_df['beta'] = enhanced_df['returns'].rolling(20).cov(enhanced_df['market_return']) / enhanced_df['market_return'].rolling(20).var()\n",
    "        \n",
    "        # Relative strength\n",
    "        enhanced_df['relative_strength'] = enhanced_df['close'] / enhanced_df['close'].shift(20)\n",
    "        enhanced_df['market_relative_strength'] = market_index_df['close'] / market_index_df['close'].shift(20)\n",
    "        enhanced_df['rs_ratio'] = enhanced_df['relative_strength'] / enhanced_df['market_relative_strength']\n",
    "    \n",
    "    # Drop NaN values created by rolling windows\n",
    "    enhanced_df.dropna(inplace=True)\n",
    "    \n",
    "    return enhanced_df\n",
    "\n",
    "def create_stratified_dataset(data_df: pd.DataFrame, min_regime_count: int = 50, \n",
    "                             min_total_samples: int = 1000,\n",
    "                             max_sampling_fraction: float = 0.7,\n",
    "                             max_oversample_factor: float = 5.0) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a dataset with balanced market regimes for more even training.\n",
    "    \n",
    "    Args:\n",
    "        data_df: DataFrame with OHLCV data and calculated features\n",
    "        min_regime_count: Minimum number of samples per regime\n",
    "        min_total_samples: Minimum total samples in the resulting dataset\n",
    "        max_sampling_fraction: Maximum reduction as a fraction of original size\n",
    "        max_oversample_factor: Maximum factor for oversampling any regime\n",
    "    \n",
    "    Returns:\n",
    "        Balanced DataFrame with representation from all market regimes\n",
    "    \"\"\"\n",
    "    # Define market regimes if not already done\n",
    "    if 'regime' not in data_df.columns:\n",
    "        # Initialize regime column\n",
    "        data_df['regime'] = 'neutral'\n",
    "        \n",
    "        # Identify high volatility periods (more sensitive thresholds)\n",
    "        volatility_threshold = data_df['volatility'].quantile(0.65)\n",
    "        high_vol = data_df['volatility'] >= volatility_threshold\n",
    "        \n",
    "        # Identify up and down days (more sensitive thresholds)\n",
    "        up_day = data_df['returns'] > 0.0008\n",
    "        down_day = data_df['returns'] < -0.0008\n",
    "        \n",
    "        # Combine into regimes\n",
    "        data_df.loc[high_vol & up_day, 'regime'] = 'high_vol_up'\n",
    "        data_df.loc[high_vol & down_day, 'regime'] = 'high_vol_down'\n",
    "        data_df.loc[(~high_vol) & up_day, 'regime'] = 'low_vol_up'\n",
    "        data_df.loc[(~high_vol) & down_day, 'regime'] = 'low_vol_down'\n",
    "    \n",
    "    # Get counts by regime\n",
    "    regime_counts = data_df['regime'].value_counts()\n",
    "    logger.info(f\"Original regime distribution: {regime_counts.to_dict()}\")\n",
    "    \n",
    "    # Check if we have sufficient data in any regime\n",
    "    non_zero_counts = regime_counts[regime_counts > 0]\n",
    "    if len(non_zero_counts) == 0:\n",
    "        logger.warning(\"No valid regimes found in dataset\")\n",
    "        return data_df\n",
    "        \n",
    "    # If we have very few samples in any regime, consider all data as neutral\n",
    "    if non_zero_counts.min() < 10:\n",
    "        logger.warning(f\"Very few samples in some regimes. Classifying all as neutral.\")\n",
    "        data_df['regime'] = 'neutral'\n",
    "        regime_counts = data_df['regime'].value_counts()\n",
    "        non_zero_counts = regime_counts\n",
    "    \n",
    "    # Find minimum count, but ensure it's at least min_regime_count\n",
    "    min_count = max(int(non_zero_counts.min() * 0.8), min_regime_count)\n",
    "    \n",
    "    # Calculate total samples after balancing\n",
    "    num_regimes = len(data_df['regime'].unique())\n",
    "    total_balanced_samples = min_count * num_regimes\n",
    "    \n",
    "    # Adjust if total samples would be too small compared to original\n",
    "    if total_balanced_samples < min_total_samples or total_balanced_samples < max_sampling_fraction * len(data_df):\n",
    "        # Increase min_count to meet the minimum total requirement\n",
    "        required_count = max(\n",
    "            min_total_samples // num_regimes,\n",
    "            int(max_sampling_fraction * len(data_df) // num_regimes)\n",
    "        )\n",
    "        min_count = max(min_count, required_count)\n",
    "        logger.info(f\"Adjusted regime count to {min_count} to maintain sufficient data volume\")\n",
    "    \n",
    "    # Sample from each regime\n",
    "    balanced_df = pd.DataFrame()\n",
    "    for regime in data_df['regime'].unique():\n",
    "        regime_data = data_df[data_df['regime'] == regime]\n",
    "        if len(regime_data) == 0:\n",
    "            continue\n",
    "            \n",
    "        if len(regime_data) > min_count:\n",
    "            # Sample without replacement if enough data\n",
    "            sampled_data = regime_data.sample(min_count)\n",
    "            logger.info(f\"Sampled without replacement for regime '{regime}': {len(regime_data)} → {min_count}\")\n",
    "        else:\n",
    "            # Check if we would exceed maximum oversampling factor\n",
    "            if len(regime_data) * max_oversample_factor < min_count:\n",
    "                # Cap the number of samples based on max oversampling factor\n",
    "                actual_samples = int(len(regime_data) * max_oversample_factor)\n",
    "                logger.warning(f\"Limiting oversampling for regime '{regime}': {len(regime_data)} → {actual_samples} (capped at {max_oversample_factor}x)\")\n",
    "                sampled_data = regime_data.sample(actual_samples, replace=True)\n",
    "            else:\n",
    "                # Sample with replacement up to min_count\n",
    "                sampled_data = regime_data.sample(min_count, replace=True)\n",
    "                logger.info(f\"Sampled with replacement for regime '{regime}': {len(regime_data)} → {min_count}\")\n",
    "            \n",
    "        balanced_df = pd.concat([balanced_df, sampled_data])\n",
    "    \n",
    "    # Shuffle the final dataset\n",
    "    balanced_df = balanced_df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    # Log final distribution\n",
    "    final_regime_counts = balanced_df['regime'].value_counts()\n",
    "    logger.info(f\"Balanced regime distribution: {final_regime_counts.to_dict()}\")\n",
    "    logger.info(f\"Original dataset: {len(data_df)} samples → Balanced dataset: {len(balanced_df)} samples\")\n",
    "    \n",
    "    return balanced_df\n",
    "\n",
    "class DirectionalPredictionLoss(nn.Module):\n",
    "    def __init__(self, direction_weight=2.5, magnitude_weight=0.8, \n",
    "                short_penalty_multiplier=1.5, bias_correction_weight=0.8):  # Reduced from 3.5 to 1.5 and from 1.5 to 0.8\n",
    "        super(DirectionalPredictionLoss, self).__init__()\n",
    "        self.direction_weight = direction_weight\n",
    "        self.magnitude_weight = magnitude_weight\n",
    "        self.short_penalty_multiplier = short_penalty_multiplier\n",
    "        self.bias_correction_weight = bias_correction_weight\n",
    "        \n",
    "        # Ticker-specific adaptations - balance up/down weights, with more equal weighting\n",
    "        self.ticker_adapters = {\n",
    "            'MSFT': {'up_weight': 1.0, 'down_weight': 1.2},    # More balanced (was 1.0, 1.5)\n",
    "            'GOOGL': {'up_weight': 1.0, 'down_weight': 1.2},   # More balanced (was 1.0, 1.5)\n",
    "            'TSLA': {'up_weight': 1.0, 'down_weight': 1.3},    # More balanced (was 1.0, 1.6)\n",
    "            'NVDA': {'up_weight': 1.0, 'down_weight': 1.2},    # More balanced (was 1.0, 1.5)\n",
    "            'TQQQ': {'up_weight': 1.0, 'down_weight': 1.3},    # More balanced (was 0.9, 1.6)\n",
    "            'QQQ': {'up_weight': 1.0, 'down_weight': 1.2}      # More balanced (was 0.9, 1.4)\n",
    "        }\n",
    "        \n",
    "        # Default adapter for tickers not explicitly listed - more balanced\n",
    "        self.default_adapter = {'up_weight': 1.0, 'down_weight': 1.2}  # Changed from 0.9/1.4 to 1.0/1.2\n",
    "        \n",
    "        self.current_ticker = None\n",
    "        \n",
    "        # For tracking error bias\n",
    "        self.prediction_history = []\n",
    "        self.direction_bias_history = []\n",
    "        self.max_history_length = 50\n",
    "        \n",
    "    def set_ticker(self, ticker: str):\n",
    "        \"\"\"Set the current ticker for adaptive weights.\"\"\"\n",
    "        self.current_ticker = ticker\n",
    "        self.prediction_history = []\n",
    "        self.direction_bias_history = []\n",
    "        \n",
    "    def forward(self, y_pred, y_true, current_prices=None):\n",
    "        # Use current price if provided, otherwise use previous price\n",
    "        if current_prices is None:\n",
    "            current_prices = y_true.roll(1)\n",
    "            current_prices[0] = y_true[0]  # Avoid NaN in first element\n",
    "        \n",
    "        # Calculate price movements\n",
    "        true_movement = y_true - current_prices\n",
    "        pred_movement = y_pred - current_prices\n",
    "        \n",
    "        # Direction of movement\n",
    "        true_direction = torch.sign(true_movement)\n",
    "        pred_direction = torch.sign(pred_movement)\n",
    "        \n",
    "        # Direction error (0 if correct, 1 if wrong)\n",
    "        direction_error = (pred_direction != true_direction).float()\n",
    "        \n",
    "        # Magnitude error\n",
    "        magnitude_error = torch.abs(y_pred - y_true)\n",
    "        \n",
    "        # Store current prediction error for bias tracking\n",
    "        with torch.no_grad():\n",
    "            current_error = (y_pred - y_true).mean().item()\n",
    "            self.prediction_history.append(current_error)\n",
    "            \n",
    "            # Track directional bias (positive means more up predictions)\n",
    "            direction_bias = ((pred_direction > 0).float().mean() - 0.5) * 2  # Scale -1 to 1\n",
    "            self.direction_bias_history.append(direction_bias.item())\n",
    "            \n",
    "            # Maintain max history length\n",
    "            if len(self.prediction_history) > self.max_history_length:\n",
    "                self.prediction_history.pop(0)\n",
    "                self.direction_bias_history.pop(0)\n",
    "        \n",
    "        # Apply reduced bias correction with more weight on recent errors\n",
    "        if len(self.prediction_history) > 10:\n",
    "            # Use exponentially weighted bias correction - more weight to recent errors\n",
    "            weights = torch.tensor([0.8 ** i for i in range(min(10, len(self.prediction_history)))], device=y_pred.device)\n",
    "            weights = weights / weights.sum()\n",
    "            \n",
    "            recent_errors = torch.tensor(self.prediction_history[-10:], device=y_pred.device)\n",
    "            weighted_bias = (recent_errors * weights).sum()\n",
    "            \n",
    "            # Reduced bias correction weight\n",
    "            bias_correction = torch.abs(weighted_bias) * self.bias_correction_weight\n",
    "            \n",
    "            # Add directional bias penalty - reduced penalty\n",
    "            direction_bias = torch.tensor(np.mean(self.direction_bias_history[-10:]), device=y_pred.device) \n",
    "            if direction_bias > 0:  # If biased toward up predictions\n",
    "                bias_correction += direction_bias * 0.5  # Reduced from 0.8 for less aggressive penalty\n",
    "        else:\n",
    "            bias_correction = torch.tensor(0.0, device=y_pred.device)\n",
    "        \n",
    "        # Apply ticker-specific direction weights\n",
    "        if self.current_ticker and self.current_ticker in self.ticker_adapters:\n",
    "            adapter = self.ticker_adapters[self.current_ticker]\n",
    "        else:\n",
    "            adapter = self.default_adapter\n",
    "            \n",
    "        # Create separate masks for up and down movements\n",
    "        up_mask = (true_direction > 0)\n",
    "        down_mask = (true_direction < 0)\n",
    "        \n",
    "        # Apply different weights based on direction - with more balanced weights\n",
    "        direction_error_weighted = direction_error.clone()\n",
    "        direction_error_weighted[up_mask] *= adapter['up_weight']\n",
    "        direction_error_weighted[down_mask] *= adapter['down_weight']  # Reduced penalty\n",
    "        \n",
    "        # Replace original with weighted version\n",
    "        direction_error = direction_error_weighted\n",
    "            \n",
    "        # Add reduced extra penalty for missing downward movements\n",
    "        down_movement_mask = (true_direction < 0)\n",
    "        direction_error[down_movement_mask] *= self.short_penalty_multiplier  # Reduced from 3.5 to 1.5\n",
    "        \n",
    "        # Combined loss with bias correction\n",
    "        loss = (self.direction_weight * direction_error + \n",
    "                self.magnitude_weight * magnitude_error + \n",
    "                bias_correction)\n",
    "        \n",
    "        # Add directional balance constraint with updated target\n",
    "        with torch.no_grad():\n",
    "            pred_directions = torch.sign(y_pred - current_prices)\n",
    "            up_ratio = (pred_directions > 0).float().mean()\n",
    "            \n",
    "            # Target a more balanced range (0.40-0.60)\n",
    "            ideal_up_ratio = 0.45  # Adjusted toward more balanced (was 0.40)\n",
    "            distribution_penalty = 3.0 * torch.abs(up_ratio - ideal_up_ratio)  # Reduced from 4.0 to 3.0\n",
    "        \n",
    "        # Add the distribution penalty to the loss\n",
    "        loss = loss + distribution_penalty\n",
    "\n",
    "        return loss.mean()\n",
    "\n",
    "class HFTDataset(Dataset):\n",
    "    \"\"\"Enhanced Dataset for High-Frequency Trading data preparation.\"\"\"\n",
    "\n",
    "    def __init__(self, data: pd.DataFrame, sequence_length: int, \n",
    "                target_column: str = 'close',\n",
    "                scaler: Optional[StandardScaler] = None,\n",
    "                include_current_price: bool = True,\n",
    "                relative_normalization: bool = True,\n",
    "                ticker: str = None):\n",
    "        \"\"\"Initialize the dataset with enhanced features and sequence normalization.\"\"\"\n",
    "        self.sequence_length = sequence_length\n",
    "        self.include_current_price = include_current_price\n",
    "        self.scaler = scaler or StandardScaler()\n",
    "        self.relative_normalization = relative_normalization\n",
    "        self.ticker = ticker\n",
    "        \n",
    "        # Define required feature columns\n",
    "        feature_cols = [\n",
    "            'close', 'volume', 'SMA_20', 'EMA_20', 'RSI', 'MA5',\n",
    "            'Bollinger_High', 'Bollinger_Low', 'MACD', 'MACD_Signal'\n",
    "        ]\n",
    "        \n",
    "        # Add enhanced features if available\n",
    "        enhanced_features = [\n",
    "            'volatility', 'trend_strength', 'body_size', 'volume_ratio', \n",
    "            'price_volume', 'price_acceleration', 'price_dist_from_mean_5d',\n",
    "            'price_dist_from_mean_20d', 'price_momentum_5d', 'price_momentum_20d',\n",
    "            'log_abs_return', 'daily_range_ratio', 'range_momentum'\n",
    "        ]\n",
    "        \n",
    "        for feat in enhanced_features:\n",
    "            if feat in data.columns:\n",
    "                feature_cols.append(feat)\n",
    "                \n",
    "        # Add market features if available\n",
    "        market_features = ['market_return', 'market_vol', 'beta', 'rs_ratio']\n",
    "        for feat in market_features:\n",
    "            if feat in data.columns:\n",
    "                feature_cols.append(feat)\n",
    "\n",
    "        # Validate feature columns\n",
    "        missing_cols = [col for col in feature_cols if col not in data.columns]\n",
    "        if missing_cols:\n",
    "            logger.warning(f\"Missing feature columns: {missing_cols}\")\n",
    "            feature_cols = [col for col in feature_cols if col in data.columns]\n",
    "            \n",
    "        # Prepare features and labels\n",
    "        self.features = data[feature_cols].values\n",
    "        self.target_col_idx = data.columns.get_loc(target_column)\n",
    "        \n",
    "        # NEW: Create more balanced target by using both next price and percent change\n",
    "        self.labels = data[target_column].shift(-1).values  # Next price\n",
    "        \n",
    "        # Store current prices if needed\n",
    "        if include_current_price:\n",
    "            self.current_prices = data[target_column].values\n",
    "\n",
    "        # Remove last row (contains NaN label)\n",
    "        self.features = self.features[:-1]\n",
    "        self.labels = self.labels[:-1]\n",
    "        if include_current_price:\n",
    "            self.current_prices = self.current_prices[:-1]\n",
    "\n",
    "        # Apply standard scaling with bias correction for up/down balance\n",
    "        self.features = self.scaler.fit_transform(self.features)\n",
    "        \n",
    "        # NEW: Apply symmetric normalization for price-related features\n",
    "        price_col_indices = [i for i, name in enumerate(feature_cols) \n",
    "                             if 'close' in name or 'price' in name or 'high' in name or 'low' in name]\n",
    "        \n",
    "        if price_col_indices and relative_normalization:\n",
    "            # Calculate relative changes for price features\n",
    "            for idx in price_col_indices:\n",
    "                price_series = self.features[:, idx]\n",
    "                price_mean = np.mean(price_series)\n",
    "                price_std = np.std(price_series)\n",
    "                \n",
    "                # Center around mean with normalized variance\n",
    "                if price_std > 0:\n",
    "                    self.features[:, idx] = (price_series - price_mean) / price_std\n",
    "        \n",
    "        # Store the original scaled features for sequence creation\n",
    "        self.scaled_features = self.features.copy()\n",
    "        \n",
    "        # Store column names for reference\n",
    "        self.feature_names = feature_cols\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Return the total number of sequences in the dataset.\"\"\"\n",
    "        return max(0, len(self.labels) - self.sequence_length)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple:\n",
    "        \"\"\"Get a single sequence and its corresponding label with relative normalization.\"\"\"\n",
    "        # Extract the sequence\n",
    "        seq = self.scaled_features[idx:idx + self.sequence_length].copy()\n",
    "        \n",
    "        # Apply relative normalization if enabled\n",
    "        if self.relative_normalization:\n",
    "            # NEW: Apply symmetrical normalization centered around zero\n",
    "            # This helps ensure up/down movements are treated equally\n",
    "            first_values = seq[0].copy().reshape(1, -1)\n",
    "            epsilon = 1e-8\n",
    "            \n",
    "            # Calculate relative changes from first value (symmetrical around 0)\n",
    "            seq = (seq - first_values) / (np.abs(first_values) + epsilon)\n",
    "        \n",
    "        y = self.labels[idx + self.sequence_length]\n",
    "        \n",
    "        if self.include_current_price:\n",
    "            current_price = self.current_prices[idx + self.sequence_length]\n",
    "            return (\n",
    "                torch.tensor(seq, dtype=torch.float32),\n",
    "                torch.tensor(y, dtype=torch.float32),\n",
    "                torch.tensor(current_price, dtype=torch.float32)\n",
    "            )\n",
    "        else:\n",
    "            return (\n",
    "                torch.tensor(seq, dtype=torch.float32),\n",
    "                torch.tensor(y, dtype=torch.float32)\n",
    "            )\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    \"\"\"Enhanced LSTM model for time series prediction with attention mechanism and stronger regularization.\"\"\"\n",
    "\n",
    "    def __init__(self, input_size: int, hidden_size: int, num_layers: int, \n",
    "                output_size: int = 1, dropout: float = 0.5):  # Increased dropout from 0.35 to 0.5\n",
    "        \"\"\"\n",
    "        Initialize an LSTM model with dropout for regularization.\n",
    "        \n",
    "        Args:\n",
    "            input_size: Number of input features\n",
    "            hidden_size: Size of hidden state in LSTM\n",
    "            num_layers: Number of LSTM layers\n",
    "            output_size: Size of output\n",
    "            dropout: Dropout probability for regularization\n",
    "        \"\"\"\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # LSTM layers with dropout\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True  # Use bidirectional LSTM for better context\n",
    "        )\n",
    "        \n",
    "        # Adjusted hidden size for bidirectional LSTM\n",
    "        bidirectional_hidden_size = hidden_size * 2\n",
    "        \n",
    "        # Add dropout layer with higher rate\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Attention mechanism with stronger regularization\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(bidirectional_hidden_size, bidirectional_hidden_size // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(dropout * 0.5),  # Add dropout in attention mechanism\n",
    "            nn.Linear(bidirectional_hidden_size // 2, 1)\n",
    "        )\n",
    "        \n",
    "        # Add batch normalization for better training stability\n",
    "        self.batch_norm1 = nn.BatchNorm1d(bidirectional_hidden_size)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(bidirectional_hidden_size // 2)\n",
    "        \n",
    "        # Price variance prediction for dynamic range\n",
    "        self.fc_variance = nn.Linear(bidirectional_hidden_size, 1)\n",
    "        \n",
    "        # Main prediction branch\n",
    "        self.fc1 = nn.Linear(bidirectional_hidden_size, bidirectional_hidden_size // 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(bidirectional_hidden_size // 2, output_size)\n",
    "        \n",
    "        # Output transformation with stronger regularization\n",
    "        self.output_transform = nn.Sequential(\n",
    "            nn.Linear(2, 32),  # Increased from 16 to 32 nodes\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),   # Add dropout in output transformation\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, output_size)\n",
    "        )\n",
    "        \n",
    "        # Weight initialization for better convergence\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initialize weights for faster convergence.\"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "        elif isinstance(module, nn.LSTM):\n",
    "            for name, param in module.named_parameters():\n",
    "                if 'weight_ih' in name:\n",
    "                    nn.init.xavier_uniform_(param)\n",
    "                elif 'weight_hh' in name:\n",
    "                    nn.init.orthogonal_(param)\n",
    "                elif 'bias' in name:\n",
    "                    nn.init.constant_(param, 0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass through the network.\"\"\"\n",
    "        # Initialize hidden state with zeros\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(self.num_layers * 2, batch_size, self.hidden_size).to(x.device)  # *2 for bidirectional\n",
    "        c0 = torch.zeros(self.num_layers * 2, batch_size, self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Apply attention\n",
    "        attn_weights = F.softmax(self.attention(out).squeeze(-1), dim=1)\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), out).squeeze(1)\n",
    "        \n",
    "        # Apply batch normalization\n",
    "        if batch_size > 1:  # Batch norm requires more than 1 sample\n",
    "            context = self.batch_norm1(context)\n",
    "        \n",
    "        # Apply dropout to the attention output\n",
    "        context = self.dropout(context)\n",
    "        \n",
    "        # Base prediction\n",
    "        hidden = self.fc1(context)\n",
    "        hidden = self.relu(hidden)\n",
    "        if batch_size > 1:\n",
    "            hidden = self.batch_norm2(hidden)\n",
    "        hidden = self.dropout(hidden)  # Additional dropout\n",
    "        base_pred = self.fc2(hidden)\n",
    "        \n",
    "        # Predict variance (confidence)\n",
    "        pred_variance = torch.exp(self.fc_variance(context))\n",
    "        \n",
    "        # Combine for final prediction\n",
    "        combined = torch.cat((base_pred, pred_variance), dim=1)\n",
    "        final_output = self.output_transform(combined)\n",
    "        \n",
    "        return final_output\n",
    "\n",
    "class EnsembleModel:\n",
    "    \"\"\"\n",
    "    Ensemble of multiple LSTM models for more robust predictions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, models: List[nn.Module], weights: Optional[List[float]] = None):\n",
    "        \"\"\"\n",
    "        Initialize ensemble with multiple models and optional weights.\n",
    "        \n",
    "        Args:\n",
    "            models: List of trained PyTorch models\n",
    "            weights: Optional weights for each model (defaults to equal weights)\n",
    "        \"\"\"\n",
    "        self.models = models\n",
    "        \n",
    "        # Default to equal weights if not provided\n",
    "        if weights is None:\n",
    "            self.weights = [1.0 / len(models)] * len(models)\n",
    "        else:\n",
    "            # Normalize weights to sum to 1\n",
    "            total = sum(weights)\n",
    "            self.weights = [w / total for w in weights]\n",
    "            \n",
    "        if len(self.weights) != len(self.models):\n",
    "            raise ValueError(\"Number of weights must match number of models\")\n",
    "        \n",
    "    def predict(self, data_loader: DataLoader, device: torch.device) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Generate ensemble predictions by weighted averaging.\n",
    "        \n",
    "        Args:\n",
    "            data_loader: DataLoader with test data\n",
    "            device: Computation device (CPU/GPU)\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of true values and ensemble predictions\n",
    "        \"\"\"\n",
    "        all_predictions = []\n",
    "        true_values = None\n",
    "        current_prices = None\n",
    "        \n",
    "        # Get predictions from each model\n",
    "        for model in self.models:\n",
    "            model.eval()\n",
    "            model = model.to(device)\n",
    "            \n",
    "            predictions = []\n",
    "            y_true_list = []\n",
    "            current_prices_list = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch in data_loader:\n",
    "                    if len(batch) == 3:  # Includes current prices\n",
    "                        batch_x, batch_y, batch_current_prices = batch\n",
    "                        batch_x = batch_x.to(device)\n",
    "                        outputs = model(batch_x).cpu().numpy()\n",
    "                        predictions.extend(outputs.flatten())\n",
    "                        y_true_list.extend(batch_y.numpy())\n",
    "                        current_prices_list.extend(batch_current_prices.numpy())\n",
    "                    else:\n",
    "                        batch_x, batch_y = batch\n",
    "                        batch_x = batch_x.to(device)\n",
    "                        outputs = model(batch_x).cpu().numpy()\n",
    "                        predictions.extend(outputs.flatten())\n",
    "                        y_true_list.extend(batch_y.numpy())\n",
    "            \n",
    "            all_predictions.append(np.array(predictions))\n",
    "            \n",
    "            # Store true values and current prices (same for all models)\n",
    "            if true_values is None:\n",
    "                true_values = np.array(y_true_list)\n",
    "            if current_prices is None and current_prices_list:\n",
    "                current_prices = np.array(current_prices_list)\n",
    "        \n",
    "        # Compute weighted ensemble predictions\n",
    "        ensemble_pred = np.zeros_like(all_predictions[0])\n",
    "        for i, pred in enumerate(all_predictions):\n",
    "            ensemble_pred += pred * self.weights[i]\n",
    "            \n",
    "        return true_values, ensemble_pred, current_prices\n",
    "    \n",
    "    def evaluate(self, data_loader: DataLoader, device: torch.device, \n",
    "                evaluator: Optional[ModelEvaluator] = None,\n",
    "                ticker: str = None) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluate ensemble performance with detailed metrics.\n",
    "        \n",
    "        Args:\n",
    "            data_loader: DataLoader with test data\n",
    "            device: Computation device (CPU/GPU)\n",
    "            evaluator: Optional ModelEvaluator for metrics calculation\n",
    "            ticker: Optional stock ticker symbol\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of evaluation metrics\n",
    "        \"\"\"\n",
    "        # Get ensemble predictions\n",
    "        y_true, y_pred, current_prices = self.predict(data_loader, device)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        if evaluator is None:\n",
    "            evaluator = ModelEvaluator()\n",
    "            \n",
    "        # Apply adaptive correction if ticker is provided\n",
    "        if ticker is not None:\n",
    "            correction = evaluator.get_adaptive_correction(ticker)\n",
    "            if correction != 0:\n",
    "                logger.info(f\"Applying adaptive correction of {correction:.4f} for {ticker}\")\n",
    "                y_pred = y_pred - correction\n",
    "        \n",
    "        # Apply reasonable range clipping if current prices are available\n",
    "        if current_prices is not None:\n",
    "            # Calculate reasonable prediction bounds\n",
    "            max_pct_change = 0.015  # 1.5% maximum change per minute\n",
    "            min_bound = current_prices * (1 - max_pct_change)\n",
    "            max_bound = current_prices * (1 + max_pct_change)\n",
    "            \n",
    "            # Clip predictions to reasonable range\n",
    "            y_pred = np.clip(y_pred, min_bound, max_bound)\n",
    "            \n",
    "        # Calculate and return metrics\n",
    "        metrics = evaluator.calculate_metrics(\n",
    "            y_true=y_true,\n",
    "            y_pred=y_pred,\n",
    "            current_prices=current_prices,\n",
    "            ticker=ticker\n",
    "        )\n",
    "        \n",
    "        return metrics, y_true, y_pred, current_prices\n",
    "\n",
    "def train_ensemble_models(\n",
    "    data_df: pd.DataFrame,\n",
    "    model_configs: List[Dict],\n",
    "    device: torch.device,\n",
    "    sequence_length: int = 60,\n",
    "    train_ratio: float = 0.7,\n",
    "    val_ratio: float = 0.15,\n",
    "    batch_size: int = 64,\n",
    "    ticker: str = None) -> EnsembleModel:\n",
    "    \"\"\"\n",
    "    Train multiple models with different configurations for ensemble.\n",
    "    \n",
    "    Args:\n",
    "        data_df: DataFrame with features and target\n",
    "        model_configs: List of model configuration dictionaries\n",
    "        device: Computation device (CPU/GPU)\n",
    "        sequence_length: LSTM sequence length\n",
    "        train_ratio: Ratio of data to use for training\n",
    "        val_ratio: Ratio of data to use for validation\n",
    "        batch_size: Batch size for training\n",
    "        ticker: Optional stock ticker symbol\n",
    "        \n",
    "    Returns:\n",
    "        Trained EnsembleModel object\n",
    "    \"\"\"\n",
    "    # Prepare dataset\n",
    "    dataset = HFTDataset(\n",
    "        data=data_df,\n",
    "        sequence_length=sequence_length,\n",
    "        include_current_price=True,\n",
    "        ticker=ticker\n",
    "    )\n",
    "    \n",
    "    # Split data\n",
    "    train_size = int(train_ratio * len(dataset))\n",
    "    val_size = int(val_ratio * len(dataset))\n",
    "    test_size = len(dataset) - train_size - val_size\n",
    "    \n",
    "    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "        dataset, [train_size, val_size, test_size]\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # Train models\n",
    "    trained_models = []\n",
    "    validation_scores = []\n",
    "    \n",
    "    for i, config in enumerate(model_configs):\n",
    "        logger.info(f\"Training model {i+1}/{len(model_configs)}\")\n",
    "        logger.info(f\"Configuration: {config}\")\n",
    "        \n",
    "        # Initialize model\n",
    "        input_size = len(dataset.feature_names)\n",
    "        model = LSTMModel(\n",
    "            input_size=input_size,\n",
    "            hidden_size=config['hidden_size'],\n",
    "            num_layers=config['num_layers'],\n",
    "            dropout=config['dropout']\n",
    "        )\n",
    "        \n",
    "        # Initialize training components\n",
    "        criterion = DirectionalPredictionLoss(\n",
    "            direction_weight=config.get('direction_weight', 2.5),\n",
    "            magnitude_weight=config.get('magnitude_weight', 0.8),\n",
    "            short_penalty_multiplier=config.get('short_penalty_multiplier', 3.5),\n",
    "            bias_correction_weight=config.get('bias_correction_weight', 1.5)\n",
    "        )\n",
    "        \n",
    "        if ticker:\n",
    "            criterion.set_ticker(ticker)\n",
    "        \n",
    "        optimizer = optim.Adam(\n",
    "            model.parameters(), \n",
    "            lr=config.get('learning_rate', 0.001),\n",
    "            weight_decay=config.get('weight_decay', 1e-2)\n",
    "        )\n",
    "        \n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, \n",
    "            mode='min', \n",
    "            factor=config.get('lr_factor', 0.3),\n",
    "            patience=config.get('lr_patience', 2),\n",
    "            verbose=True,\n",
    "            min_lr=config.get('min_lr', 1e-6)\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        training_history = TrainingHistory()\n",
    "        model = train_model(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            num_epochs=config.get('num_epochs', 100),\n",
    "            device=device,\n",
    "            training_history=training_history,\n",
    "            early_stopping_patience=config.get('early_stopping_patience', 15),\n",
    "            scheduler=scheduler,\n",
    "            ticker=ticker\n",
    "        )\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        evaluator = ModelEvaluator()\n",
    "        y_true, y_pred, current_prices = evaluate_model(\n",
    "            model, \n",
    "            val_loader, \n",
    "            device,\n",
    "            return_predictions=True,\n",
    "            evaluator=evaluator,\n",
    "            ticker=ticker\n",
    "        )\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = evaluator.calculate_metrics(\n",
    "            y_true=y_true,\n",
    "            y_pred=y_pred,\n",
    "            current_prices=current_prices,\n",
    "            ticker=ticker\n",
    "        )\n",
    "        \n",
    "        # Store model and validation score\n",
    "        trained_models.append(model)\n",
    "        validation_scores.append(metrics.get('direction_accuracy', 0))\n",
    "        \n",
    "        logger.info(f\"Model {i+1} validation direction accuracy: {validation_scores[-1]:.2f}%\")\n",
    "    \n",
    "    # Create ensemble with weights based on validation scores\n",
    "    # Convert validation scores to weights, giving more weight to better models\n",
    "    weights = [max(0.1, score - 49) for score in validation_scores]  # Minimum weight of 0.1\n",
    "    \n",
    "    ensemble = EnsembleModel(trained_models, weights)\n",
    "    \n",
    "    # Evaluate ensemble\n",
    "    ensemble_metrics, _, _, _ = ensemble.evaluate(test_loader, device, ticker=ticker)\n",
    "    logger.info(f\"Ensemble direction accuracy: {ensemble_metrics.get('direction_accuracy', 0):.2f}%\")\n",
    "    \n",
    "    return ensemble\n",
    "\n",
    "class MultiTickerMonitor:\n",
    "    \"\"\"Monitor multiple tickers and generate trading signals.\"\"\"\n",
    "\n",
    "    def __init__(self, signal_generator: AdaptiveSignalGenerator):\n",
    "        self.tracked_tickers = {}\n",
    "        self.signal_generator = signal_generator\n",
    "        \n",
    "        # Ticker-specific direction confidence - balance up/down confidence values\n",
    "        self.ticker_confidence = {\n",
    "            'MSFT': {'up': 0.75, 'down': 0.75},  # Balanced\n",
    "            'GOOGL': {'up': 0.75, 'down': 0.75},  # Balanced\n",
    "            'NVDA': {'up': 0.75, 'down': 0.75},   # Balanced\n",
    "            'TSLA': {'up': 0.75, 'down': 0.75},   # Balanced\n",
    "            'TQQQ': {'up': 0.70, 'down': 0.80},   # Slightly favor down predictions for volatile ETF\n",
    "            'QQQ': {'up': 0.75, 'down': 0.75},    # Balanced\n",
    "        }\n",
    "        # Default confidence levels - equal for both directions\n",
    "        self.default_confidence = {'up': 0.75, 'down': 0.75}\n",
    "\n",
    "    def add_ticker(self, ticker: str, initial_price: float):\n",
    "        \"\"\"Add a new ticker to monitor.\"\"\"\n",
    "        self.tracked_tickers[ticker] = {\n",
    "            'current_price': initial_price,\n",
    "            'signals': [],\n",
    "            'last_update': datetime.now()\n",
    "        }\n",
    "\n",
    "    def update_ticker(self, ticker: str, current_price: float,\n",
    "                     predicted_price: float, timestamp: pd.Timestamp,\n",
    "                     market_data: pd.DataFrame,\n",
    "                     order_book_data: Dict = None,\n",
    "                     prediction_metrics: Dict = None) -> Optional[Dict]:\n",
    "        \"\"\"Update ticker information and generate signals.\"\"\"\n",
    "        if ticker not in self.tracked_tickers:\n",
    "            self.add_ticker(ticker, current_price)\n",
    "    \n",
    "        self.tracked_tickers[ticker]['current_price'] = current_price\n",
    "        self.tracked_tickers[ticker]['last_update'] = timestamp\n",
    "        \n",
    "        # Get prediction confidence for this ticker\n",
    "        price_change = predicted_price - current_price\n",
    "        direction = 'up' if price_change > 0 else 'down'\n",
    "        \n",
    "        # Use stored confidence or default\n",
    "        confidence = self.ticker_confidence.get(ticker, self.default_confidence)\n",
    "        \n",
    "        # Update confidence if metrics provided\n",
    "        if prediction_metrics:\n",
    "            if 'up_direction_accuracy' in prediction_metrics:\n",
    "                confidence['up'] = 0.5 + (prediction_metrics['up_direction_accuracy'] / 200)\n",
    "            if 'down_direction_accuracy' in prediction_metrics:\n",
    "                confidence['down'] = 0.5 + (prediction_metrics['down_direction_accuracy'] / 200)\n",
    "    \n",
    "        signal = self.signal_generator.generate_signals(\n",
    "            ticker=ticker,\n",
    "            current_price=current_price,\n",
    "            predicted_price=predicted_price,\n",
    "            timestamp=timestamp,\n",
    "            market_data=market_data,\n",
    "            order_book_data=order_book_data,\n",
    "            prediction_confidence=confidence\n",
    "        )\n",
    "    \n",
    "        if signal and signal['action']:\n",
    "            self.tracked_tickers[ticker]['signals'].append(signal)\n",
    "            return signal\n",
    "    \n",
    "        return None\n",
    "\n",
    "    def get_signals(self, ticker: str) -> List[Dict]:\n",
    "        \"\"\"Retrieve signals for a specific ticker.\"\"\"\n",
    "        return self.tracked_tickers.get(ticker, {}).get('signals', [])\n",
    "\n",
    "def apply_dynamic_scaling(model: LSTMModel, train_loader: DataLoader, device: torch.device):\n",
    "    \"\"\"Apply dynamic output scaling based on target distribution.\"\"\"\n",
    "    price_ranges = []\n",
    "    with torch.no_grad():\n",
    "        for batch in train_loader:\n",
    "            if len(batch) == 3:\n",
    "                _, batch_y, _ = batch\n",
    "            else:\n",
    "                _, batch_y = batch\n",
    "            price_ranges.append((batch_y.min().item(), batch_y.max().item()))\n",
    "    \n",
    "    # Calculate global min and max\n",
    "    global_min = min([r[0] for r in price_ranges])\n",
    "    global_max = max([r[1] for r in price_ranges])\n",
    "    price_range = global_max - global_min\n",
    "    \n",
    "    # Set model's output transformation parameters\n",
    "    if hasattr(model, 'output_transform'):\n",
    "        # Initialize last layer to produce outputs in appropriate range\n",
    "        last_layer = model.output_transform[-1]\n",
    "        with torch.no_grad():\n",
    "            # Scale the last layer weights to produce wider range\n",
    "            current_range = last_layer.weight.abs().mean().item()\n",
    "            target_range = price_range / 10  # Aim for 10% of full range\n",
    "            scale_factor = target_range / (current_range + 1e-8)\n",
    "            last_layer.weight.mul_(scale_factor)\n",
    "    \n",
    "    logger.info(f\"Applied dynamic scaling with price range: {price_range:.2f}, scale factor: {scale_factor:.4f}\")\n",
    "\n",
    "def train_model(model: nn.Module, \n",
    "              train_loader: DataLoader, \n",
    "              val_loader: DataLoader, \n",
    "              criterion: nn.Module, \n",
    "              optimizer: torch.optim.Optimizer, \n",
    "              num_epochs: int, \n",
    "              device: torch.device,\n",
    "              training_history: Optional[TrainingHistory] = None,\n",
    "              early_stopping_patience: int = 10,\n",
    "              scheduler: Optional[torch.optim.lr_scheduler._LRScheduler] = None,\n",
    "              ticker: str = None) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Train the LSTM model with enhanced monitoring and early stopping.\n",
    "    \n",
    "    Args:\n",
    "        model: The LSTM model to train\n",
    "        train_loader: DataLoader for training data\n",
    "        val_loader: DataLoader for validation data\n",
    "        criterion: Loss function\n",
    "        optimizer: Optimization algorithm\n",
    "        num_epochs: Number of training epochs\n",
    "        device: Computation device (CPU/GPU)\n",
    "        training_history: Optional history tracker\n",
    "        early_stopping_patience: Number of epochs to wait before early stopping\n",
    "        scheduler: Optional learning rate scheduler\n",
    "        ticker: Optional ticker symbol for ticker-specific adaptations\n",
    "        \n",
    "    Returns:\n",
    "        Trained model\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    # Apply dynamic scaling for output range\n",
    "    apply_dynamic_scaling(model, train_loader, device)\n",
    "    \n",
    "    # Set ticker for directional loss if applicable\n",
    "    if ticker and isinstance(criterion, DirectionalPredictionLoss):\n",
    "        criterion.set_ticker(ticker)\n",
    "        logger.info(f\"Set ticker '{ticker}' for directional loss\")\n",
    "\n",
    "    # Check if dataset provides current price for directional loss\n",
    "    sample_batch = next(iter(train_loader))\n",
    "    has_current_price = len(sample_batch) == 3\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        direction_accuracies = []\n",
    "        long_accuracies = []\n",
    "        short_accuracies = []\n",
    "\n",
    "        for batch in train_loader:\n",
    "            if has_current_price:\n",
    "                batch_x, batch_y, current_prices = batch\n",
    "                batch_x = batch_x.to(device)\n",
    "                batch_y = batch_y.to(device).unsqueeze(1)\n",
    "                current_prices = current_prices.to(device).unsqueeze(1)\n",
    "            else:\n",
    "                batch_x, batch_y = batch\n",
    "                batch_x = batch_x.to(device)\n",
    "                batch_y = batch_y.to(device).unsqueeze(1)\n",
    "                current_prices = None\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            \n",
    "            # Calculate loss using current prices if available\n",
    "            if has_current_price and isinstance(criterion, DirectionalPredictionLoss):\n",
    "                loss = criterion(outputs, batch_y, current_prices)\n",
    "            else:\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                \n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "            \n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "            # Calculate direction accuracy\n",
    "            if has_current_price:\n",
    "                with torch.no_grad():\n",
    "                    true_direction = torch.sign(batch_y - current_prices)\n",
    "                    pred_direction = torch.sign(outputs - current_prices)\n",
    "                    \n",
    "                    # Overall direction accuracy\n",
    "                    direction_match = (true_direction == pred_direction).float()\n",
    "                    direction_accuracy = direction_match.mean().item() * 100\n",
    "                    direction_accuracies.append(direction_accuracy)\n",
    "                    \n",
    "                    # Long accuracy (when true direction is up)\n",
    "                    long_mask = (true_direction > 0).squeeze()\n",
    "                    if long_mask.any():\n",
    "                        long_accuracy = direction_match[long_mask].mean().item() * 100\n",
    "                        long_accuracies.append(long_accuracy)\n",
    "                        \n",
    "                    # Short accuracy (when true direction is down)\n",
    "                    short_mask = (true_direction < 0).squeeze()\n",
    "                    if short_mask.any():\n",
    "                        short_accuracy = direction_match[short_mask].mean().item() * 100\n",
    "                        short_accuracies.append(short_accuracy)\n",
    "\n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "        avg_direction_accuracy = np.mean(direction_accuracies) if direction_accuracies else None\n",
    "        avg_long_accuracy = np.mean(long_accuracies) if long_accuracies else None\n",
    "        avg_short_accuracy = np.mean(short_accuracies) if short_accuracies else None\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        val_direction_accuracies = []\n",
    "        val_long_accuracies = []\n",
    "        val_short_accuracies = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                if has_current_price:\n",
    "                    batch_x, batch_y, current_prices = batch\n",
    "                    batch_x = batch_x.to(device)\n",
    "                    batch_y = batch_y.to(device).unsqueeze(1)\n",
    "                    current_prices = current_prices.to(device).unsqueeze(1)\n",
    "                else:\n",
    "                    batch_x, batch_y = batch\n",
    "                    batch_x = batch_x.to(device)\n",
    "                    batch_y = batch_y.to(device).unsqueeze(1)\n",
    "                    current_prices = None\n",
    "\n",
    "                outputs = model(batch_x)\n",
    "                \n",
    "                # Calculate loss\n",
    "                if has_current_price and isinstance(criterion, DirectionalPredictionLoss):\n",
    "                    loss = criterion(outputs, batch_y, current_prices)\n",
    "                else:\n",
    "                    loss = criterion(outputs, batch_y)\n",
    "                    \n",
    "                val_losses.append(loss.item())\n",
    "                \n",
    "                # Calculate direction accuracy\n",
    "                if has_current_price:\n",
    "                    true_direction = torch.sign(batch_y - current_prices)\n",
    "                    pred_direction = torch.sign(outputs - current_prices)\n",
    "                    \n",
    "                    # Overall direction accuracy\n",
    "                    direction_match = (true_direction == pred_direction).float()\n",
    "                    direction_accuracy = direction_match.mean().item() * 100\n",
    "                    val_direction_accuracies.append(direction_accuracy)\n",
    "                    \n",
    "                    # Long accuracy\n",
    "                    long_mask = (true_direction > 0).squeeze()\n",
    "                    if long_mask.any():\n",
    "                        long_accuracy = direction_match[long_mask].mean().item() * 100\n",
    "                        val_long_accuracies.append(long_accuracy)\n",
    "                        \n",
    "                    # Short accuracy\n",
    "                    short_mask = (true_direction < 0).squeeze()\n",
    "                    if short_mask.any():\n",
    "                        short_accuracy = direction_match[short_mask].mean().item() * 100\n",
    "                        val_short_accuracies.append(short_accuracy)\n",
    "\n",
    "        avg_val_loss = np.mean(val_losses)\n",
    "        avg_val_direction_accuracy = np.mean(val_direction_accuracies) if val_direction_accuracies else None\n",
    "        avg_val_long_accuracy = np.mean(val_long_accuracies) if val_long_accuracies else None\n",
    "        avg_val_short_accuracy = np.mean(val_short_accuracies) if val_short_accuracies else None\n",
    "\n",
    "        # Update training history\n",
    "        if training_history:\n",
    "            training_history.update(\n",
    "                epoch, \n",
    "                avg_train_loss, \n",
    "                avg_val_loss,\n",
    "                avg_direction_accuracy,\n",
    "                avg_long_accuracy,\n",
    "                avg_short_accuracy\n",
    "            )\n",
    "\n",
    "        # Learning rate scheduler step\n",
    "        if scheduler:\n",
    "            if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                scheduler.step(avg_val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "\n",
    "        # Early stopping check\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            # Save best model state\n",
    "            best_model_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                logger.info(f\"Early stopping triggered at epoch {epoch}\")\n",
    "                break\n",
    "\n",
    "        # Logging with direction-specific metrics\n",
    "        log_msg = f\"Epoch [{epoch}/{num_epochs}], Train Loss: {avg_train_loss:.6f}, Val Loss: {avg_val_loss:.6f}\"\n",
    "        \n",
    "        if avg_direction_accuracy is not None:\n",
    "            log_msg += f\", Direction Acc: {avg_direction_accuracy:.2f}%\"\n",
    "            \n",
    "        if avg_long_accuracy is not None and avg_short_accuracy is not None:\n",
    "            log_msg += f\", Long/Short Acc: {avg_long_accuracy:.2f}%/{avg_short_accuracy:.2f}%\"\n",
    "            \n",
    "        logger.info(log_msg)\n",
    "\n",
    "    # Restore best model\n",
    "    if best_model_state:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        model = model.to(device)\n",
    "        \n",
    "    return model\n",
    "\n",
    "def evaluate_model(model: nn.Module, \n",
    "                  test_loader: DataLoader, \n",
    "                  device: torch.device,\n",
    "                  return_predictions: bool = False,\n",
    "                  evaluator: Optional[ModelEvaluator] = None,\n",
    "                  ticker: str = None) -> Tuple:\n",
    "    \"\"\"Evaluate the trained model on test data with enhanced metrics and range clipping.\"\"\"\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    current_prices = []\n",
    "\n",
    "    # Check if dataset provides current price\n",
    "    sample_batch = next(iter(test_loader))\n",
    "    has_current_price = len(sample_batch) == 3\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            if has_current_price:\n",
    "                batch_x, batch_y, batch_current = batch\n",
    "                batch_x = batch_x.to(device)\n",
    "                outputs = model(batch_x).cpu().numpy()\n",
    "                y_true.extend(batch_y.numpy())\n",
    "                y_pred.extend(outputs.flatten())\n",
    "                current_prices.extend(batch_current.numpy())\n",
    "            else:\n",
    "                batch_x, batch_y = batch\n",
    "                batch_x = batch_x.to(device)\n",
    "                outputs = model(batch_x).cpu().numpy()\n",
    "                y_true.extend(batch_y.numpy())\n",
    "                y_pred.extend(outputs.flatten())\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    # Apply adaptive correction if evaluator is provided\n",
    "    if evaluator is not None and ticker is not None:\n",
    "        correction = evaluator.get_adaptive_correction(ticker)\n",
    "        if correction != 0:\n",
    "            logger.info(f\"Applying adaptive correction of {correction:.4f} for {ticker}\")\n",
    "            y_pred = y_pred - correction\n",
    "    \n",
    "    if has_current_price:\n",
    "        current_prices = np.array(current_prices)\n",
    "        \n",
    "        # Calculate prediction range statistics before clipping\n",
    "        pred_range_before = np.max(y_pred) - np.min(y_pred)\n",
    "        true_range = np.max(y_true) - np.min(y_true)\n",
    "        range_ratio_before = pred_range_before / true_range if true_range > 0 else 0\n",
    "        \n",
    "        logger.info(f\"Prediction range before clipping - True range: {true_range:.2f}, \" \n",
    "                    f\"Predicted range: {pred_range_before:.2f}, Ratio: {range_ratio_before:.2f}\")\n",
    "        \n",
    "        # MODIFIED: Enforce strict range clipping to realistic minute-by-minute changes (±1.5%)\n",
    "        max_pct_change = 0.015  # 1.5% maximum change per minute (strict limit)\n",
    "        \n",
    "        # Calculate reasonable prediction bounds\n",
    "        min_bound = current_prices * (1 - max_pct_change)\n",
    "        max_bound = current_prices * (1 + max_pct_change)\n",
    "        \n",
    "        # Clip predictions to reasonable range\n",
    "        y_pred_before = y_pred.copy()  # Save original predictions\n",
    "        y_pred = np.clip(y_pred, min_bound, max_bound)\n",
    "        \n",
    "        # Calculate how many predictions were clipped\n",
    "        num_clipped = np.sum((y_pred != y_pred_before))\n",
    "        pct_clipped = (num_clipped / len(y_pred)) * 100\n",
    "        \n",
    "        # Calculate average clipping amount\n",
    "        if num_clipped > 0:\n",
    "            avg_clip_amount = np.mean(np.abs(y_pred - y_pred_before)[y_pred != y_pred_before])\n",
    "            avg_clip_pct = avg_clip_amount / np.mean(current_prices) * 100\n",
    "        else:\n",
    "            avg_clip_amount = 0\n",
    "            avg_clip_pct = 0\n",
    "        \n",
    "        # Calculate prediction range statistics after clipping\n",
    "        pred_range_after = np.max(y_pred) - np.min(y_pred)\n",
    "        range_ratio_after = pred_range_after / true_range if true_range > 0 else 0\n",
    "        \n",
    "        logger.info(f\"Prediction range after clipping - Predicted range: {pred_range_after:.2f}, Ratio: {range_ratio_after:.2f}\")\n",
    "        logger.info(f\"Clipped {num_clipped} predictions ({pct_clipped:.2f}%), average clip amount: {avg_clip_amount:.4f} ({avg_clip_pct:.4f}%)\")\n",
    "    \n",
    "        # Calculate prediction bias metrics before and after clipping\n",
    "        pred_direction_before = np.sign(y_pred_before - current_prices)\n",
    "        up_pred_pct_before = np.mean(pred_direction_before > 0) * 100\n",
    "        down_pred_pct_before = np.mean(pred_direction_before < 0) * 100\n",
    "        \n",
    "        pred_direction_after = np.sign(y_pred - current_prices)\n",
    "        up_pred_pct_after = np.mean(pred_direction_after > 0) * 100\n",
    "        down_pred_pct_after = np.mean(pred_direction_after < 0) * 100\n",
    "        \n",
    "        logger.info(f\"Direction bias before clipping - Up: {up_pred_pct_before:.2f}%, Down: {down_pred_pct_before:.2f}%\")\n",
    "        logger.info(f\"Direction bias after clipping - Up: {up_pred_pct_after:.2f}%, Down: {down_pred_pct_after:.2f}%\")\n",
    "    \n",
    "    # Add range clipping to the MultiTickerMonitor.update_ticker method\n",
    "    if return_predictions:\n",
    "        if has_current_price:\n",
    "            return y_true, y_pred, current_prices\n",
    "        else:\n",
    "            return y_true, y_pred, None\n",
    "    else:\n",
    "        if has_current_price:\n",
    "            return y_true, y_pred, current_prices\n",
    "        else:\n",
    "            return y_true, y_pred, None\n",
    "\n",
    "def update_ticker(self, ticker: str, current_price: float,\n",
    "                 predicted_price: float, timestamp: pd.Timestamp,\n",
    "                 market_data: pd.DataFrame,\n",
    "                 order_book_data: Dict = None,\n",
    "                 prediction_metrics: Dict = None) -> Optional[Dict]:\n",
    "    \"\"\"Update ticker information and generate signals with range clipping.\"\"\"\n",
    "    if ticker not in self.tracked_tickers:\n",
    "        self.add_ticker(ticker, current_price)\n",
    "\n",
    "    self.tracked_tickers[ticker]['current_price'] = current_price\n",
    "    self.tracked_tickers[ticker]['last_update'] = timestamp\n",
    "    \n",
    "    # Get prediction confidence for this ticker\n",
    "    price_change = predicted_price - current_price\n",
    "    direction = 'up' if price_change > 0 else 'down'\n",
    "    \n",
    "    # Use stored confidence or default\n",
    "    confidence = self.ticker_confidence.get(ticker, self.default_confidence)\n",
    "    \n",
    "    # Update confidence if metrics provided\n",
    "    if prediction_metrics:\n",
    "        if 'up_direction_accuracy' in prediction_metrics:\n",
    "            confidence['up'] = 0.5 + (prediction_metrics['up_direction_accuracy'] / 200)\n",
    "        if 'down_direction_accuracy' in prediction_metrics:\n",
    "            confidence['down'] = 0.5 + (prediction_metrics['down_direction_accuracy'] / 200)\n",
    "    \n",
    "    # MODIFIED: Apply range clipping to predicted price\n",
    "    max_pct_change = 0.015  # 1.5% maximum change per minute\n",
    "    min_bound = current_price * (1 - max_pct_change)\n",
    "    max_bound = current_price * (1 + max_pct_change)\n",
    "    \n",
    "    original_prediction = predicted_price\n",
    "    clipped_prediction = np.clip(predicted_price, min_bound, max_bound)\n",
    "    \n",
    "    # Log if clipping was applied\n",
    "    if clipped_prediction != original_prediction:\n",
    "        clipping_amount = abs(clipped_prediction - original_prediction)\n",
    "        clipping_pct = (clipping_amount / current_price) * 100\n",
    "        logger.info(f\"Prediction clipped for {ticker}: {original_prediction:.4f} -> {clipped_prediction:.4f} ({clipping_pct:.4f}%)\")\n",
    "    \n",
    "    # Use clipped prediction for signal generation\n",
    "    signal = self.signal_generator.generate_signals(\n",
    "        ticker=ticker,\n",
    "        current_price=current_price,\n",
    "        predicted_price=clipped_prediction,\n",
    "        timestamp=timestamp,\n",
    "        market_data=market_data,\n",
    "        order_book_data=order_book_data,\n",
    "        prediction_confidence=confidence\n",
    "    )\n",
    "\n",
    "    if signal and signal['action']:\n",
    "        self.tracked_tickers[ticker]['signals'].append(signal)\n",
    "        return signal\n",
    "\n",
    "    return None\n",
    "\n",
    "def backtest_trades_with_costs(signals: List[Dict], df: pd.DataFrame, \n",
    "                             commission_rate: float = 0.001, \n",
    "                             slippage_factor: float = 0.0002) -> Tuple[List[Dict], float, float]:\n",
    "    \"\"\"\n",
    "    Execute backtesting with realistic transaction costs and slippage.\n",
    "    \n",
    "    Args:\n",
    "        signals: List of trading signals\n",
    "        df: DataFrame with market data\n",
    "        commission_rate: Trading commission as a percentage\n",
    "        slippage_factor: Price slippage as a percentage\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of trades list, total commission, and total slippage\n",
    "    \"\"\"\n",
    "    trades = []\n",
    "    entry_signal = None\n",
    "    total_commission = 0\n",
    "    total_slippage = 0\n",
    "    \n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "\n",
    "    for signal in signals:\n",
    "        signal_time = pd.to_datetime(signal['timestamp'])\n",
    "        \n",
    "        # Apply slippage to entry and exit prices\n",
    "        price_with_slippage = signal['price']\n",
    "        slippage_amount = 0\n",
    "        \n",
    "        if signal['action'].startswith('enter_long') or signal['action'].startswith('exit_short'):\n",
    "            # Buy operations - price moves against us (higher)\n",
    "            slippage_amount = signal['price'] * slippage_factor\n",
    "            price_with_slippage = signal['price'] + slippage_amount\n",
    "        elif signal['action'].startswith('enter_short') or signal['action'].startswith('exit_long'):\n",
    "            # Sell operations - price moves against us (lower)\n",
    "            slippage_amount = signal['price'] * slippage_factor\n",
    "            price_with_slippage = signal['price'] - slippage_amount\n",
    "        \n",
    "        # Track slippage\n",
    "        total_slippage += slippage_amount\n",
    "        \n",
    "        # Calculate commission\n",
    "        commission = price_with_slippage * commission_rate\n",
    "        total_commission += commission\n",
    "        \n",
    "        # Process entry signals\n",
    "        if signal['action'].startswith('enter_'):\n",
    "            if entry_signal is None:\n",
    "                # Use position_size from signal if available\n",
    "                position_size = signal.get('position_size', 1.0)\n",
    "                \n",
    "                entry_signal = {\n",
    "                    'timestamp': signal_time,\n",
    "                    'price': price_with_slippage,\n",
    "                    'action': signal['action'],\n",
    "                    'commission': commission,\n",
    "                    'slippage': slippage_amount,\n",
    "                    'position_size': position_size\n",
    "                }\n",
    "        \n",
    "        # Process exit signals\n",
    "        elif signal['action'].startswith('exit_') and entry_signal is not None:\n",
    "            exit_reason = signal.get('exit_reason', 'unspecified')\n",
    "            duration = (signal_time - entry_signal['timestamp']).total_seconds() / 60\n",
    "            \n",
    "            # Calculate profit/loss including transaction costs\n",
    "            total_trade_commission = entry_signal['commission'] + commission\n",
    "            total_trade_slippage = entry_signal['slippage'] + slippage_amount\n",
    "            position_size = entry_signal.get('position_size', 1.0)\n",
    "            \n",
    "            if entry_signal['action'] == 'enter_long':\n",
    "                profit = (price_with_slippage - entry_signal['price'] - total_trade_commission) * position_size\n",
    "            else:  # Short trade\n",
    "                profit = (entry_signal['price'] - price_with_slippage - total_trade_commission) * position_size\n",
    "            \n",
    "            trade = {\n",
    "                'entry_time': entry_signal['timestamp'],\n",
    "                'entry_price': entry_signal['price'],\n",
    "                'exit_time': signal_time,\n",
    "                'exit_price': price_with_slippage,\n",
    "                'position': entry_signal['action'],\n",
    "                'position_size': position_size,\n",
    "                'profit': profit,\n",
    "                'transaction_costs': total_trade_commission,\n",
    "                'slippage_costs': total_trade_slippage,\n",
    "                'duration': duration,\n",
    "                'exit_reason': exit_reason\n",
    "            }\n",
    "            trades.append(trade)\n",
    "            entry_signal = None\n",
    "    \n",
    "    # Close any open position at the end of the dataset\n",
    "    if entry_signal is not None:\n",
    "        last_time = df.index[-1]\n",
    "        last_price = df['close'].iloc[-1]\n",
    "        \n",
    "        # Apply slippage to last price\n",
    "        slippage_amount = 0\n",
    "        price_with_slippage = last_price\n",
    "        \n",
    "        if entry_signal['action'] == 'enter_short':\n",
    "            # For short positions, buying to close (higher price)\n",
    "            slippage_amount = last_price * slippage_factor\n",
    "            price_with_slippage = last_price + slippage_amount\n",
    "        else:\n",
    "            # For long positions, selling to close (lower price)\n",
    "            slippage_amount = last_price * slippage_factor\n",
    "            price_with_slippage = last_price - slippage_amount\n",
    "        \n",
    "        # Track slippage\n",
    "        total_slippage += slippage_amount\n",
    "        \n",
    "        # Calculate commission\n",
    "        commission = price_with_slippage * commission_rate\n",
    "        total_commission += commission\n",
    "        \n",
    "        duration = (last_time - entry_signal['timestamp']).total_seconds() / 60\n",
    "        \n",
    "        # Calculate profit/loss\n",
    "        total_trade_commission = entry_signal['commission'] + commission\n",
    "        total_trade_slippage = entry_signal['slippage'] + slippage_amount\n",
    "        position_size = entry_signal.get('position_size', 1.0)\n",
    "        \n",
    "        if entry_signal['action'] == 'enter_long':\n",
    "            profit = (price_with_slippage - entry_signal['price'] - total_trade_commission) * position_size\n",
    "        else:  # Short trade\n",
    "            profit = (entry_signal['price'] - price_with_slippage - total_trade_commission) * position_size\n",
    "        \n",
    "        trade = {\n",
    "            'entry_time': entry_signal['timestamp'],\n",
    "            'entry_price': entry_signal['price'],\n",
    "            'exit_time': last_time,\n",
    "            'exit_price': price_with_slippage,\n",
    "            'position': entry_signal['action'],\n",
    "            'position_size': position_size,\n",
    "            'profit': profit,\n",
    "            'transaction_costs': total_trade_commission,\n",
    "            'slippage_costs': total_trade_slippage,\n",
    "            'duration': duration,\n",
    "            'exit_reason': 'end_of_data'\n",
    "        }\n",
    "        trades.append(trade)\n",
    "    \n",
    "    return trades, total_commission, total_slippage\n",
    "\n",
    "def plot_candlestick_analysis(df: pd.DataFrame, signals: Optional[List[Dict]] = None, \n",
    "                            trades: Optional[List[Dict]] = None,\n",
    "                            ticker: str = '') -> None:\n",
    "    \"\"\"\n",
    "    Plot candlestick chart with trade signals and performance metrics.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with OHLCV data\n",
    "        signals: Optional list of trading signals\n",
    "        trades: Optional list of executed trades\n",
    "        ticker: Stock ticker symbol\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df_plot = df.copy()\n",
    "        df_plot = df_plot[['open', 'high', 'low', 'close', 'volume']]\n",
    "        df_plot.index.name = 'Date'\n",
    "\n",
    "        # Create subplots\n",
    "        fig = plt.figure(figsize=(20, 12))\n",
    "        gs = gridspec.GridSpec(3, 1, height_ratios=[3, 1, 1], hspace=0.05)\n",
    "        ax1 = plt.subplot(gs[0])\n",
    "        ax2 = plt.subplot(gs[1], sharex=ax1)\n",
    "        ax3 = plt.subplot(gs[2], sharex=ax1)\n",
    "\n",
    "        # Plot candlesticks and volume\n",
    "        mc = mpf.make_marketcolors(up='g', down='r', inherit=True)\n",
    "        s = mpf.make_mpf_style(marketcolors=mc)\n",
    "        mpf.plot(df_plot, type='candle', style=s, ax=ax1, volume=False, show_nontrading=False)\n",
    "\n",
    "        # Plot Volume\n",
    "        df_plot['volume'].plot(ax=ax2, color='blue', alpha=0.5)\n",
    "        ax2.set_ylabel('Volume')\n",
    "        ax2.grid(True)\n",
    "        \n",
    "        # Plot additional indicator (e.g., RSI)\n",
    "        if 'RSI' in df.columns:\n",
    "            df['RSI'].plot(ax=ax3, color='purple')\n",
    "            ax3.set_ylabel('RSI')\n",
    "            ax3.grid(True)\n",
    "            ax3.axhline(y=70, color='r', linestyle='--', alpha=0.5)\n",
    "            ax3.axhline(y=30, color='g', linestyle='--', alpha=0.5)\n",
    "        else:\n",
    "            # Plot close price with SMA\n",
    "            df['close'].plot(ax=ax3, color='blue')\n",
    "            if 'SMA_20' in df.columns:\n",
    "                df['SMA_20'].plot(ax=ax3, color='orange')\n",
    "            ax3.set_ylabel('Price')\n",
    "            ax3.grid(True)\n",
    "\n",
    "        # Add signals if provided\n",
    "        if signals:\n",
    "            for signal in signals:\n",
    "                if signal.get('action') is None:\n",
    "                    continue\n",
    "                    \n",
    "                signal_time = pd.to_datetime(signal['timestamp'])\n",
    "                price = signal['price']\n",
    "                \n",
    "                if 'enter_long' in signal['action']:\n",
    "                    ax1.scatter(signal_time, price, \n",
    "                              marker='^', color='g', s=100, label='Buy Signal')\n",
    "                elif 'enter_short' in signal['action']:\n",
    "                    ax1.scatter(signal_time, price, \n",
    "                              marker='v', color='r', s=100, label='Sell Signal')\n",
    "                elif 'exit_long' in signal['action'] or 'exit_short' in signal['action']:\n",
    "                    ax1.scatter(signal_time, price, \n",
    "                              marker='o', color='k', s=100, label='Exit Signal')\n",
    "\n",
    "        # Plot trades if available\n",
    "        if trades:\n",
    "            # Prepare data for trade annotations\n",
    "            for trade in trades:\n",
    "                entry_time = trade['entry_time']\n",
    "                exit_time = trade['exit_time']\n",
    "                entry_price = trade['entry_price']\n",
    "                exit_price = trade['exit_price']\n",
    "                profit = trade.get('profit', 0)\n",
    "                position_size = trade.get('position_size', 1.0)\n",
    "                \n",
    "                # Choose color based on profit\n",
    "                color = 'g' if profit > 0 else 'r'\n",
    "                \n",
    "                # Draw line connecting entry and exit\n",
    "                ax1.plot([entry_time, exit_time], [entry_price, exit_price], \n",
    "                       color=color, linestyle='-', linewidth=1.5 * position_size, alpha=0.7)\n",
    "                \n",
    "                # Add profit annotation\n",
    "                if 'profit' in trade:\n",
    "                    mid_time = entry_time + (exit_time - entry_time) / 2\n",
    "                    ax1.annotate(f\"${profit:.2f}\", \n",
    "                               xy=(mid_time, max(entry_price, exit_price)), \n",
    "                               xytext=(0, 5), textcoords='offset points',\n",
    "                               fontsize=8, color=color)\n",
    "\n",
    "        # Add legend\n",
    "        handles, labels = ax1.get_legend_handles_labels()\n",
    "        by_label = dict(zip(labels, handles))\n",
    "        ax1.legend(by_label.values(), by_label.keys())\n",
    "\n",
    "        plt.title(f\"Candlestick Chart with Trade Signals for {ticker}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in plot_candlestick_analysis: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "def plot_trading_metrics(metrics: Dict[str, float], ticker: str) -> None:\n",
    "    \"\"\"\n",
    "    Create comprehensive visualization of trading metrics.\n",
    "    \n",
    "    Args:\n",
    "        metrics: Dictionary of trading metrics\n",
    "        ticker: Stock ticker symbol\n",
    "    \"\"\"\n",
    "    plt.style.use('default')\n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    gs = gridspec.GridSpec(2, 2)\n",
    "\n",
    "    # Rates Comparison\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    rates = ['win_rate', 'loss_rate', 'long_win_rate', 'short_win_rate']\n",
    "    values = [metrics.get(rate, 0) for rate in rates]\n",
    "    colors = ['green', 'red', 'lightgreen', 'lightcoral']\n",
    "    ax1.bar(rates, values, color=colors)\n",
    "    ax1.set_title('Trading Rates Comparison')\n",
    "    ax1.set_ylabel('Percentage (%)')\n",
    "    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "    # Profit Metrics\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    profit_metrics = ['total_profit', 'average_profit_per_trade']\n",
    "    values = [metrics.get(metric, 0) for metric in profit_metrics]\n",
    "    ax2.bar(profit_metrics, values, color='blue')\n",
    "    ax2.set_title('Profit Metrics')\n",
    "    ax2.set_ylabel('Amount ($)')\n",
    "    plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "    # Risk Metrics\n",
    "    ax3 = fig.add_subplot(gs[1, 0])\n",
    "    risk_metrics = ['maximum_drawdown', 'sharpe_ratio', 'profit_factor']\n",
    "    values = [metrics.get(metric, 0) for metric in risk_metrics]\n",
    "    ax3.bar(risk_metrics, values, color='purple')\n",
    "    ax3.set_title('Risk Metrics')\n",
    "    plt.setp(ax3.xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "    # Trade Analysis\n",
    "    ax4 = fig.add_subplot(gs[1, 1])\n",
    "    trade_metrics = ['total_trades', 'long_trades', 'short_trades', 'average_trade_duration']\n",
    "    values = [metrics.get(metric, 0) for metric in trade_metrics]\n",
    "    ax4.bar(trade_metrics, values, color='orange')\n",
    "    ax4.set_title('Trade Analysis')\n",
    "    plt.setp(ax4.xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "    plt.suptitle(f'Trading Performance Metrics for {ticker}', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_learning_curves(training_history: TrainingHistory, ticker: str) -> None:\n",
    "    \"\"\"\n",
    "    Plot learning curves showing training metrics over epochs.\n",
    "    \n",
    "    Args:\n",
    "        training_history: Object containing training and validation metrics\n",
    "        ticker: Stock ticker symbol for plot title\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(18, 12))\n",
    "    \n",
    "    # Create subplot grid\n",
    "    gs = gridspec.GridSpec(2, 2)\n",
    "    \n",
    "    # Plot training and validation loss\n",
    "    ax1 = plt.subplot(gs[0, 0])\n",
    "    epochs = range(1, len(training_history.loss_history) + 1)\n",
    "    ax1.plot(epochs, training_history.loss_history, 'b-', label='Training Loss')\n",
    "    ax1.plot(epochs, training_history.validation_loss_history, 'r-', label='Validation Loss')\n",
    "    ax1.set_title('Loss Curves')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    ax1.set_yscale('log')  # Use log scale for better visualization\n",
    "    \n",
    "    # Plot direction accuracy if available\n",
    "    if hasattr(training_history, 'direction_accuracy_history') and training_history.direction_accuracy_history:\n",
    "        ax2 = plt.subplot(gs[0, 1])\n",
    "        ax2.plot(epochs, training_history.direction_accuracy_history, 'g-', label='Direction Accuracy')\n",
    "        ax2.set_title('Direction Prediction Accuracy')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Accuracy (%)')\n",
    "        ax2.grid(True)\n",
    "        ax2.legend()\n",
    "        \n",
    "    # Plot long/short accuracy if available\n",
    "    if (hasattr(training_history, 'long_accuracy_history') and training_history.long_accuracy_history and\n",
    "        hasattr(training_history, 'short_accuracy_history') and training_history.short_accuracy_history):\n",
    "        ax3 = plt.subplot(gs[1, 0])\n",
    "        ax3.plot(epochs, training_history.long_accuracy_history, 'g-', label='Long Accuracy')\n",
    "        ax3.plot(epochs, training_history.short_accuracy_history, 'r-', label='Short Accuracy')\n",
    "        ax3.set_title('Long vs. Short Prediction Accuracy')\n",
    "        ax3.set_xlabel('Epoch')\n",
    "        ax3.set_ylabel('Accuracy (%)')\n",
    "        ax3.grid(True)\n",
    "        ax3.legend()\n",
    "        \n",
    "        # Plot accuracy ratio (long/short)\n",
    "        if len(training_history.long_accuracy_history) == len(training_history.short_accuracy_history):\n",
    "            ax4 = plt.subplot(gs[1, 1])\n",
    "            ratio = [l/s if s > 0 else 1.0 for l, s in zip(\n",
    "                training_history.long_accuracy_history, \n",
    "                training_history.short_accuracy_history\n",
    "            )]\n",
    "            ax4.plot(epochs, ratio, 'b-', label='Long/Short Accuracy Ratio')\n",
    "            ax4.axhline(y=1.0, color='k', linestyle='--', alpha=0.5)\n",
    "            ax4.set_title('Direction Prediction Balance')\n",
    "            ax4.set_xlabel('Epoch')\n",
    "            ax4.set_ylabel('Ratio')\n",
    "            ax4.grid(True)\n",
    "            ax4.legend()\n",
    "    \n",
    "    plt.suptitle(f'Learning Curves for {ticker}', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_prediction_analysis(y_true: np.ndarray, y_pred: np.ndarray, \n",
    "                          current_prices: Optional[np.ndarray] = None,\n",
    "                          ticker: str = '') -> None:\n",
    "    \"\"\"\n",
    "    Plot analysis of prediction performance with focus on directional accuracy.\n",
    "    \n",
    "    Args:\n",
    "        y_true: True values\n",
    "        y_pred: Predicted values\n",
    "        current_prices: Current prices (for direction calculation)\n",
    "        ticker: Stock ticker symbol\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(20, 16))\n",
    "    \n",
    "    # Create subplot grid\n",
    "    gs = gridspec.GridSpec(3, 2)\n",
    "    \n",
    "    # Plot 1: True vs Predicted prices\n",
    "    ax1 = plt.subplot(gs[0, 0])\n",
    "    ax1.scatter(y_true, y_pred, alpha=0.3)\n",
    "    min_val = min(np.min(y_true), np.min(y_pred))\n",
    "    max_val = max(np.max(y_true), np.max(y_pred))\n",
    "    ax1.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
    "    ax1.set_xlabel('True Price')\n",
    "    ax1.set_ylabel('Predicted Price')\n",
    "    ax1.set_title('True vs Predicted Prices')\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot 2: Prediction Error Distribution\n",
    "    ax2 = plt.subplot(gs[0, 1])\n",
    "    errors = y_pred - y_true\n",
    "    mean_error = np.mean(errors)\n",
    "    std_error = np.std(errors)\n",
    "    ax2.hist(errors, bins=50, alpha=0.7, color='blue')\n",
    "    ax2.axvline(x=0, color='r', linestyle='--')\n",
    "    ax2.set_xlabel('Prediction Error')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.set_title(f'Error Distribution (Mean: {mean_error:.4f}, Std: {std_error:.4f})')\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    # Calculate direction accuracy if current prices are available\n",
    "    if current_prices is not None:\n",
    "        # Plot 3: Direction Accuracy\n",
    "        ax3 = plt.subplot(gs[1, 0])\n",
    "        \n",
    "        # Calculate true and predicted directions\n",
    "        true_directions = np.sign(y_true - current_prices)\n",
    "        pred_directions = np.sign(y_pred - current_prices)\n",
    "        \n",
    "        # Calculate direction matches\n",
    "        matches = (true_directions == pred_directions)\n",
    "        \n",
    "        # Separate into up and down movements\n",
    "        up_indices = np.where(true_directions > 0)[0]\n",
    "        down_indices = np.where(true_directions < 0)[0]\n",
    "        \n",
    "        # Calculate accuracies\n",
    "        overall_accuracy = np.mean(matches) * 100\n",
    "        up_accuracy = np.mean(matches[up_indices]) * 100 if len(up_indices) > 0 else 0\n",
    "        down_accuracy = np.mean(matches[down_indices]) * 100 if len(down_indices) > 0 else 0\n",
    "        \n",
    "        # Create bar chart\n",
    "        accuracies = [overall_accuracy, up_accuracy, down_accuracy]\n",
    "        labels = ['Overall', 'Up Movement', 'Down Movement']\n",
    "        colors = ['blue', 'green', 'red']\n",
    "        \n",
    "        ax3.bar(labels, accuracies, color=colors)\n",
    "        ax3.set_ylabel('Direction Accuracy (%)')\n",
    "        ax3.set_title(f'Direction Prediction Accuracy')\n",
    "        ax3.grid(True)\n",
    "        \n",
    "        # Add text annotations\n",
    "        for i, acc in enumerate(accuracies):\n",
    "            ax3.text(i, acc + 1, f'{acc:.1f}%', ha='center')\n",
    "            \n",
    "        # Plot 4: Movement Distribution\n",
    "        ax4 = plt.subplot(gs[1, 1])\n",
    "        \n",
    "        # Count movement types\n",
    "        up_count = len(up_indices)\n",
    "        down_count = len(down_indices)\n",
    "        no_move_count = len(true_directions) - up_count - down_count\n",
    "        \n",
    "        # Create pie chart\n",
    "        labels = ['Up', 'Down', 'No Change']\n",
    "        sizes = [up_count, down_count, no_move_count]\n",
    "        colors = ['green', 'red', 'gray']\n",
    "        \n",
    "        ax4.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "        ax4.axis('equal')\n",
    "        ax4.set_title('True Price Movement Distribution')\n",
    "        \n",
    "        # Plot 5: Prediction Bias\n",
    "        ax5 = plt.subplot(gs[2, 0])\n",
    "        \n",
    "        # Calculate predicted movement types\n",
    "        pred_up = np.sum(pred_directions > 0)\n",
    "        pred_down = np.sum(pred_directions < 0)\n",
    "        pred_no_move = len(pred_directions) - pred_up - pred_down\n",
    "        \n",
    "        # Create side-by-side bar chart\n",
    "        labels = ['Up', 'Down', 'No Change']\n",
    "        true_counts = [up_count, down_count, no_move_count]\n",
    "        pred_counts = [pred_up, pred_down, pred_no_move]\n",
    "        \n",
    "        x = np.arange(len(labels))\n",
    "        width = 0.35\n",
    "        \n",
    "        ax5.bar(x - width/2, true_counts, width, label='True')\n",
    "        ax5.bar(x + width/2, pred_counts, width, label='Predicted')\n",
    "        \n",
    "        ax5.set_xlabel('Movement Direction')\n",
    "        ax5.set_ylabel('Count')\n",
    "        ax5.set_title('True vs Predicted Movement Distribution')\n",
    "        ax5.set_xticks(x)\n",
    "        ax5.set_xticklabels(labels)\n",
    "        ax5.legend()\n",
    "        ax5.grid(True)\n",
    "        \n",
    "        # Plot 6: Error by True Direction\n",
    "        ax6 = plt.subplot(gs[2, 1])\n",
    "        \n",
    "        # Separate errors by direction\n",
    "        up_errors = errors[up_indices]\n",
    "        down_errors = errors[down_indices]\n",
    "        \n",
    "        # Create box plot with updated parameter name\n",
    "        data = [up_errors, down_errors]\n",
    "        ax6.boxplot(data, tick_labels=['Up Movement', 'Down Movement'])\n",
    "        ax6.axhline(y=0, color='r', linestyle='--')\n",
    "        ax6.set_ylabel('Prediction Error')\n",
    "        ax6.set_title('Error Distribution by True Direction')\n",
    "        ax6.grid(True)\n",
    "    \n",
    "    plt.suptitle(f'Prediction Analysis for {ticker}', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def validate_model_quality(evaluation_metrics: Dict[str, float], ticker: str) -> bool:\n",
    "    \"\"\"\n",
    "    Validate model quality to ensure it meets minimum standards before generating signals.\n",
    "    \n",
    "    Args:\n",
    "        evaluation_metrics: Dictionary containing model evaluation metrics\n",
    "        ticker: Stock ticker symbol\n",
    "        \n",
    "    Returns:\n",
    "        Boolean indicating whether model meets quality standards\n",
    "    \"\"\"\n",
    "    # Define minimum quality thresholds\n",
    "    min_direction_accuracy = 50.0  # Direction accuracy should be better than random\n",
    "    max_direction_bias = 15.0      # Directional bias should not be too extreme\n",
    "    max_range_ratio = 2.0          # Prediction range should be reasonable\n",
    "    \n",
    "    # Check direction accuracy\n",
    "    direction_accuracy = evaluation_metrics.get('direction_accuracy', 0)\n",
    "    if direction_accuracy < min_direction_accuracy:\n",
    "        logger.warning(f\"Model for {ticker} failed validation: direction accuracy {direction_accuracy:.2f}% < {min_direction_accuracy}%\")\n",
    "        return False\n",
    "    \n",
    "    # Check for excessive directional bias\n",
    "    direction_bias = abs(evaluation_metrics.get('direction_bias', 0))\n",
    "    if direction_bias > max_direction_bias:\n",
    "        logger.warning(f\"Model for {ticker} failed validation: excessive direction bias {direction_bias:.2f}% > {max_direction_bias}%\")\n",
    "        return False\n",
    "    \n",
    "    # Check for unreasonable prediction range\n",
    "    range_ratio = evaluation_metrics.get('range_ratio', 1.0)\n",
    "    if range_ratio > max_range_ratio:\n",
    "        logger.warning(f\"Model for {ticker} failed validation: unreasonable prediction range ratio {range_ratio:.2f} > {max_range_ratio}\")\n",
    "        return False\n",
    "    \n",
    "    # Check balance between up/down direction accuracies\n",
    "    up_accuracy = evaluation_metrics.get('up_direction_accuracy', 0)\n",
    "    down_accuracy = evaluation_metrics.get('down_direction_accuracy', 0)\n",
    "    \n",
    "    if up_accuracy > 0 and down_accuracy > 0:\n",
    "        accuracy_ratio = up_accuracy / down_accuracy\n",
    "        if accuracy_ratio > 1.5 or accuracy_ratio < 0.67:\n",
    "            logger.warning(f\"Model for {ticker} failed validation: imbalanced direction accuracies (up: {up_accuracy:.2f}%, down: {down_accuracy:.2f}%)\")\n",
    "            return False\n",
    "    \n",
    "    logger.info(f\"Model for {ticker} passed validation criteria\")\n",
    "    return True\n",
    "\n",
    "def get_optimizer_and_scheduler(model: nn.Module, params: Dict = None) -> Tuple[\n",
    "    torch.optim.Optimizer, torch.optim.lr_scheduler._LRScheduler]:\n",
    "    \"\"\"\n",
    "    Create optimizer and learning rate scheduler with enhanced parameters.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        params: Optional parameters dictionary\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of optimizer and scheduler\n",
    "    \"\"\"\n",
    "    if params is None:\n",
    "        params = {}\n",
    "    \n",
    "    # Extract parameters with defaults\n",
    "    lr = params.get('learning_rate', 0.001)\n",
    "    weight_decay = params.get('weight_decay', 1e-2)  # Increased from 5e-3\n",
    "    scheduler_factor = params.get('scheduler_factor', 0.3)  # Increased from 0.4\n",
    "    scheduler_patience = params.get('scheduler_patience', 2)  # Decreased from 3\n",
    "    min_lr = params.get('min_lr', 1e-6)  # Decreased from 1e-5\n",
    "    \n",
    "    # Create optimizer with gradient clipping\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    # Create scheduler with early detection of plateaus\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='min',\n",
    "        factor=scheduler_factor,\n",
    "        patience=scheduler_patience,\n",
    "        verbose=True,\n",
    "        min_lr=min_lr,\n",
    "        threshold=0.0001,  # More sensitive threshold\n",
    "        threshold_mode='rel'\n",
    "    )\n",
    "    \n",
    "    return optimizer, scheduler\n",
    "\n",
    "def perform_time_series_cross_validation(\n",
    "    data_df: pd.DataFrame, \n",
    "    model_class: nn.Module, \n",
    "    seq_length: int, \n",
    "    n_splits: int = 3, \n",
    "    test_size: int = 1000,\n",
    "    device: torch.device = torch.device('cpu'),\n",
    "    model_params: Dict = None) -> Tuple[List[float], List[Dict], nn.Module]:\n",
    "    \"\"\"\n",
    "    Perform time series cross-validation for model evaluation.\n",
    "    \n",
    "    Args:\n",
    "        data_df: DataFrame with features and targets\n",
    "        model_class: PyTorch model class to use\n",
    "        seq_length: Sequence length for LSTM\n",
    "        n_splits: Number of splits for cross-validation\n",
    "        test_size: Size of each test set\n",
    "        device: Computation device (CPU/GPU)\n",
    "        model_params: Dictionary with model parameters\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of validation accuracies, metrics per fold, and best model\n",
    "    \"\"\"\n",
    "    if model_params is None:\n",
    "        model_params = {\n",
    "            'hidden_size': 256,\n",
    "            'num_layers': 2,\n",
    "            'dropout': 0.5\n",
    "        }\n",
    "    \n",
    "    # Define validation metrics storage\n",
    "    val_accuracies = []\n",
    "    fold_metrics = []\n",
    "    best_model = None\n",
    "    best_accuracy = 0\n",
    "    \n",
    "    # Prepare cross-validation indices - forward chaining\n",
    "    total_samples = len(data_df)\n",
    "    fold_size = (total_samples - test_size) // n_splits\n",
    "    \n",
    "    for fold in range(n_splits):\n",
    "        logger.info(f\"Processing fold {fold+1}/{n_splits}\")\n",
    "        \n",
    "        # Define train/test indices for this fold\n",
    "        if fold < n_splits - 1:\n",
    "            train_end = test_size + fold_size * (fold + 1)\n",
    "            train_indices = range(0, train_end)\n",
    "            test_indices = range(train_end, train_end + test_size)\n",
    "        else:\n",
    "            # Last fold uses all remaining data\n",
    "            train_indices = range(0, total_samples - test_size)\n",
    "            test_indices = range(total_samples - test_size, total_samples)\n",
    "        \n",
    "        # Create datasets for this fold\n",
    "        train_data = data_df.iloc[train_indices].copy()\n",
    "        test_data = data_df.iloc[test_indices].copy()\n",
    "        \n",
    "        # Create HFTDataset objects\n",
    "        train_dataset = HFTDataset(\n",
    "            data=train_data,\n",
    "            sequence_length=seq_length,\n",
    "            include_current_price=True\n",
    "        )\n",
    "        \n",
    "        test_dataset = HFTDataset(\n",
    "            data=test_data,\n",
    "            sequence_length=seq_length,\n",
    "            scaler=train_dataset.scaler,  # Use same scaler\n",
    "            include_current_price=True\n",
    "        )\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "        \n",
    "        # Initialize model\n",
    "        input_size = len(train_dataset.feature_names)\n",
    "        model = model_class(\n",
    "            input_size=input_size,\n",
    "            hidden_size=model_params['hidden_size'],\n",
    "            num_layers=model_params['num_layers'],\n",
    "            dropout=model_params['dropout']\n",
    "        )\n",
    "        \n",
    "        # Initialize training components\n",
    "        criterion = DirectionalPredictionLoss(\n",
    "            direction_weight=2.5,\n",
    "            magnitude_weight=0.8,\n",
    "            short_penalty_multiplier=3.5,\n",
    "            bias_correction_weight=1.5\n",
    "        )\n",
    "        \n",
    "        optimizer = optim.Adam(\n",
    "            model.parameters(), \n",
    "            lr=0.001,\n",
    "            weight_decay=1e-2\n",
    "        )\n",
    "        \n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, \n",
    "            mode='min', \n",
    "            factor=0.3,\n",
    "            patience=2,\n",
    "            verbose=True,\n",
    "            min_lr=1e-6\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        training_history = TrainingHistory()\n",
    "        model = train_model(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=test_loader,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            num_epochs=50,  # Reduced for cross-validation\n",
    "            device=device,\n",
    "            training_history=training_history,\n",
    "            early_stopping_patience=5,  # Reduced for cross-validation\n",
    "            scheduler=scheduler\n",
    "        )\n",
    "        \n",
    "        # Evaluate model\n",
    "        evaluator = ModelEvaluator()\n",
    "        y_true, y_pred, current_prices = evaluate_model(\n",
    "            model, \n",
    "            test_loader, \n",
    "            device,\n",
    "            return_predictions=True,\n",
    "            evaluator=evaluator\n",
    "        )\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = evaluator.calculate_metrics(\n",
    "            y_true=y_true,\n",
    "            y_pred=y_pred,\n",
    "            current_prices=current_prices\n",
    "        )\n",
    "        \n",
    "        # Store metrics\n",
    "        fold_metrics.append(metrics)\n",
    "        direction_accuracy = metrics.get('direction_accuracy', 0)\n",
    "        val_accuracies.append(direction_accuracy)\n",
    "        \n",
    "        # Check if this is the best model so far\n",
    "        if direction_accuracy > best_accuracy:\n",
    "            best_accuracy = direction_accuracy\n",
    "            best_model = model.state_dict().copy()\n",
    "            \n",
    "        logger.info(f\"Fold {fold+1} direction accuracy: {direction_accuracy:.2f}%\")\n",
    "        \n",
    "    # Log average metrics across folds\n",
    "    avg_accuracy = np.mean(val_accuracies)\n",
    "    logger.info(f\"Average direction accuracy across {n_splits} folds: {avg_accuracy:.2f}%\")\n",
    "    \n",
    "    # Load best model weights into the model\n",
    "    if best_model is not None:\n",
    "        model.load_state_dict(best_model)\n",
    "        \n",
    "    return val_accuracies, fold_metrics, model\n",
    "\n",
    "def optimize_trading_parameters(ticker: str, stock_df: pd.DataFrame, market_index_df: pd.DataFrame,\n",
    "                              device: torch.device, evaluator: ModelEvaluator, n_trials: int = 30):\n",
    "    \"\"\"\n",
    "    Use time-series cross-validation to optimize trading parameters.\n",
    "    \n",
    "    Args:\n",
    "        ticker: Stock ticker symbol\n",
    "        stock_df: DataFrame with stock price data\n",
    "        market_index_df: DataFrame with market index data\n",
    "        device: PyTorch device (CPU/GPU)\n",
    "        evaluator: ModelEvaluator instance\n",
    "        n_trials: Number of optimization trials\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of optimized parameters\n",
    "    \"\"\"\n",
    "    import optuna\n",
    "    from functools import partial\n",
    "    \n",
    "    # Prepare data with feature engineering\n",
    "    data_df = enhance_features(stock_df, market_index_df)\n",
    "    \n",
    "    # Create time-series split for cross-validation\n",
    "    # Use 5 folds with forward chaining\n",
    "    n_splits = 3\n",
    "    test_size = len(data_df) // 8  # 12.5% of data for testing in each fold\n",
    "    \n",
    "    def time_series_split(df, n_splits, test_size):\n",
    "        \"\"\"Create time-series splits for cross-validation.\"\"\"\n",
    "        splits = []\n",
    "        total_size = len(df)\n",
    "        fold_size = (total_size - test_size) // n_splits\n",
    "        \n",
    "        for fold in range(n_splits):\n",
    "            if fold < n_splits - 1:\n",
    "                train_end = test_size + fold_size * (fold + 1)\n",
    "                train_indices = range(0, train_end)\n",
    "                test_indices = range(train_end, train_end + test_size)\n",
    "            else:\n",
    "                # Last fold uses all remaining data\n",
    "                train_indices = range(0, total_size - test_size)\n",
    "                test_indices = range(total_size - test_size, total_size)\n",
    "            \n",
    "            splits.append((train_indices, test_indices))\n",
    "        \n",
    "        return splits\n",
    "    \n",
    "    ts_splits = time_series_split(data_df, n_splits, test_size)\n",
    "    \n",
    "    def objective(trial, data_df, ts_splits, ticker, device, evaluator):\n",
    "        \"\"\"Optuna objective function for parameter optimization.\"\"\"\n",
    "        # Parameters to optimize\n",
    "        params = {\n",
    "            # Model parameters\n",
    "            'dropout': trial.suggest_float('dropout', 0.3, 0.7),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True),\n",
    "            'weight_decay': trial.suggest_float('weight_decay', 1e-4, 1e-2, log=True),\n",
    "            \n",
    "            # Loss function parameters\n",
    "            'direction_weight': trial.suggest_float('direction_weight', 1.5, 3.5),\n",
    "            'magnitude_weight': trial.suggest_float('magnitude_weight', 0.5, 1.2),\n",
    "            'short_penalty_multiplier': trial.suggest_float('short_penalty_multiplier', 1.0, 2.0),\n",
    "            'bias_correction_weight': trial.suggest_float('bias_correction_weight', 0.5, 1.2),\n",
    "            \n",
    "            # Signal generator parameters\n",
    "            'base_entry_threshold': trial.suggest_float('base_entry_threshold', 0.0005, 0.0025),\n",
    "            'short_entry_threshold_factor': trial.suggest_float('short_entry_threshold_factor', 0.7, 1.0),\n",
    "            'base_exit_threshold': trial.suggest_float('base_exit_threshold', 0.0004, 0.0020),\n",
    "            'base_stop_loss': trial.suggest_float('base_stop_loss', 0.0008, 0.0040),\n",
    "            'atr_multiplier': trial.suggest_float('atr_multiplier', 0.8, 1.8),\n",
    "            \n",
    "            # Signal balance parameters\n",
    "            'downward_bias_correction_high': trial.suggest_float('downward_bias_correction_high', 0.001, 0.004),\n",
    "            'downward_bias_correction_low': trial.suggest_float('downward_bias_correction_low', 0.0005, 0.002),\n",
    "            'adaptive_factor_cap': trial.suggest_float('adaptive_factor_cap', 0.001, 0.003),\n",
    "        }\n",
    "        \n",
    "        # Fixed parameters\n",
    "        params.update({\n",
    "            'sequence_length': 60,\n",
    "            'hidden_size': 256,\n",
    "            'num_layers': 2,\n",
    "            'batch_size': 64,\n",
    "            'num_epochs': 50,  # Reduced for optimization runs\n",
    "            'early_stopping_patience': 10,\n",
    "        })\n",
    "        \n",
    "        # Cross-validation scores\n",
    "        cv_direction_accuracies = []\n",
    "        cv_direction_balances = []\n",
    "        cv_trade_metrics = []\n",
    "        \n",
    "        for fold, (train_idx, test_idx) in enumerate(ts_splits):\n",
    "            try:\n",
    "                train_data = data_df.iloc[train_idx].copy()\n",
    "                test_data = data_df.iloc[test_idx].copy()\n",
    "                \n",
    "                # Create datasets\n",
    "                train_dataset = HFTDataset(\n",
    "                    data=train_data,\n",
    "                    sequence_length=params['sequence_length'],\n",
    "                    include_current_price=True,\n",
    "                    ticker=ticker\n",
    "                )\n",
    "                \n",
    "                test_dataset = HFTDataset(\n",
    "                    data=test_data,\n",
    "                    sequence_length=params['sequence_length'],\n",
    "                    scaler=train_dataset.scaler,\n",
    "                    include_current_price=True,\n",
    "                    ticker=ticker\n",
    "                )\n",
    "                \n",
    "                # Create data loaders\n",
    "                train_loader = DataLoader(\n",
    "                    train_dataset, \n",
    "                    batch_size=params['batch_size'],\n",
    "                    shuffle=True\n",
    "                )\n",
    "                \n",
    "                test_loader = DataLoader(\n",
    "                    test_dataset,\n",
    "                    batch_size=params['batch_size']\n",
    "                )\n",
    "                \n",
    "                # Initialize model\n",
    "                input_size = len(train_dataset.feature_names)\n",
    "                model = LSTMModel(\n",
    "                    input_size=input_size,\n",
    "                    hidden_size=params['hidden_size'],\n",
    "                    num_layers=params['num_layers'],\n",
    "                    dropout=params['dropout']\n",
    "                )\n",
    "                \n",
    "                # Initialize loss function with optimized parameters\n",
    "                criterion = DirectionalPredictionLoss(\n",
    "                    direction_weight=params['direction_weight'],\n",
    "                    magnitude_weight=params['magnitude_weight'],\n",
    "                    short_penalty_multiplier=params['short_penalty_multiplier'],\n",
    "                    bias_correction_weight=params['bias_correction_weight']\n",
    "                )\n",
    "                criterion.set_ticker(ticker)\n",
    "                \n",
    "                # Initialize optimizer\n",
    "                optimizer = optim.Adam(\n",
    "                    model.parameters(),\n",
    "                    lr=params['learning_rate'],\n",
    "                    weight_decay=params['weight_decay']\n",
    "                )\n",
    "                \n",
    "                # Initialize scheduler\n",
    "                scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                    optimizer,\n",
    "                    mode='min',\n",
    "                    factor=0.3,\n",
    "                    patience=3,\n",
    "                    verbose=False\n",
    "                )\n",
    "                \n",
    "                # Train model\n",
    "                model = train_model(\n",
    "                    model=model,\n",
    "                    train_loader=train_loader,\n",
    "                    val_loader=test_loader,\n",
    "                    criterion=criterion,\n",
    "                    optimizer=optimizer,\n",
    "                    num_epochs=params['num_epochs'],\n",
    "                    device=device,\n",
    "                    training_history=None,\n",
    "                    early_stopping_patience=params['early_stopping_patience'],\n",
    "                    scheduler=scheduler,\n",
    "                    ticker=ticker\n",
    "                )\n",
    "                \n",
    "                # Evaluate model\n",
    "                y_true, y_pred, current_prices = evaluate_model(\n",
    "                    model=model,\n",
    "                    test_loader=test_loader,\n",
    "                    device=device,\n",
    "                    return_predictions=True,\n",
    "                    evaluator=evaluator,\n",
    "                    ticker=ticker\n",
    "                )\n",
    "                \n",
    "                # Apply range clipping\n",
    "                max_pct_change = 0.015\n",
    "                min_bound = current_prices * (1 - max_pct_change)\n",
    "                max_bound = current_prices * (1 + max_pct_change)\n",
    "                y_pred = np.clip(y_pred, min_bound, max_bound)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                metrics = evaluator.calculate_metrics(\n",
    "                    y_true=y_true,\n",
    "                    y_pred=y_pred,\n",
    "                    current_prices=current_prices,\n",
    "                    ticker=ticker\n",
    "                )\n",
    "                \n",
    "                # Initialize signal generator with optimized parameters\n",
    "                signal_generator = AdaptiveSignalGenerator(ticker)\n",
    "                signal_generator.params.update({\n",
    "                    'base_entry_threshold': params['base_entry_threshold'],\n",
    "                    'short_entry_threshold_factor': params['short_entry_threshold_factor'],\n",
    "                    'base_exit_threshold': params['base_exit_threshold'],\n",
    "                    'base_stop_loss': params['base_stop_loss'],\n",
    "                    'atr_multiplier': params['atr_multiplier'],\n",
    "                })\n",
    "                \n",
    "                # Initialize monitor\n",
    "                monitor = MultiTickerMonitor(signal_generator)\n",
    "                \n",
    "                # Generate signals\n",
    "                all_signals = []\n",
    "                for idx in range(len(y_true)):\n",
    "                    timestamp = test_data.index[idx]\n",
    "                    actual = y_true[idx]\n",
    "                    predicted = y_pred[idx]\n",
    "                    current_price = current_prices[idx]\n",
    "                    \n",
    "                    # Get market data window\n",
    "                    window_start_idx = max(0, test_idx[0] + idx - 20)\n",
    "                    window_end_idx = test_idx[0] + idx + 1\n",
    "                    market_data_window = data_df.iloc[window_start_idx:window_end_idx].copy()\n",
    "                    \n",
    "                    # Apply custom bias correction settings\n",
    "                    market_regime = signal_generator.detect_market_regime(market_data_window)\n",
    "                    price_changes = market_data_window['close'].pct_change().dropna()\n",
    "                    mean_price_change = price_changes[-20:].mean() if len(price_changes) >= 20 else 0\n",
    "                    \n",
    "                    # Apply optimized bias correction\n",
    "                    if market_regime in ['trending_up', 'high_volatility']:\n",
    "                        downward_bias_correction = current_price * params['downward_bias_correction_high']\n",
    "                    else:\n",
    "                        downward_bias_correction = current_price * params['downward_bias_correction_low']\n",
    "                    \n",
    "                    if mean_price_change > 0:\n",
    "                        adaptive_factor = min(params['adaptive_factor_cap'], mean_price_change)\n",
    "                        downward_bias_correction += current_price * adaptive_factor\n",
    "                    \n",
    "                    corrected_prediction = predicted - downward_bias_correction\n",
    "                    \n",
    "                    # Generate signal with corrected prediction\n",
    "                    signal = monitor.update_ticker(\n",
    "                        ticker=ticker,\n",
    "                        current_price=actual,\n",
    "                        predicted_price=corrected_prediction,\n",
    "                        timestamp=timestamp,\n",
    "                        market_data=market_data_window,\n",
    "                        prediction_metrics=metrics\n",
    "                    )\n",
    "                    \n",
    "                    if signal and signal['action']:\n",
    "                        all_signals.append(signal)\n",
    "                \n",
    "                # Generate trades\n",
    "                trades, _, _ = backtest_trades_with_costs(\n",
    "                    all_signals,\n",
    "                    test_data,\n",
    "                    commission_rate=0.001,\n",
    "                    slippage_factor=0.0002\n",
    "                )\n",
    "                \n",
    "                # Calculate trade metrics\n",
    "                if trades:\n",
    "                    trade_metrics = evaluator.calculate_trade_performance_metrics(\n",
    "                        trades=trades,\n",
    "                        initial_capital=10000.0,\n",
    "                        include_transaction_costs=True\n",
    "                    )\n",
    "                    \n",
    "                    # Balance metrics\n",
    "                    long_trades = trade_metrics.get('long_trades', 0)\n",
    "                    short_trades = trade_metrics.get('short_trades', 0)\n",
    "                    total_trades = long_trades + short_trades\n",
    "                    \n",
    "                    if total_trades > 0:\n",
    "                        long_pct = (long_trades / total_trades) * 100\n",
    "                        short_pct = (short_trades / total_trades) * 100\n",
    "                        trade_balance = abs(50 - long_pct)  # 0 means perfect balance\n",
    "                    else:\n",
    "                        trade_balance = 100  # Worst possible balance\n",
    "                else:\n",
    "                    trade_metrics = {\n",
    "                        'total_trades': 0,\n",
    "                        'win_rate': 0,\n",
    "                        'long_trades': 0,\n",
    "                        'short_trades': 0,\n",
    "                        'total_profit': 0\n",
    "                    }\n",
    "                    trade_balance = 100\n",
    "                \n",
    "                # Store metrics\n",
    "                cv_direction_accuracies.append(metrics.get('direction_accuracy', 0))\n",
    "                cv_direction_balances.append(abs(metrics.get('up_direction_accuracy', 50) - metrics.get('down_direction_accuracy', 50)))\n",
    "                cv_trade_metrics.append(trade_metrics)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in optimization fold {fold}: {e}\")\n",
    "                # Return poor score for failed trials\n",
    "                return -1000\n",
    "        \n",
    "        # Calculate average metrics across folds\n",
    "        avg_direction_accuracy = np.mean(cv_direction_accuracies)\n",
    "        avg_direction_balance = np.mean(cv_direction_balances)\n",
    "        \n",
    "        # Calculate average trade metrics\n",
    "        total_trades = sum(m['total_trades'] for m in cv_trade_metrics) / len(cv_trade_metrics)\n",
    "        avg_win_rate = np.mean([m['win_rate'] for m in cv_trade_metrics if m['total_trades'] > 0] or [0])\n",
    "        \n",
    "        # Calculate trade balance\n",
    "        long_trades = sum(m['long_trades'] for m in cv_trade_metrics)\n",
    "        short_trades = sum(m['short_trades'] for m in cv_trade_metrics)\n",
    "        total_trade_count = long_trades + short_trades\n",
    "        \n",
    "        if total_trade_count > 0:\n",
    "            long_pct = (long_trades / total_trade_count) * 100\n",
    "            trade_balance_score = abs(50 - long_pct)  # 0 means perfect balance\n",
    "        else:\n",
    "            trade_balance_score = 100\n",
    "        \n",
    "        # Calculate combined score\n",
    "        # We want: high direction accuracy, low direction imbalance, more trades, higher win rate, balanced trade types\n",
    "        score = (\n",
    "            avg_direction_accuracy * 10 \n",
    "            - avg_direction_balance * 5 \n",
    "            + min(100, total_trades) * 0.5\n",
    "            + avg_win_rate * 2\n",
    "            - trade_balance_score * 5\n",
    "        )\n",
    "        \n",
    "        # Log trial results\n",
    "        logger.info(f\"Trial {trial.number} - Score: {score:.2f}, Direction Accuracy: {avg_direction_accuracy:.2f}%, \"\n",
    "                   f\"Trades: {total_trades:.1f}, Win Rate: {avg_win_rate:.2f}%, \"\n",
    "                   f\"Long/Short: {long_trades}/{short_trades}\")\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    # Create Optuna study\n",
    "    logger.info(f\"Starting parameter optimization for {ticker} with {n_trials} trials\")\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    objective_func = partial(objective, data_df=data_df, ts_splits=ts_splits, \n",
    "                            ticker=ticker, device=device, evaluator=evaluator)\n",
    "    \n",
    "    # Run optimization\n",
    "    study.optimize(objective_func, n_trials=n_trials)\n",
    "    \n",
    "    # Get best parameters\n",
    "    best_params = study.best_params\n",
    "    best_score = study.best_value\n",
    "    \n",
    "    # Log best parameters\n",
    "    logger.info(f\"Optimization completed for {ticker}\")\n",
    "    logger.info(f\"Best score: {best_score:.2f}\")\n",
    "    logger.info(f\"Best parameters: {best_params}\")\n",
    "    \n",
    "    # Add fixed parameters\n",
    "    best_params.update({\n",
    "        'sequence_length': 60,\n",
    "        'hidden_size': 256,\n",
    "        'num_layers': 2,\n",
    "        'batch_size': 64,\n",
    "        'num_epochs': 100,  # Restore full epochs for final training\n",
    "        'early_stopping_patience': 15,\n",
    "    })\n",
    "    \n",
    "    return best_params\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function for the high-frequency trading system with optimization and improved model performance.\"\"\"\n",
    "    \n",
    "    # Configuration\n",
    "    TICKERS = ['MSFT', 'GOOGL', 'TSLA', 'NVDA', 'TQQQ', 'SQQQ', 'QQQ', 'PSQ', 'QLD']\n",
    "    MARKET_INDEX = 'SPY'\n",
    "    END_DATE = '2025-03-14'  # Current date\n",
    "    \n",
    "    # Set the number of days for training, validation, and testing\n",
    "    TRAINING_DAYS = 90\n",
    "    \n",
    "    # Set hyperparameters (these will be overridden by optimization if enabled)\n",
    "    MODEL_PARAMS = {\n",
    "        'sequence_length': 60,\n",
    "        'hidden_size': 256,\n",
    "        'num_layers': 2,\n",
    "        'batch_size': 64,\n",
    "        'num_epochs': 100,\n",
    "        'learning_rate': 0.001,\n",
    "        'dropout': 0.5,\n",
    "        'early_stopping_patience': 15\n",
    "    }\n",
    "    \n",
    "    # Transaction cost parameters for realistic backtesting\n",
    "    TRANSACTION_COSTS = {\n",
    "        'commission_rate': 0.001,  # 0.1% commission\n",
    "        'slippage_factor': 0.0002  # 0.02% slippage\n",
    "    }\n",
    "    \n",
    "    # Optimization control\n",
    "    ENABLE_OPTIMIZATION = True  # Set to False to skip optimization\n",
    "    OPTIMIZATION_TRIALS = 3    # Number of trials for optimization\n",
    "\n",
    "    # Initialize components\n",
    "    fetcher = PolygonDataFetcher(API_KEY)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "    evaluator = ModelEvaluator()\n",
    "\n",
    "    # Define date range\n",
    "    end_date = datetime.strptime(END_DATE, '%Y-%m-%d')\n",
    "    start_date = end_date - timedelta(days=TRAINING_DAYS)\n",
    "    start_date_str = start_date.strftime('%Y-%m-%d')\n",
    "    end_date_str = end_date.strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Fetch market index data for context\n",
    "    logger.info(f\"Fetching market index data from {start_date_str} to {end_date_str}\")\n",
    "    market_index_df = fetcher.fetch_market_index_data(\n",
    "        index_symbol=MARKET_INDEX,\n",
    "        start_date=start_date_str,\n",
    "        end_date=end_date_str\n",
    "    )\n",
    "    \n",
    "    if market_index_df.empty:\n",
    "        logger.warning(f\"No market index data available. Proceeding without market context.\")\n",
    "\n",
    "    # Create output directories if they don't exist\n",
    "    os.makedirs('balanced_lstm_models', exist_ok=True)\n",
    "    os.makedirs('plots', exist_ok=True)\n",
    "    \n",
    "    # Process each ticker\n",
    "    for ticker in TICKERS:\n",
    "        try:\n",
    "            logger.info(f\"Processing {ticker}\")\n",
    "            print(f\"\\nProcessing {ticker}...\")\n",
    "            \n",
    "            # Create ticker-specific output directory\n",
    "            ticker_dir = f'balanced_lstm_models/{ticker}/'\n",
    "            os.makedirs(ticker_dir, exist_ok=True)\n",
    "\n",
    "            # Fetch and prepare market data\n",
    "            logger.info(f\"Fetching market data from {start_date_str} to {end_date_str}\")\n",
    "            stock_df = fetcher.fetch_stock_data(ticker, start_date_str, end_date_str)\n",
    "\n",
    "            if stock_df.empty:\n",
    "                logger.warning(f\"No data fetched for {ticker}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Apply enhanced feature engineering\n",
    "            stock_df = enhance_features(stock_df, market_index_df)\n",
    "            logger.info(f\"Enhanced features created for {ticker}\")\n",
    "            \n",
    "            # Create balanced dataset with stratified sampling\n",
    "            balanced_df = create_stratified_dataset(\n",
    "                stock_df, \n",
    "                min_regime_count=50,\n",
    "                min_total_samples=1000,\n",
    "                max_sampling_fraction=0.7,\n",
    "                max_oversample_factor=5.0\n",
    "            )\n",
    "            logger.info(f\"Created balanced dataset with {len(balanced_df)} samples\")\n",
    "            \n",
    "            # Check if stratified dataset has sufficient data\n",
    "            if len(balanced_df) < 200:\n",
    "                logger.warning(f\"WARNING: Stratified dataset for {ticker} has only {len(balanced_df)} samples\")\n",
    "                \n",
    "                # Fallback to original dataset if samples are extremely low\n",
    "                if len(balanced_df) < 100:\n",
    "                    logger.warning(f\"Using original dataset instead for {ticker} due to insufficient stratified samples\")\n",
    "                    balanced_df = stock_df\n",
    "                    logger.info(f\"Using original dataset with {len(balanced_df)} samples\")\n",
    "            \n",
    "            # Initialize stock-specific components\n",
    "            signal_generator = AdaptiveSignalGenerator(ticker)\n",
    "            \n",
    "            # Hyperparameter Optimization with Time-Series Cross-Validation\n",
    "            if ENABLE_OPTIMIZATION:\n",
    "                print(f\"Optimizing trading parameters for {ticker}...\")\n",
    "                try:\n",
    "                    optimized_params = optimize_trading_parameters(\n",
    "                        ticker=ticker,\n",
    "                        stock_df=stock_df,\n",
    "                        market_index_df=market_index_df,\n",
    "                        device=device,\n",
    "                        evaluator=evaluator,\n",
    "                        n_trials=OPTIMIZATION_TRIALS\n",
    "                    )\n",
    "                    \n",
    "                    # Update model parameters with optimized values\n",
    "                    MODEL_PARAMS.update({\n",
    "                        'dropout': optimized_params['dropout'],\n",
    "                        'learning_rate': optimized_params['learning_rate'],\n",
    "                        'weight_decay': optimized_params['weight_decay']\n",
    "                    })\n",
    "                    \n",
    "                    # Update loss function parameters (will be used when creating criterion)\n",
    "                    loss_params = {\n",
    "                        'direction_weight': optimized_params['direction_weight'],\n",
    "                        'magnitude_weight': optimized_params['magnitude_weight'],\n",
    "                        'short_penalty_multiplier': optimized_params['short_penalty_multiplier'],\n",
    "                        'bias_correction_weight': optimized_params['bias_correction_weight']\n",
    "                    }\n",
    "                    \n",
    "                    # Update signal generator parameters\n",
    "                    signal_generator.params.update({\n",
    "                        'base_entry_threshold': optimized_params['base_entry_threshold'],\n",
    "                        'short_entry_threshold_factor': optimized_params['short_entry_threshold_factor'],\n",
    "                        'base_exit_threshold': optimized_params['base_exit_threshold'],\n",
    "                        'base_stop_loss': optimized_params['base_stop_loss'],\n",
    "                        'atr_multiplier': optimized_params['atr_multiplier']\n",
    "                    })\n",
    "                    \n",
    "                    # Set bias correction parameters for signal generation later\n",
    "                    bias_correction_params = {\n",
    "                        'downward_bias_correction_high': optimized_params['downward_bias_correction_high'],\n",
    "                        'downward_bias_correction_low': optimized_params['downward_bias_correction_low'],\n",
    "                        'adaptive_factor_cap': optimized_params['adaptive_factor_cap']\n",
    "                    }\n",
    "                    \n",
    "                    # Save optimized parameters\n",
    "                    params_filename = f'{ticker_dir}/optimized_params_{ticker}_{end_date_str}.pkl'\n",
    "                    with open(params_filename, 'wb') as f:\n",
    "                        pickle.dump(optimized_params, f)\n",
    "                    \n",
    "                    print(f\"Optimized parameters saved to {params_filename}\")\n",
    "                    logger.info(f\"Optimization completed successfully with score: {optimized_params.get('score', 'N/A')}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error during parameter optimization: {e}\")\n",
    "                    print(f\"Optimization failed with error: {str(e)}. Using default parameters.\")\n",
    "                    # Continue with default parameters if optimization fails\n",
    "                    loss_params = {\n",
    "                        'direction_weight': 2.5,\n",
    "                        'magnitude_weight': 0.8, \n",
    "                        'short_penalty_multiplier': 1.5,\n",
    "                        'bias_correction_weight': 0.8\n",
    "                    }\n",
    "                    bias_correction_params = {\n",
    "                        'downward_bias_correction_high': 0.0025,\n",
    "                        'downward_bias_correction_low': 0.001,\n",
    "                        'adaptive_factor_cap': 0.0015\n",
    "                    }\n",
    "            else:\n",
    "                # Use default parameters without optimization\n",
    "                loss_params = {\n",
    "                    'direction_weight': 2.5,\n",
    "                    'magnitude_weight': 0.8, \n",
    "                    'short_penalty_multiplier': 1.5,\n",
    "                    'bias_correction_weight': 0.8\n",
    "                }\n",
    "                bias_correction_params = {\n",
    "                    'downward_bias_correction_high': 0.0025,\n",
    "                    'downward_bias_correction_low': 0.001,\n",
    "                    'adaptive_factor_cap': 0.0015\n",
    "                }\n",
    "            \n",
    "            # Initialize monitor with configured signal generator\n",
    "            monitor = MultiTickerMonitor(signal_generator)\n",
    "            \n",
    "            # Prepare datasets with current price for directional loss\n",
    "            dataset = HFTDataset(\n",
    "                data=balanced_df,\n",
    "                sequence_length=MODEL_PARAMS['sequence_length'],\n",
    "                scaler=StandardScaler(),\n",
    "                include_current_price=True,\n",
    "                relative_normalization=True,\n",
    "                ticker=ticker\n",
    "            )\n",
    "            logger.info(f\"Dataset created with {len(dataset)} sequences and {len(dataset.feature_names)} features\")\n",
    "\n",
    "            # Split data with proper sizes\n",
    "            train_size = int(0.7 * len(dataset))\n",
    "            val_size = int(0.15 * len(dataset))\n",
    "            test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "            if train_size <= 0 or val_size <= 0 or test_size <= 0:\n",
    "                logger.warning(f\"Insufficient data for {ticker} after preprocessing. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Set random seed before splitting for reproducibility\n",
    "            torch.manual_seed(42)\n",
    "            train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "                dataset, [train_size, val_size, test_size]\n",
    "            )\n",
    "\n",
    "            # Create data loaders\n",
    "            train_loader = DataLoader(\n",
    "                train_dataset,\n",
    "                batch_size=MODEL_PARAMS['batch_size'],\n",
    "                shuffle=True,\n",
    "                drop_last=False,\n",
    "                num_workers=4 if torch.cuda.is_available() else 0\n",
    "            )\n",
    "            val_loader = DataLoader(\n",
    "                val_dataset,\n",
    "                batch_size=MODEL_PARAMS['batch_size'],\n",
    "                drop_last=False,\n",
    "                num_workers=4 if torch.cuda.is_available() else 0\n",
    "            )\n",
    "            test_loader = DataLoader(\n",
    "                test_dataset,\n",
    "                batch_size=MODEL_PARAMS['batch_size'],\n",
    "                drop_last=False,\n",
    "                num_workers=4 if torch.cuda.is_available() else 0\n",
    "            )\n",
    "\n",
    "            # For problematic tickers, use the ensemble approach\n",
    "            if ticker in ['MSFT', 'GOOGL', 'NVDA', 'TQQQ']:\n",
    "                print(f\"Using ensemble approach for {ticker}...\")\n",
    "                \n",
    "                # Define model configurations for ensemble with improved parameters\n",
    "                model_configs = [\n",
    "                    # Base configuration with optimized parameters\n",
    "                    {\n",
    "                        'hidden_size': MODEL_PARAMS['hidden_size'],\n",
    "                        'num_layers': MODEL_PARAMS['num_layers'],\n",
    "                        'dropout': MODEL_PARAMS['dropout'],\n",
    "                        'learning_rate': MODEL_PARAMS['learning_rate'],\n",
    "                        'weight_decay': MODEL_PARAMS.get('weight_decay', 1e-2),\n",
    "                        'direction_weight': loss_params['direction_weight'],\n",
    "                        'magnitude_weight': loss_params['magnitude_weight'],\n",
    "                        'short_penalty_multiplier': loss_params['short_penalty_multiplier'],\n",
    "                        'bias_correction_weight': loss_params['bias_correction_weight'],\n",
    "                        'lr_factor': 0.3,\n",
    "                        'lr_patience': 2,\n",
    "                        'num_epochs': MODEL_PARAMS['num_epochs'],\n",
    "                        'early_stopping_patience': MODEL_PARAMS['early_stopping_patience']\n",
    "                    },\n",
    "                    # Variation with higher dropout\n",
    "                    {\n",
    "                        'hidden_size': MODEL_PARAMS['hidden_size'],\n",
    "                        'num_layers': MODEL_PARAMS['num_layers'],\n",
    "                        'dropout': min(0.7, MODEL_PARAMS['dropout'] * 1.2),  # Increase dropout but cap at 0.7\n",
    "                        'learning_rate': MODEL_PARAMS['learning_rate'],\n",
    "                        'weight_decay': MODEL_PARAMS.get('weight_decay', 1.5e-2),\n",
    "                        'direction_weight': loss_params['direction_weight'] * 1.1,\n",
    "                        'magnitude_weight': loss_params['magnitude_weight'] * 0.9,\n",
    "                        'short_penalty_multiplier': loss_params['short_penalty_multiplier'],\n",
    "                        'bias_correction_weight': loss_params['bias_correction_weight'],\n",
    "                        'lr_factor': 0.3,\n",
    "                        'lr_patience': 2,\n",
    "                        'num_epochs': MODEL_PARAMS['num_epochs'],\n",
    "                        'early_stopping_patience': MODEL_PARAMS['early_stopping_patience']\n",
    "                    },\n",
    "                    # Variation with different architecture\n",
    "                    {\n",
    "                        'hidden_size': MODEL_PARAMS['hidden_size'] * 2,\n",
    "                        'num_layers': 1,\n",
    "                        'dropout': MODEL_PARAMS['dropout'],\n",
    "                        'learning_rate': MODEL_PARAMS['learning_rate'] * 0.5,\n",
    "                        'weight_decay': MODEL_PARAMS.get('weight_decay', 1e-2),\n",
    "                        'direction_weight': loss_params['direction_weight'],\n",
    "                        'magnitude_weight': loss_params['magnitude_weight'],\n",
    "                        'short_penalty_multiplier': loss_params['short_penalty_multiplier'],\n",
    "                        'bias_correction_weight': loss_params['bias_correction_weight'],\n",
    "                        'lr_factor': 0.3,\n",
    "                        'lr_patience': 2,\n",
    "                        'num_epochs': MODEL_PARAMS['num_epochs'],\n",
    "                        'early_stopping_patience': MODEL_PARAMS['early_stopping_patience']\n",
    "                    }\n",
    "                ]\n",
    "                \n",
    "                # Train ensemble models\n",
    "                ensemble_models = []\n",
    "                ensemble_weights = []\n",
    "                best_validation_score = 0\n",
    "                best_model = None\n",
    "                \n",
    "                # Initialize training history for plotting\n",
    "                training_history = TrainingHistory()\n",
    "                \n",
    "                for i, config in enumerate(model_configs):\n",
    "                    logger.info(f\"Training ensemble model {i+1}/{len(model_configs)} with config: {config}\")\n",
    "                    print(f\"Training ensemble model {i+1}/{len(model_configs)}...\")\n",
    "                    \n",
    "                    # Initialize model\n",
    "                    input_size = len(dataset.feature_names)\n",
    "                    model = LSTMModel(\n",
    "                        input_size=input_size,\n",
    "                        hidden_size=config['hidden_size'],\n",
    "                        num_layers=config['num_layers'],\n",
    "                        dropout=config['dropout']\n",
    "                    )\n",
    "                    \n",
    "                    # Initialize training components with improved parameters\n",
    "                    criterion = DirectionalPredictionLoss(\n",
    "                        direction_weight=config['direction_weight'],\n",
    "                        magnitude_weight=config['magnitude_weight'],\n",
    "                        short_penalty_multiplier=config['short_penalty_multiplier'],\n",
    "                        bias_correction_weight=config['bias_correction_weight']\n",
    "                    )\n",
    "                    \n",
    "                    # Set ticker for loss function\n",
    "                    criterion.set_ticker(ticker)\n",
    "                    \n",
    "                    # Create optimizer and scheduler\n",
    "                    optimizer = optim.Adam(\n",
    "                        model.parameters(), \n",
    "                        lr=config['learning_rate'],\n",
    "                        weight_decay=config['weight_decay']\n",
    "                    )\n",
    "                    \n",
    "                    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                        optimizer, \n",
    "                        mode='min', \n",
    "                        factor=config['lr_factor'],\n",
    "                        patience=config['lr_patience'],\n",
    "                        verbose=True,\n",
    "                        min_lr=1e-6\n",
    "                    )\n",
    "                    \n",
    "                    # Track history for this model\n",
    "                    model_history = TrainingHistory()\n",
    "                    \n",
    "                    # Train model\n",
    "                    trained_model = train_model(\n",
    "                        model=model,\n",
    "                        train_loader=train_loader,\n",
    "                        val_loader=val_loader,\n",
    "                        criterion=criterion,\n",
    "                        optimizer=optimizer,\n",
    "                        num_epochs=config['num_epochs'],\n",
    "                        device=device,\n",
    "                        training_history=model_history,\n",
    "                        early_stopping_patience=config['early_stopping_patience'],\n",
    "                        scheduler=scheduler,\n",
    "                        ticker=ticker\n",
    "                    )\n",
    "                    \n",
    "                    # Update main training history for plotting\n",
    "                    if i == 0 or (model_history.validation_loss_history and \n",
    "                                 min(model_history.validation_loss_history) < \n",
    "                                 min(training_history.validation_loss_history or [float('inf')])):\n",
    "                        training_history = model_history\n",
    "                    \n",
    "                    # Evaluate on validation set with range clipping\n",
    "                    y_true_val, y_pred_val, current_prices_val = evaluate_model(\n",
    "                        trained_model, \n",
    "                        val_loader, \n",
    "                        device,\n",
    "                        return_predictions=True,\n",
    "                        evaluator=evaluator,\n",
    "                        ticker=ticker\n",
    "                    )\n",
    "                    \n",
    "                    # Apply range clipping to predictions\n",
    "                    max_pct_change = 0.015  # 1.5% maximum change per minute\n",
    "                    min_bound = current_prices_val * (1 - max_pct_change)\n",
    "                    max_bound = current_prices_val * (1 + max_pct_change)\n",
    "                    y_pred_val = np.clip(y_pred_val, min_bound, max_bound)\n",
    "                    \n",
    "                    # Calculate validation metrics\n",
    "                    val_metrics = evaluator.calculate_metrics(\n",
    "                        y_true=y_true_val,\n",
    "                        y_pred=y_pred_val,\n",
    "                        current_prices=current_prices_val,\n",
    "                        ticker=ticker\n",
    "                    )\n",
    "                    \n",
    "                    # Get validation score for weighting\n",
    "                    direction_accuracy = val_metrics.get('direction_accuracy', 0)\n",
    "                    \n",
    "                    # Save best individual model\n",
    "                    if direction_accuracy > best_validation_score:\n",
    "                        best_validation_score = direction_accuracy\n",
    "                        best_model = trained_model\n",
    "                    \n",
    "                    # Add model to ensemble with more balanced weight\n",
    "                    ensemble_models.append(trained_model)\n",
    "                    \n",
    "                    # Weight based on accuracy above random (50%)\n",
    "                    model_weight = max(0.1, (direction_accuracy - 49) / 10)\n",
    "                    ensemble_weights.append(model_weight)\n",
    "                    \n",
    "                    logger.info(f\"Model {i+1} validation accuracy: {direction_accuracy:.2f}%, weight: {model_weight:.2f}\")\n",
    "                \n",
    "                # Normalize weights\n",
    "                total_weight = sum(ensemble_weights)\n",
    "                ensemble_weights = [w / total_weight for w in ensemble_weights]\n",
    "                \n",
    "                # Create ensemble\n",
    "                ensemble = EnsembleModel(ensemble_models, ensemble_weights)\n",
    "                \n",
    "                # Plot learning curves from best model's history\n",
    "                logger.info(\"Plotting learning curves...\")\n",
    "                plot_learning_curves(training_history, ticker)\n",
    "                plt.savefig(f\"{ticker_dir}/learning_curves_{ticker}.png\")\n",
    "                \n",
    "                # Evaluate ensemble on test set with range clipping\n",
    "                logger.info(\"Evaluating ensemble model...\")\n",
    "                ensemble_metrics, y_true, y_pred, current_prices = ensemble.evaluate(\n",
    "                    test_loader, \n",
    "                    device, \n",
    "                    evaluator=evaluator,\n",
    "                    ticker=ticker\n",
    "                )\n",
    "                \n",
    "                # Apply range clipping\n",
    "                max_pct_change = 0.015  # 1.5% maximum change per minute\n",
    "                min_bound = current_prices * (1 - max_pct_change)\n",
    "                max_bound = current_prices * (1 + max_pct_change)\n",
    "                y_pred = np.clip(y_pred, min_bound, max_bound)\n",
    "                \n",
    "                # Recalculate metrics after clipping\n",
    "                evaluation_metrics = evaluator.calculate_metrics(\n",
    "                    y_true=y_true,\n",
    "                    y_pred=y_pred,\n",
    "                    current_prices=current_prices,\n",
    "                    ticker=ticker\n",
    "                )\n",
    "                \n",
    "                # Plot prediction analysis\n",
    "                plot_prediction_analysis(y_true, y_pred, current_prices, ticker)\n",
    "                plt.savefig(f\"{ticker_dir}/prediction_analysis_{ticker}.png\")\n",
    "                \n",
    "                # Use best individual model for inference\n",
    "                model = best_model\n",
    "                \n",
    "            else:\n",
    "                # Standard single model approach for other tickers\n",
    "                # Initialize model\n",
    "                input_size = len(dataset.feature_names)\n",
    "                model = LSTMModel(\n",
    "                    input_size=input_size,\n",
    "                    hidden_size=MODEL_PARAMS['hidden_size'],\n",
    "                    num_layers=MODEL_PARAMS['num_layers'],\n",
    "                    dropout=MODEL_PARAMS['dropout']\n",
    "                )\n",
    "\n",
    "                # Initialize training components with improved loss function\n",
    "                criterion = DirectionalPredictionLoss(\n",
    "                    direction_weight=loss_params['direction_weight'],\n",
    "                    magnitude_weight=loss_params['magnitude_weight'],\n",
    "                    short_penalty_multiplier=loss_params['short_penalty_multiplier'],\n",
    "                    bias_correction_weight=loss_params['bias_correction_weight']\n",
    "                )\n",
    "                \n",
    "                # Set ticker for loss function\n",
    "                criterion.set_ticker(ticker)\n",
    "                \n",
    "                # Create optimizer with appropriate regularization\n",
    "                optimizer = optim.Adam(\n",
    "                    model.parameters(), \n",
    "                    lr=MODEL_PARAMS['learning_rate'],\n",
    "                    weight_decay=MODEL_PARAMS.get('weight_decay', 1e-2)\n",
    "                )\n",
    "                \n",
    "                # Create responsive scheduler\n",
    "                scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                    optimizer, \n",
    "                    mode='min', \n",
    "                    factor=0.3,\n",
    "                    patience=2,\n",
    "                    verbose=True,\n",
    "                    min_lr=1e-6\n",
    "                )\n",
    "                \n",
    "                # Track training history\n",
    "                training_history = TrainingHistory()\n",
    "                \n",
    "                # Train model\n",
    "                logger.info(\"Starting model training...\")\n",
    "                print(\"Training model with enhanced direction-specific loss...\")\n",
    "                model = train_model(\n",
    "                    model=model,\n",
    "                    train_loader=train_loader,\n",
    "                    val_loader=val_loader,\n",
    "                    criterion=criterion,\n",
    "                    optimizer=optimizer,\n",
    "                    num_epochs=MODEL_PARAMS['num_epochs'],\n",
    "                    device=device,\n",
    "                    training_history=training_history,\n",
    "                    early_stopping_patience=MODEL_PARAMS['early_stopping_patience'],\n",
    "                    scheduler=scheduler,\n",
    "                    ticker=ticker\n",
    "                )\n",
    "\n",
    "                # Plot learning curves\n",
    "                logger.info(\"Plotting learning curves...\")\n",
    "                plot_learning_curves(training_history, ticker)\n",
    "                plt.savefig(f\"{ticker_dir}/learning_curves_{ticker}.png\")\n",
    "\n",
    "                # Evaluate model with range clipping\n",
    "                logger.info(\"Evaluating model...\")\n",
    "                y_true, y_pred, current_prices = evaluate_model(\n",
    "                    model, \n",
    "                    test_loader, \n",
    "                    device,\n",
    "                    return_predictions=True,\n",
    "                    evaluator=evaluator,\n",
    "                    ticker=ticker\n",
    "                )\n",
    "                \n",
    "                # Apply range clipping\n",
    "                max_pct_change = 0.015  # 1.5% maximum change per minute\n",
    "                min_bound = current_prices * (1 - max_pct_change)\n",
    "                max_bound = current_prices * (1 + max_pct_change)\n",
    "                y_pred = np.clip(y_pred, min_bound, max_bound)\n",
    "                \n",
    "                # Calculate metrics after clipping\n",
    "                evaluation_metrics = evaluator.calculate_metrics(\n",
    "                    y_true=y_true,\n",
    "                    y_pred=y_pred,\n",
    "                    current_prices=current_prices,\n",
    "                    ticker=ticker\n",
    "                )\n",
    "                \n",
    "                # Plot prediction analysis\n",
    "                plot_prediction_analysis(y_true, y_pred, current_prices, ticker)\n",
    "                plt.savefig(f\"{ticker_dir}/prediction_analysis_{ticker}.png\")\n",
    "            \n",
    "            # Log evaluation metrics\n",
    "            logger.info(f\"Model Evaluation Metrics for {ticker}:\")\n",
    "            for metric, value in evaluation_metrics.items():\n",
    "                logger.info(f\"  {metric}: {value}\")\n",
    "            \n",
    "            # Validate model quality before generating signals\n",
    "            model_valid = validate_model_quality(evaluation_metrics, ticker)\n",
    "            if not model_valid:\n",
    "                print(f\"Model for {ticker} failed quality validation. Skipping signal generation.\")\n",
    "                continue\n",
    "            \n",
    "            # Update signal generator with direction-specific accuracies\n",
    "            signal_generator.update_direction_confidence(evaluation_metrics)\n",
    "            \n",
    "            # Generate trading signals using stock-specific signal generator\n",
    "            logger.info(\"Generating trading signals...\")\n",
    "            \n",
    "            # Use original (non-balanced) data for signal generation and backtesting\n",
    "            stock_df_orig = enhance_features(stock_df, market_index_df)\n",
    "            \n",
    "            # Prepare test period for signal generation\n",
    "            test_period = stock_df_orig.iloc[-len(y_true):]\n",
    "            test_timestamps = test_period.index\n",
    "            \n",
    "            print(f\"Generating signals for {len(test_timestamps)} timestamps...\")\n",
    "            \n",
    "            # Reset monitor for clean signal generation\n",
    "            monitor = MultiTickerMonitor(signal_generator)\n",
    "            all_signals = []\n",
    "            \n",
    "            # Process test data in batches for signal generation\n",
    "            batch_size = 1000\n",
    "            num_batches = (len(test_timestamps) + batch_size - 1) // batch_size\n",
    "            \n",
    "            for batch_idx in range(num_batches):\n",
    "                start_idx = batch_idx * batch_size\n",
    "                end_idx = min((batch_idx + 1) * batch_size, len(test_timestamps))\n",
    "                \n",
    "                print(f\"Processing batch {batch_idx+1}/{num_batches} (indices {start_idx}-{end_idx})...\")\n",
    "                \n",
    "                for idx in range(start_idx, end_idx):\n",
    "                    timestamp = test_timestamps[idx]\n",
    "                    actual = y_true[idx - start_idx]\n",
    "                    predicted = y_pred[idx - start_idx]\n",
    "                    current_price = current_prices[idx - start_idx] if current_prices is not None else actual\n",
    "                    \n",
    "                    # Print progress update for large datasets\n",
    "                    if (idx - start_idx) % 1000 == 0:\n",
    "                        print(f\"Processing index {idx}: Current price = {actual:.2f}, Predicted price = {predicted:.2f}\")\n",
    "                    \n",
    "                    # Get market data window for signal generation\n",
    "                    window_start_idx = max(0, stock_df_orig.index.get_indexer([timestamp])[0] - 20)\n",
    "                    window_end_idx = stock_df_orig.index.get_indexer([timestamp])[0] + 1\n",
    "                    market_data_window = stock_df_orig.iloc[window_start_idx:window_end_idx].copy()\n",
    "                    \n",
    "                    if len(market_data_window) == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    # Apply optimized bias correction if available\n",
    "                    if ENABLE_OPTIMIZATION and 'bias_correction_params' in locals():\n",
    "                        # Custom bias correction using optimized parameters\n",
    "                        market_regime = signal_generator.detect_market_regime(market_data_window)\n",
    "                        price_changes = market_data_window['close'].pct_change().dropna()\n",
    "                        mean_price_change = price_changes[-20:].mean() if len(price_changes) >= 20 else 0\n",
    "                        \n",
    "                        if market_regime in ['trending_up', 'high_volatility']:\n",
    "                            correction = current_price * bias_correction_params['downward_bias_correction_high']\n",
    "                        else:\n",
    "                            correction = current_price * bias_correction_params['downward_bias_correction_low']\n",
    "                        \n",
    "                        if mean_price_change > 0:\n",
    "                            adaptive_factor = min(bias_correction_params['adaptive_factor_cap'], mean_price_change)\n",
    "                            correction += current_price * adaptive_factor\n",
    "                    else:\n",
    "                        # Apply default correction based on evaluator\n",
    "                        correction = evaluator.get_adaptive_correction(ticker) * 0.5  # Reduced by 50%\n",
    "                    \n",
    "                    corrected_prediction = predicted - correction\n",
    "                    \n",
    "                    # Apply range clipping\n",
    "                    max_pct_change = 0.015  # 1.5% maximum change per minute\n",
    "                    min_bound = current_price * (1 - max_pct_change)\n",
    "                    max_bound = current_price * (1 + max_pct_change)\n",
    "                    clipped_prediction = np.clip(corrected_prediction, min_bound, max_bound)\n",
    "                    \n",
    "                    try:\n",
    "                        # Generate signal with proper error handling\n",
    "                        signal = monitor.update_ticker(\n",
    "                            ticker=ticker,\n",
    "                            current_price=actual,\n",
    "                            predicted_price=clipped_prediction,\n",
    "                            timestamp=timestamp,\n",
    "                            market_data=market_data_window,\n",
    "                            prediction_metrics=evaluation_metrics\n",
    "                        )\n",
    "                        \n",
    "                        if signal and signal['action']:\n",
    "                            if (idx - start_idx) < 10 or (idx - start_idx) % 500 == 0:  # Limit logging for clarity\n",
    "                                print(f\"Generated signal at {timestamp}: {signal['action']}\")\n",
    "                            all_signals.append(signal)\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Error generating signal at {timestamp}: {e}\")\n",
    "                        # Continue processing despite errors\n",
    "                        continue\n",
    "            \n",
    "            print(f\"Generated {len(all_signals)} signals\")\n",
    "            \n",
    "            # Add diagnostic information if no signals were generated\n",
    "            if len(all_signals) == 0:\n",
    "                print(\"\\nDiagnostic information for signal generation:\")\n",
    "                # Check the first few test samples\n",
    "                for idx in range(min(5, len(test_timestamps))):\n",
    "                    timestamp = test_timestamps[idx]\n",
    "                    actual = y_true[idx]\n",
    "                    predicted = y_pred[idx]\n",
    "                    price_change_pct = (predicted - actual) / actual\n",
    "                    \n",
    "                    # Get market data window\n",
    "                    window_start_idx = max(0, stock_df_orig.index.get_indexer([timestamp])[0] - 20)\n",
    "                    window_end_idx = stock_df_orig.index.get_indexer([timestamp])[0] + 1\n",
    "                    market_data_window = stock_df_orig.iloc[window_start_idx:window_end_idx].copy()\n",
    "                    \n",
    "                    # Check trend condition\n",
    "                    if 'MA5' not in market_data_window.columns:\n",
    "                        market_data_window['MA5'] = market_data_window['close'].rolling(5).mean()\n",
    "                    if 'SMA_20' not in market_data_window.columns:\n",
    "                        market_data_window['SMA_20'] = market_data_window['close'].rolling(20).mean()\n",
    "                    \n",
    "                    ma5 = market_data_window['MA5'].iloc[-1] if not market_data_window['MA5'].empty else None\n",
    "                    ma20 = market_data_window['SMA_20'].iloc[-1] if not market_data_window['SMA_20'].empty else None\n",
    "                    \n",
    "                    if ma5 is not None and ma20 is not None:\n",
    "                        trend_strength = abs((ma5 - ma20) / ma20)\n",
    "                        trend_threshold = signal_generator.params['trend_threshold']\n",
    "                        trend_ok = trend_strength <= trend_threshold\n",
    "                    else:\n",
    "                        trend_strength = None\n",
    "                        trend_threshold = signal_generator.params['trend_threshold']\n",
    "                        trend_ok = False\n",
    "                    \n",
    "                    # Check volume condition\n",
    "                    if 'volume' in market_data_window.columns and not market_data_window['volume'].empty:\n",
    "                        current_volume = market_data_window['volume'].iloc[-1]\n",
    "                        avg_volume = market_data_window['volume'].rolling(20).mean().iloc[-1]\n",
    "                        volume_ratio = current_volume / avg_volume if not pd.isna(avg_volume) and avg_volume > 0 else 0\n",
    "                        volume_threshold = signal_generator.params['volume_threshold']\n",
    "                        volume_ok = volume_ratio >= volume_threshold\n",
    "                    else:\n",
    "                        volume_ratio = None\n",
    "                        volume_threshold = signal_generator.params['volume_threshold']\n",
    "                        volume_ok = False\n",
    "                    \n",
    "                    # Check signal thresholds\n",
    "                    thresholds = signal_generator.calculate_thresholds(\n",
    "                        signal_generator.current_atr or 0.001,\n",
    "                        'neutral',\n",
    "                        signal_generator.direction_confidence\n",
    "                    )\n",
    "                    \n",
    "                    print(f\"\\nTimestamp {idx}: {timestamp}\")\n",
    "                    print(f\"  Current price: {actual:.2f}, Predicted: {predicted:.2f}\")\n",
    "                    print(f\"  Price change %: {price_change_pct*100:.4f}% (need ±{thresholds['long_entry']*100:.4f}% for long, ±{thresholds['short_entry']*100:.4f}% for short)\")\n",
    "                    print(f\"  Trend check: {'PASS' if trend_ok else 'FAIL'} (strength: {trend_strength:.4f if trend_strength is not None else 'N/A'}, threshold: {trend_threshold:.4f})\")\n",
    "                    print(f\"  Volume check: {'PASS' if volume_ok else 'FAIL'} (ratio: {volume_ratio:.2f if volume_ratio is not None else 'N/A'}, threshold: {volume_threshold:.2f})\")\n",
    "                    print(f\"  Signal would be generated: {'Yes' if abs(price_change_pct) > thresholds['long_entry'] and trend_ok and volume_ok else 'No'}\")\n",
    "            \n",
    "            # Perform backtesting with realistic transaction costs\n",
    "            try:\n",
    "                trades, total_commission, total_slippage = backtest_trades_with_costs(\n",
    "                    all_signals, \n",
    "                    stock_df_orig,\n",
    "                    commission_rate=TRANSACTION_COSTS['commission_rate'],\n",
    "                    slippage_factor=TRANSACTION_COSTS['slippage_factor']\n",
    "                )\n",
    "                print(f\"Generated {len(trades)} trades\")\n",
    "                \n",
    "                # Calculate trade performance metrics\n",
    "                trade_metrics = evaluator.calculate_trade_performance_metrics(\n",
    "                    trades=trades,\n",
    "                    initial_capital=10000.0,\n",
    "                    include_transaction_costs=True\n",
    "                )\n",
    "                \n",
    "                # Log costs\n",
    "                logger.info(f\"Total commission: ${total_commission:.2f}, Total slippage: ${total_slippage:.2f}\")\n",
    "                \n",
    "                # Print performance summary\n",
    "                print(\"\\nTrading Performance Summary:\")\n",
    "                print(f\"Total Trades: {trade_metrics['total_trades']}\")\n",
    "                print(f\"Win Rate: {trade_metrics['win_rate']:.2f}%\")\n",
    "                print(f\"Long Trades: {trade_metrics.get('long_trades', 0)}, Short Trades: {trade_metrics.get('short_trades', 0)}\")\n",
    "                print(f\"Long Win Rate: {trade_metrics.get('long_win_rate', 0):.2f}%, Short Win Rate: {trade_metrics.get('short_win_rate', 0):.2f}%\")\n",
    "                print(f\"Total Profit: ${trade_metrics['total_profit']:.2f}\")\n",
    "                print(f\"Maximum Drawdown: ${trade_metrics['maximum_drawdown']:.2f}\")\n",
    "                print(f\"Sharpe Ratio: {trade_metrics['sharpe_ratio']:.2f}\")\n",
    "                print(f\"Transaction Costs: ${total_commission + total_slippage:.2f}\")\n",
    "                \n",
    "                # Generate visualizations\n",
    "                plot_trading_metrics(trade_metrics, ticker)\n",
    "                plt.savefig(f\"{ticker_dir}/trading_metrics_{ticker}.png\")\n",
    "                \n",
    "                plot_candlestick_analysis(stock_df_orig, signals=all_signals, trades=trades, ticker=ticker)\n",
    "                plt.savefig(f\"{ticker_dir}/candlestick_analysis_{ticker}.png\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in backtesting for {ticker}: {e}\")\n",
    "                traceback.print_exc()\n",
    "            \n",
    "            # Save model and artifacts\n",
    "            try:\n",
    "                model_filename = f'{ticker_dir}/lstm_model_{ticker}_{end_date_str}.pth'\n",
    "                scaler_filename = f'{ticker_dir}/scaler_{ticker}_{end_date_str}.pkl'\n",
    "                history_filename = f'{ticker_dir}/training_history_{ticker}_{end_date_str}.pkl'\n",
    "                metrics_filename = f'{ticker_dir}/performance_metrics_{ticker}_{end_date_str}.pkl'\n",
    "                signals_filename = f'{ticker_dir}/signals_{ticker}_{end_date_str}.pkl'\n",
    "                \n",
    "                # Save model\n",
    "                torch.save(model.state_dict(), model_filename)\n",
    "                \n",
    "                # Save scaler\n",
    "                with open(scaler_filename, 'wb') as f:\n",
    "                    pickle.dump(dataset.scaler, f)\n",
    "                    \n",
    "                # Save training history\n",
    "                with open(history_filename, 'wb') as f:\n",
    "                    pickle.dump(training_history, f)\n",
    "                \n",
    "                # Save signals and trades\n",
    "                with open(signals_filename, 'wb') as f:\n",
    "                    signals_data = {\n",
    "                        'signals': all_signals,\n",
    "                        'trades': trades if 'trades' in locals() else []\n",
    "                    }\n",
    "                    pickle.dump(signals_data, f)\n",
    "                    \n",
    "                # Save performance metrics\n",
    "                with open(metrics_filename, 'wb') as f:\n",
    "                    metrics_data = {\n",
    "                        'evaluation_metrics': evaluation_metrics,\n",
    "                        'trade_metrics': trade_metrics if 'trade_metrics' in locals() else {},\n",
    "                        'transaction_costs': {\n",
    "                            'commission': total_commission if 'total_commission' in locals() else 0,\n",
    "                            'slippage': total_slippage if 'total_slippage' in locals() else 0\n",
    "                        }\n",
    "                    }\n",
    "                    pickle.dump(metrics_data, f)\n",
    "                    \n",
    "                print(f\"\\nSaved model and artifacts to {ticker_dir}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error saving artifacts for {ticker}: {e}\")\n",
    "                traceback.print_exc()\n",
    "            \n",
    "            logger.info(f\"Completed processing for {ticker}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing {ticker}: {e}\")\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "            \n",
    "        finally:\n",
    "            # Clean up memory\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    logger.info(\"Completed processing all tickers\")\n",
    "    print(\"\\nTrading model training pipeline completed!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "   main()        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
