{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential Imports\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import mplfinance as mpf\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "import ta\n",
    "import logging\n",
    "import gc\n",
    "import traceback\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional, Dict, Tuple\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import matplotlib.gridspec as gridspec\n",
    "from ta.volatility import BollingerBands\n",
    "from ta.trend import MACD\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F  # This import was missing\n",
    "import mplfinance\n",
    "\n",
    "# Suppress mplfinance warnings for too much data\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"mplfinance\")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    filename='structured_prediction_hft.log',\n",
    "    filemode='a',\n",
    "    format='%(asctime)s:%(levelname)s:%(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "# API Configuration\n",
    "API_KEY = os.getenv(\"POLYGON_API_KEY\")\n",
    "\n",
    "@dataclass\n",
    "class TrainingHistory:\n",
    "    \"\"\"Tracks training metrics during model training.\"\"\"\n",
    "    loss_history: List[float] = field(default_factory=list)\n",
    "    validation_loss_history: List[float] = field(default_factory=list)\n",
    "\n",
    "    def update(self, epoch: int, loss: float, val_loss: float):\n",
    "        \"\"\"Update training history with new metrics.\"\"\"\n",
    "        self.loss_history.append(loss)\n",
    "        self.validation_loss_history.append(val_loss)\n",
    "\n",
    "class ModelEvaluator:\n",
    "    def __init__(self, prediction_threshold: float = 0.001):\n",
    "        self.prediction_threshold = prediction_threshold\n",
    "        self.mse_scores = []\n",
    "        self.mae_scores = []\n",
    "        self.seasonal_errors = []\n",
    "\n",
    "    def calculate_metrics(self, y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"Calculate various error metrics based on predictions.\"\"\"\n",
    "        metrics = {}\n",
    "        \n",
    "        # Ensure inputs are 1-D and matching length\n",
    "        y_true = y_true.flatten()\n",
    "        y_pred = y_pred.flatten()\n",
    "        \n",
    "        if len(y_true) != len(y_pred):\n",
    "            raise ValueError(\"y_true and y_pred must have the same length\")\n",
    "            \n",
    "        # Handle invalid values\n",
    "        if np.isnan(y_true).any() or np.isnan(y_pred).any():\n",
    "            raise ValueError(\"Inputs contain NaN values\")\n",
    "        if np.isinf(y_true).any() or np.isinf(y_pred).any():\n",
    "            raise ValueError(\"Inputs contain infinite values\")\n",
    "            \n",
    "        try:\n",
    "            # Direction Accuracy\n",
    "            direction_true = np.sign(y_true[1:] - y_true[:-1])\n",
    "            direction_pred = np.sign(y_pred[1:] - y_pred[:-1])\n",
    "            direction_accuracy = np.mean(direction_true == direction_pred) * 100\n",
    "            metrics['direction_accuracy'] = direction_accuracy\n",
    "            \n",
    "            # Magnitude Correlation\n",
    "            magnitude_correlation = np.corrcoef(y_true, y_pred)[0, 1]\n",
    "            metrics['magnitude_correlation'] = magnitude_correlation\n",
    "            \n",
    "            # Timing Accuracy\n",
    "            trend_changes_true = np.where(direction_true != 0)[0]\n",
    "            trend_changes_pred = np.where(direction_pred != 0)[0]\n",
    "            if len(trend_changes_true) > 0:\n",
    "                timing_accuracy = len(set(trend_changes_true).intersection(set(trend_changes_pred))) / len(trend_changes_true) * 100\n",
    "            else:\n",
    "                timing_accuracy = 0.0\n",
    "            metrics['timing_accuracy'] = timing_accuracy\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in calculate_metrics: {e}\")\n",
    "            raise\n",
    "            \n",
    "        return metrics\n",
    "\n",
    "    def calculate_trade_performance_metrics(self, trades: List[Dict], initial_capital: float = 10000.0) -> Dict[str, float]:\n",
    "        \"\"\"Calculate comprehensive performance metrics for trades.\"\"\"\n",
    "        if not trades:\n",
    "            return {\n",
    "                'total_trades': 0,\n",
    "                'total_profit': 0.0,\n",
    "                'win_rate': 0.0,\n",
    "                'loss_rate': 0.0,\n",
    "                'average_profit_per_trade': 0.0,\n",
    "                'maximum_drawdown': 0.0,\n",
    "                'profit_factor': 0.0,\n",
    "                'sharpe_ratio': 0.0\n",
    "            }\n",
    "\n",
    "        profits = []\n",
    "        daily_returns = defaultdict(float)\n",
    "\n",
    "        # Calculate trade results\n",
    "        for trade in trades:\n",
    "            if trade['position'] == 'enter_long':\n",
    "                profit = trade['exit_price'] - trade['entry_price']\n",
    "            else:  # enter_short\n",
    "                profit = trade['entry_price'] - trade['exit_price']\n",
    "                \n",
    "            profits.append(profit)\n",
    "            trade_date = trade['exit_time'].date()\n",
    "            daily_returns[trade_date] += profit\n",
    "\n",
    "        # Calculate metrics\n",
    "        total_profit = sum(profits)\n",
    "        winning_trades = [p for p in profits if p > 0]\n",
    "        losing_trades = [p for p in profits if p < 0]\n",
    "        \n",
    "        win_rate = len(winning_trades) / len(profits) * 100 if profits else 0\n",
    "        loss_rate = len(losing_trades) / len(profits) * 100 if profits else 0\n",
    "        \n",
    "        avg_win = np.mean(winning_trades) if winning_trades else 0\n",
    "        avg_loss = abs(np.mean(losing_trades)) if losing_trades else 0\n",
    "        \n",
    "        # Calculate daily metrics\n",
    "        daily_returns_list = list(daily_returns.values())\n",
    "        sharpe_ratio = (np.mean(daily_returns_list) / np.std(daily_returns_list) * np.sqrt(252)\n",
    "                       if len(daily_returns_list) > 1 else 0)\n",
    "\n",
    "        return {\n",
    "            'total_trades': len(trades),\n",
    "            'total_profit': round(total_profit, 2),\n",
    "            'win_rate': round(win_rate, 2),\n",
    "            'loss_rate': round(loss_rate, 2),\n",
    "            'average_profit_per_trade': round(np.mean(profits), 2),\n",
    "            'maximum_drawdown': round(abs(min(np.minimum.accumulate(np.cumsum(profits)))), 2),\n",
    "            'profit_factor': round(sum(winning_trades) / abs(sum(losing_trades)) if losing_trades else float('inf'), 2),\n",
    "            'sharpe_ratio': round(sharpe_ratio, 2)\n",
    "        }\n",
    "\n",
    "class AdaptiveSignalGenerator:\n",
    "    def __init__(self, ticker: str):\n",
    "        self.ticker = ticker\n",
    "        self.stock_profiles = {\n",
    "            'MSFT': {\n",
    "                'base_entry_threshold': 0.0008,\n",
    "                'base_exit_threshold': 0.0006,\n",
    "                'base_stop_loss': 0.0015,\n",
    "                'atr_multiplier': 1.2,\n",
    "                'min_hold_time': 5,\n",
    "                'max_daily_trades': 20,\n",
    "                'position_size': 0.2,\n",
    "                'volatility_threshold': 0.8,\n",
    "                'volume_threshold': 1.0,\n",
    "                'trend_threshold': 0.02\n",
    "            },\n",
    "            'GOOGL': {\n",
    "                'base_entry_threshold': 0.001,\n",
    "                'base_exit_threshold': 0.0007,\n",
    "                'base_stop_loss': 0.002,\n",
    "                'atr_multiplier': 1.3,\n",
    "                'min_hold_time': 3,\n",
    "                'max_daily_trades': 25,\n",
    "                'position_size': 0.2,\n",
    "                'volatility_threshold': 0.9,\n",
    "                'volume_threshold': 1.0,\n",
    "                'trend_threshold': 0.02\n",
    "            },\n",
    "            'TSLA': {\n",
    "                'base_entry_threshold': 0.003,\n",
    "                'base_exit_threshold': 0.002,\n",
    "                'base_stop_loss': 0.004,\n",
    "                'atr_multiplier': 2.0,\n",
    "                'min_hold_time': 2,\n",
    "                'max_daily_trades': 15,\n",
    "                'position_size': 0.1,\n",
    "                'volatility_threshold': 1.5,\n",
    "                'volume_threshold': 1.5,\n",
    "                'trend_threshold': 0.03\n",
    "            },\n",
    "            'NVDA': {\n",
    "                'base_entry_threshold': 0.0025,\n",
    "                'base_exit_threshold': 0.0018,\n",
    "                'base_stop_loss': 0.0035,\n",
    "                'atr_multiplier': 1.8,\n",
    "                'min_hold_time': 2,\n",
    "                'max_daily_trades': 18,\n",
    "                'position_size': 0.12,\n",
    "                'volatility_threshold': 1.3,\n",
    "                'volume_threshold': 1.3,\n",
    "                'trend_threshold': 0.025\n",
    "            },\n",
    "            'TQQQ': {\n",
    "                'base_entry_threshold': 0.005,\n",
    "                'base_exit_threshold': 0.004,\n",
    "                'base_stop_loss': 0.007,\n",
    "                'atr_multiplier': 3.0,\n",
    "                'min_hold_time': 2,\n",
    "                'max_daily_trades': 10,\n",
    "                'position_size': 0.06,\n",
    "                'volatility_threshold': 2.5,\n",
    "                'volume_threshold': 2.0,\n",
    "                'trend_threshold': 0.05\n",
    "            },\n",
    "            'SQQQ': {\n",
    "                'base_entry_threshold': 0.005,\n",
    "                'base_exit_threshold': 0.004,\n",
    "                'base_stop_loss': 0.007,\n",
    "                'atr_multiplier': 3.0,\n",
    "                'min_hold_time': 2,\n",
    "                'max_daily_trades': 8,\n",
    "                'position_size': 0.06,\n",
    "                'volatility_threshold': 2.5,\n",
    "                'volume_threshold': 2.2,\n",
    "                'trend_threshold': 0.05\n",
    "            },\n",
    "            'QLD': {\n",
    "                'base_entry_threshold': 0.003,\n",
    "                'base_exit_threshold': 0.002,\n",
    "                'base_stop_loss': 0.004,\n",
    "                'atr_multiplier': 2.0,\n",
    "                'min_hold_time': 2,\n",
    "                'max_daily_trades': 15,\n",
    "                'position_size': 0.10,\n",
    "                'volatility_threshold': 1.7,\n",
    "                'volume_threshold': 1.5,\n",
    "                'trend_threshold': 0.035\n",
    "            },\n",
    "            'PSQ': {\n",
    "                'base_entry_threshold': 0.003,\n",
    "                'base_exit_threshold': 0.002,\n",
    "                'base_stop_loss': 0.004,\n",
    "                'atr_multiplier': 1.8,\n",
    "                'min_hold_time': 4,\n",
    "                'max_daily_trades': 15,\n",
    "                'position_size': 0.15,\n",
    "                'volatility_threshold': 1.4,\n",
    "                'volume_threshold': 1.4,\n",
    "                'trend_threshold': 0.03\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        self.params = self.stock_profiles[ticker]\n",
    "        self._initialize_parameters()\n",
    "        \n",
    "        \n",
    "    def _initialize_parameters(self):\n",
    "        \"\"\"Initialize trading parameters from stock profile.\"\"\"\n",
    "        self.base_entry_threshold = self.params['base_entry_threshold']\n",
    "        self.base_exit_threshold = self.params['base_exit_threshold']\n",
    "        self.base_stop_loss = self.params['base_stop_loss']\n",
    "        self.atr_multiplier = self.params['atr_multiplier']\n",
    "        self.min_hold_time = self.params['min_hold_time']\n",
    "        self.volatility_threshold = self.params['volatility_threshold']\n",
    "        \n",
    "        self.position = None\n",
    "        self.entry_price = None\n",
    "        self.entry_time = None\n",
    "        self.current_atr = None\n",
    "        self.daily_trades = 0\n",
    "        self.last_trade_date = None\n",
    "        \n",
    "    def calculate_atr(self, market_data: pd.DataFrame) -> float:\n",
    "        \"\"\"Calculate Average True Range.\"\"\"\n",
    "        high = market_data['high'].values\n",
    "        low = market_data['low'].values\n",
    "        close = pd.Series(market_data['close'].values)\n",
    "        \n",
    "        tr1 = pd.Series(high - low)\n",
    "        tr2 = pd.Series(abs(high - close.shift()))\n",
    "        tr3 = pd.Series(abs(low - close.shift()))\n",
    "        \n",
    "        tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
    "        atr = tr.rolling(window=14).mean().iloc[-1]\n",
    "        \n",
    "        return atr\n",
    "        \n",
    "    def calculate_thresholds(self, current_volatility: float) -> Tuple[float, float, float]:\n",
    "        \"\"\"Calculate adaptive thresholds based on current volatility.\"\"\"\n",
    "        volatility_factor = current_volatility * self.atr_multiplier\n",
    "        \n",
    "        entry_threshold = self.base_entry_threshold * (1 + volatility_factor)\n",
    "        exit_threshold = self.base_exit_threshold * (1 + volatility_factor)\n",
    "        stop_loss = self.base_stop_loss * (1 + volatility_factor)\n",
    "        \n",
    "        return entry_threshold, exit_threshold, stop_loss\n",
    "        \n",
    "    def check_trend_condition(self, market_data: pd.DataFrame) -> bool:\n",
    "        \"\"\"Check if trend conditions are favorable.\"\"\"\n",
    "        ma5 = market_data['MA5'].iloc[-1]\n",
    "        ma20 = market_data['SMA_20'].iloc[-1]\n",
    "        \n",
    "        trend_strength = abs((ma5 - ma20) / ma20)\n",
    "        \n",
    "        if self.ticker in ['TSLA', 'NVDA']:\n",
    "            return trend_strength <= self.params['trend_threshold'] * 1.5\n",
    "        else:\n",
    "            return trend_strength <= self.params['trend_threshold']\n",
    "            \n",
    "    def check_volume_condition(self, market_data: pd.DataFrame) -> bool:\n",
    "        \"\"\"Check if volume conditions are favorable.\"\"\"\n",
    "        current_volume = market_data['volume'].iloc[-1]\n",
    "        avg_volume = market_data['volume'].rolling(20).mean().iloc[-1]\n",
    "        volume_ratio = current_volume / avg_volume\n",
    "        \n",
    "        return volume_ratio >= self.params['volume_threshold']\n",
    "        \n",
    "    def generate_signals(self, ticker: str, current_price: float, \n",
    "                        predicted_price: float, timestamp: pd.Timestamp,\n",
    "                        market_data: pd.DataFrame,\n",
    "                        order_book_data: Dict = None) -> Dict:\n",
    "        \"\"\"Generate trading signals with stock-specific adaptations.\"\"\"\n",
    "        # Reset daily trades if new day\n",
    "        current_date = timestamp.date()\n",
    "        if self.last_trade_date != current_date:\n",
    "            self.daily_trades = 0\n",
    "            self.last_trade_date = current_date\n",
    "            \n",
    "        # Check trade frequency limit\n",
    "        if self.daily_trades >= self.params['max_daily_trades']:\n",
    "            return self._create_signal(ticker, timestamp, current_price)\n",
    "            \n",
    "        # Check minimum hold time\n",
    "        if self.position and self.entry_time:\n",
    "            hold_time = (timestamp - self.entry_time).total_seconds() / 60\n",
    "            if hold_time < self.params['min_hold_time']:\n",
    "                return self._create_signal(ticker, timestamp, current_price)\n",
    "        \n",
    "        try:\n",
    "            self.current_atr = self.calculate_atr(market_data)\n",
    "            entry_threshold, exit_threshold, stop_loss = self.calculate_thresholds(self.current_atr)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating thresholds: {e}\")\n",
    "            return self._create_signal(ticker, timestamp, current_price)\n",
    "        \n",
    "        price_change_pct = (predicted_price - current_price) / current_price\n",
    "        signal = self._create_signal(ticker, timestamp, current_price)\n",
    "        \n",
    "        if self.position is None:\n",
    "            trend_ok = self.check_trend_condition(market_data)\n",
    "            volume_ok = self.check_volume_condition(market_data)\n",
    "            \n",
    "            if abs(price_change_pct) > entry_threshold and trend_ok and volume_ok:\n",
    "                if price_change_pct > 0:\n",
    "                    self.position = 'long'\n",
    "                    self.entry_price = current_price\n",
    "                    self.entry_time = timestamp\n",
    "                    signal['action'] = 'enter_long'\n",
    "                    signal['position'] = 'long'\n",
    "                    self.daily_trades += 1\n",
    "                else:\n",
    "                    self.position = 'short'\n",
    "                    self.entry_price = current_price\n",
    "                    self.entry_time = timestamp\n",
    "                    signal['action'] = 'enter_short'\n",
    "                    signal['position'] = 'short'\n",
    "                    self.daily_trades += 1\n",
    "        else:\n",
    "            if self.position == 'long':\n",
    "                price_change_from_entry = (current_price - self.entry_price) / self.entry_price\n",
    "                if (price_change_from_entry <= -stop_loss or \n",
    "                    price_change_from_entry >= exit_threshold):\n",
    "                    self.position = None\n",
    "                    self.entry_price = None\n",
    "                    self.entry_time = None\n",
    "                    signal['action'] = 'exit_long'\n",
    "                    signal['position'] = 'long'\n",
    "            elif self.position == 'short':\n",
    "                price_change_from_entry = (self.entry_price - current_price) / self.entry_price\n",
    "                if (price_change_from_entry <= -stop_loss or \n",
    "                    price_change_from_entry >= exit_threshold):\n",
    "                    self.position = None\n",
    "                    self.entry_price = None\n",
    "                    self.entry_time = None\n",
    "                    signal['action'] = 'exit_short'\n",
    "                    signal['position'] = 'short'\n",
    "        \n",
    "        return signal\n",
    "        \n",
    "    def _create_signal(self, ticker: str, timestamp: pd.Timestamp, price: float) -> Dict:\n",
    "        \"\"\"Create a base signal dictionary.\"\"\"\n",
    "        return {\n",
    "            'ticker': ticker,\n",
    "            'timestamp': timestamp,\n",
    "            'price': price,\n",
    "            'action': None,\n",
    "            'position': None,\n",
    "            'thresholds': {\n",
    "                'entry': self.base_entry_threshold,\n",
    "                'exit': self.base_exit_threshold,\n",
    "                'stop_loss': self.base_stop_loss\n",
    "            },\n",
    "            'market_conditions': {\n",
    "                'atr': self.current_atr,\n",
    "                'daily_trades': self.daily_trades\n",
    "            }\n",
    "        }\n",
    "\n",
    "class MultiTickerMonitor:\n",
    "    def __init__(self, signal_generator: AdaptiveSignalGenerator):\n",
    "        self.tracked_tickers = {}\n",
    "        self.signal_generator = signal_generator\n",
    "\n",
    "    def add_ticker(self, ticker: str, initial_price: float):\n",
    "        \"\"\"Add a new ticker to monitor.\"\"\"\n",
    "        self.tracked_tickers[ticker] = {\n",
    "            'current_price': initial_price,\n",
    "            'signals': [],\n",
    "            'last_update': datetime.now()\n",
    "        }\n",
    "\n",
    "    def update_ticker(self, ticker: str, current_price: float,\n",
    "                     predicted_price: float, timestamp: pd.Timestamp,\n",
    "                     market_data: pd.DataFrame,\n",
    "                     order_book_data: Dict = None) -> Optional[Dict]:\n",
    "        \"\"\"Update ticker information and generate signals.\"\"\"\n",
    "        if ticker not in self.tracked_tickers:\n",
    "            self.add_ticker(ticker, current_price)\n",
    "    \n",
    "        self.tracked_tickers[ticker]['current_price'] = current_price\n",
    "        self.tracked_tickers[ticker]['last_update'] = timestamp\n",
    "    \n",
    "        signal = self.signal_generator.generate_signals(\n",
    "            ticker=ticker,\n",
    "            current_price=current_price,\n",
    "            predicted_price=predicted_price,\n",
    "            timestamp=timestamp,\n",
    "            market_data=market_data,\n",
    "            order_book_data=order_book_data\n",
    "        )\n",
    "    \n",
    "        if signal and signal['action']:\n",
    "            self.tracked_tickers[ticker]['signals'].append(signal)\n",
    "            return signal\n",
    "    \n",
    "        return None\n",
    "\n",
    "    def get_signals(self, ticker: str) -> List[Dict]:\n",
    "        \"\"\"Retrieve signals for a specific ticker.\"\"\"\n",
    "        return self.tracked_tickers.get(ticker, {}).get('signals', [])\n",
    "\n",
    "class PolygonDataFetcher:\n",
    "    \"\"\"Handles data retrieval from the Polygon.io API with robust error handling and rate limiting.\"\"\"\n",
    "    \n",
    "    BASE_URL = \"https://api.polygon.io/v2\"\n",
    "\n",
    "    def __init__(self, api_key: str):\n",
    "        self.api_key = api_key\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\"Authorization\": f\"Bearer {api_key}\"})\n",
    "        self.validation_metrics = []\n",
    "\n",
    "    def _make_request(self, endpoint: str, params: Optional[Dict] = None, retries: int = 3) -> Dict:\n",
    "        \"\"\"Make an API request with retry logic and rate limiting.\"\"\"\n",
    "        url = f\"{self.BASE_URL}/{endpoint}\"\n",
    "        \n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                response = self.session.get(url, params=params, timeout=30)\n",
    "                \n",
    "                if response.status_code == 429:  # Rate limit exceeded\n",
    "                    wait_time = min(60 * (attempt + 1), 300)  # Max 5 minutes\n",
    "                    logger.warning(f\"Rate limit exceeded. Waiting {wait_time} seconds...\")\n",
    "                    time.sleep(wait_time)\n",
    "                    continue\n",
    "                    \n",
    "                response.raise_for_status()\n",
    "                data = response.json()\n",
    "                \n",
    "                logger.debug(f\"API response keys: {data.keys()}\")\n",
    "                if 'results' in data and len(data['results']) > 0:\n",
    "                    logger.debug(f\"Sample result: {data['results'][0]}\")\n",
    "                \n",
    "                return data\n",
    "                \n",
    "            except Exception as e:\n",
    "                if attempt == retries - 1:\n",
    "                    logger.error(f\"Request failed after {retries} attempts: {e}\")\n",
    "                    raise\n",
    "                logger.warning(f\"Request failed (Attempt {attempt + 1}/{retries}): {e}. Retrying in 30 seconds...\")\n",
    "                time.sleep(30)\n",
    "                \n",
    "        raise Exception(f\"Failed after {retries} attempts\")\n",
    "\n",
    "    def fetch_stock_data(self, symbol: str, start_date: str, end_date: str, \n",
    "                        timespan: str = \"minute\", multiplier: int = 1) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Fetch aggregate stock data for a specific ticker and date range.\n",
    "        \n",
    "        Args:\n",
    "            symbol: Stock ticker symbol\n",
    "            start_date: Start date in YYYY-MM-DD format\n",
    "            end_date: End date in YYYY-MM-DD format\n",
    "            timespan: Timespan of the aggregates ('minute', 'hour', 'day')\n",
    "            multiplier: The size of the timespan multiplier\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame containing aggregated stock data\n",
    "        \"\"\"\n",
    "        endpoint = f\"aggs/ticker/{symbol}/range/{multiplier}/{timespan}/{start_date}/{end_date}\"\n",
    "        params = {\"adjusted\": \"true\", \"sort\": \"asc\", \"limit\": 50000}\n",
    "        all_results = []\n",
    "\n",
    "        with tqdm(desc=f\"Fetching {symbol}\", unit='rows') as pbar:\n",
    "            while True:\n",
    "                data = self._make_request(endpoint, params)\n",
    "                \n",
    "                if 'results' in data:\n",
    "                    if len(data['results']) > 0 and 'c' not in data['results'][0]:\n",
    "                        logger.error(f\"Data for {symbol} does not contain 'c' (close) field.\")\n",
    "                        return pd.DataFrame()\n",
    "\n",
    "                    all_results.extend(data['results'])\n",
    "                    pbar.update(len(data['results']))\n",
    "                    \n",
    "                if len(data.get('results', [])) < 50000:\n",
    "                    break\n",
    "                    \n",
    "                # Update start_date for next request\n",
    "                last_timestamp = data['results'][-1]['t']\n",
    "                start_date = datetime.fromtimestamp(last_timestamp / 1000).strftime('%Y-%m-%d')\n",
    "                params['from'] = start_date\n",
    "\n",
    "        # Create DataFrame and process data\n",
    "        df = pd.DataFrame(all_results)\n",
    "        \n",
    "        if 't' in df.columns:\n",
    "            # Remove duplicate timestamps\n",
    "            duplicates_before = len(df) - df.drop_duplicates(subset=['t'], keep='last').shape[0]\n",
    "            if duplicates_before > 0:\n",
    "                logger.info(f\"Removed {duplicates_before} duplicate timestamps for {symbol}.\")\n",
    "            df = df.drop_duplicates(subset=['t'], keep='last')\n",
    "        else:\n",
    "            logger.error(\"'t' column not found in fetched data.\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        # Rename columns for clarity\n",
    "        df = df.rename(columns={\n",
    "            't': 'timestamp',\n",
    "            'o': 'open',\n",
    "            'h': 'high',\n",
    "            'l': 'low',\n",
    "            'c': 'close',\n",
    "            'v': 'volume',\n",
    "            'n': 'transactions'\n",
    "        })\n",
    "\n",
    "        # Convert timestamp to datetime and set as index\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
    "        df.set_index('timestamp', inplace=True)\n",
    "\n",
    "        return df\n",
    "\n",
    "class ImprovedHFTDataset(Dataset):\n",
    "    \"\"\"Enhanced Dataset for High-Frequency Trading data preparation.\"\"\"\n",
    "\n",
    "    def __init__(self, data, sequence_length, predict_pct_change=True, price_scaler=None, feature_scaler=None):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.predict_pct_change = predict_pct_change\n",
    "        \n",
    "        # Ensure data is a DataFrame\n",
    "        if not isinstance(data, pd.DataFrame):\n",
    "            raise ValueError(\"Input data must be a pandas DataFrame\")\n",
    "            \n",
    "        # Calculate additional technical features\n",
    "        self._add_technical_features(data)\n",
    "        \n",
    "        # Separate price data from other features\n",
    "        price_cols = ['close']\n",
    "        \n",
    "        # Define expanded feature columns\n",
    "        feature_cols = [\n",
    "            'volume', 'SMA_20', 'EMA_20', 'RSI', 'MA5',\n",
    "            'Bollinger_High', 'Bollinger_Low', 'MACD', 'MACD_Signal',\n",
    "            'volatility_atr', 'trend_adx', 'momentum_roc', 'momentum_kama',\n",
    "            'volume_cmf', 'volume_em', 'volume_sma_ratio', 'price_distance_from_ma',\n",
    "            'volatility_bbw', 'momentum_stoch', 'trend_cci', 'trend_ichimoku_a'\n",
    "        ]\n",
    "        \n",
    "        # Ensure all feature columns exist\n",
    "        available_features = [col for col in feature_cols if col in data.columns]\n",
    "        \n",
    "        # Initialize scalers if not provided\n",
    "        self.price_scaler = price_scaler if price_scaler is not None else StandardScaler()\n",
    "        self.feature_scaler = feature_scaler if feature_scaler is not None else StandardScaler()\n",
    "        \n",
    "        # Prepare features and targets\n",
    "        price_data = data[price_cols].values\n",
    "        feature_data = data[available_features].values\n",
    "        \n",
    "        # Scale price data\n",
    "        if not hasattr(self.price_scaler, 'mean_'):\n",
    "            self.normalized_prices = self.price_scaler.fit_transform(price_data)\n",
    "        else:\n",
    "            self.normalized_prices = self.price_scaler.transform(price_data)\n",
    "            \n",
    "        # Scale feature data\n",
    "        if not hasattr(self.feature_scaler, 'mean_'):\n",
    "            self.normalized_features = self.feature_scaler.fit_transform(feature_data)\n",
    "        else:\n",
    "            self.normalized_features = self.feature_scaler.transform(feature_data)\n",
    "            \n",
    "        # Combine normalized data\n",
    "        self.features = np.hstack((self.normalized_prices, self.normalized_features))\n",
    "        \n",
    "        # Create target values\n",
    "        if self.predict_pct_change:\n",
    "            # Predict percentage change in price\n",
    "            pct_change = data['close'].pct_change(1).shift(-1).values\n",
    "            self.labels = pct_change[:-1]  # Remove last row (NaN value)\n",
    "            self.features = self.features[:-1]  # Align with labels\n",
    "        else:\n",
    "            # Predict actual next price\n",
    "            self.labels = data['close'].shift(-1).values[:-1]\n",
    "            self.features = self.features[:-1]\n",
    "\n",
    "    def _add_technical_features(self, data):\n",
    "        \"\"\"Add advanced technical indicators to the dataset.\"\"\"\n",
    "        # Check if data has required columns\n",
    "        if not all(col in data.columns for col in ['open', 'high', 'low', 'close', 'volume']):\n",
    "            raise ValueError(\"Data must contain OHLCV columns\")\n",
    "            \n",
    "        # Make a copy to avoid modifying the original\n",
    "        df = data.copy()\n",
    "        \n",
    "        # Volatility indicators\n",
    "        df['volatility_atr'] = ta.volatility.average_true_range(df['high'], df['low'], df['close'])\n",
    "        df['volatility_bbw'] = ta.volatility.bollinger_pband(df['close'])\n",
    "        \n",
    "        # Trend indicators\n",
    "        df['trend_adx'] = ta.trend.adx(df['high'], df['low'], df['close'])\n",
    "        df['trend_cci'] = ta.trend.cci(df['high'], df['low'], df['close'])\n",
    "        df['trend_ichimoku_a'] = ta.trend.ichimoku_a(df['high'], df['low'])\n",
    "        \n",
    "        # Momentum indicators\n",
    "        df['momentum_roc'] = ta.momentum.roc(df['close'])\n",
    "        df['momentum_kama'] = ta.momentum.kama(df['close'])\n",
    "        df['momentum_stoch'] = ta.momentum.stoch(df['high'], df['low'], df['close'])\n",
    "        \n",
    "        # Volume indicators\n",
    "        df['volume_cmf'] = ta.volume.chaikin_money_flow(df['high'], df['low'], df['close'], df['volume'])\n",
    "        df['volume_em'] = ta.volume.ease_of_movement(df['high'], df['low'], df['close'], df['volume'])\n",
    "        df['volume_sma_ratio'] = df['volume'] / df['volume'].rolling(20).mean()\n",
    "        \n",
    "        # Price relative to moving average\n",
    "        df['price_distance_from_ma'] = (df['close'] - df['SMA_20']) / df['SMA_20']\n",
    "        \n",
    "        # Fill NaN values\n",
    "        df.fillna(method='bfill', inplace=True)\n",
    "        df.fillna(0, inplace=True)\n",
    "        \n",
    "        # Update the original data with new features\n",
    "        for col in df.columns:\n",
    "            if col not in data.columns:\n",
    "                data[col] = df[col]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the total number of sequences in the dataset.\"\"\"\n",
    "        return max(0, len(self.features) - self.sequence_length)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get a single sequence and its corresponding label.\"\"\"\n",
    "        x = self.features[idx:idx + self.sequence_length]\n",
    "        y = self.labels[idx + self.sequence_length - 1]\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32).view(-1)\n",
    "\n",
    "class DirectionalMSELoss(nn.Module):\n",
    "    def __init__(self, direction_weight=1.5, magnitude_weight=1.0):\n",
    "        super(DirectionalMSELoss, self).__init__()\n",
    "        self.direction_weight = direction_weight\n",
    "        self.magnitude_weight = magnitude_weight\n",
    "        self.mse = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Standard MSE for magnitude accuracy\n",
    "        magnitude_loss = self.mse(y_pred, y_true)\n",
    "        \n",
    "        # Get batch size for reshaping if needed\n",
    "        if len(y_pred.shape) == 1:\n",
    "            y_pred = y_pred.unsqueeze(1)\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = y_true.unsqueeze(1)\n",
    "            \n",
    "        # Calculate sign agreement loss for direction accuracy\n",
    "        pred_sign = torch.sign(y_pred)\n",
    "        true_sign = torch.sign(y_true)\n",
    "        \n",
    "        # Binary direction loss (1 if direction is wrong, 0 if correct)\n",
    "        direction_loss = torch.mean((pred_sign != true_sign).float())\n",
    "        \n",
    "        # Combined loss with weighting\n",
    "        total_loss = (self.magnitude_weight * magnitude_loss) + (self.direction_weight * direction_loss)\n",
    "        \n",
    "        return total_loss\n",
    "\n",
    "\n",
    "class EnhancedLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size=1, dropout=0.2):\n",
    "        super(EnhancedLSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Bidirectional LSTM for better pattern recognition\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = nn.Linear(hidden_size*2, 1)\n",
    "        \n",
    "        # Output layers\n",
    "        self.fc1 = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers*2, batch_size, self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers*2, batch_size, self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        lstm_out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Apply attention\n",
    "        attention_weights = F.softmax(self.attention(lstm_out), dim=1)\n",
    "        context_vector = attention_weights * lstm_out\n",
    "        context_vector = torch.sum(context_vector, dim=1)\n",
    "        \n",
    "        # Process through fully connected layers\n",
    "        out = self.fc1(context_vector)\n",
    "        out = self.dropout(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "def train_enhanced_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device,\n",
    "                        patience=10, min_delta=0.001, learning_rate_scheduler=None):\n",
    "    \"\"\"\n",
    "    Train the LSTM model with comprehensive tracking and early stopping.\n",
    "    \n",
    "    Args:\n",
    "        model: The neural network model to train\n",
    "        train_loader: DataLoader for training data\n",
    "        val_loader: DataLoader for validation data\n",
    "        criterion: Loss function\n",
    "        optimizer: Optimization algorithm\n",
    "        num_epochs: Maximum number of training epochs\n",
    "        device: Computation device (CPU/GPU)\n",
    "        patience: Number of epochs to wait for improvement before stopping\n",
    "        min_delta: Minimum change in validation loss to be considered as improvement\n",
    "        learning_rate_scheduler: Optional scheduler for adjusting learning rate\n",
    "        \n",
    "    Returns:\n",
    "        Trained model and training history\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    \n",
    "    # Initialize training history tracking\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'learning_rates': [],\n",
    "        'best_epoch': 0\n",
    "    }\n",
    "    \n",
    "    # Early stopping variables\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        \n",
    "        # Progress bar for training\n",
    "        train_progress = tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} [Train]\")\n",
    "        \n",
    "        for batch_x, batch_y in train_progress:\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            # Record loss\n",
    "            train_losses.append(loss.item())\n",
    "            train_progress.set_postfix({\"loss\": f\"{loss.item():.6f}\"})\n",
    "\n",
    "        # Calculate average training loss\n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        \n",
    "        # Record current learning rate\n",
    "        if learning_rate_scheduler:\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            history['learning_rates'].append(current_lr)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_progress = tqdm(val_loader, desc=f\"Epoch {epoch}/{num_epochs} [Valid]\")\n",
    "            \n",
    "            for batch_x, batch_y in val_progress:\n",
    "                batch_x = batch_x.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                \n",
    "                # Record loss\n",
    "                val_losses.append(loss.item())\n",
    "                val_progress.set_postfix({\"loss\": f\"{loss.item():.6f}\"})\n",
    "\n",
    "        # Calculate average validation loss\n",
    "        avg_val_loss = np.mean(val_losses)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "\n",
    "        # Print epoch summary\n",
    "        print(f\"Epoch {epoch}/{num_epochs}: \"\n",
    "              f\"Train Loss = {avg_train_loss:.6f}, \"\n",
    "              f\"Val Loss = {avg_val_loss:.6f}\")\n",
    "\n",
    "        # Check for improvement\n",
    "        if avg_val_loss < best_val_loss - min_delta:\n",
    "            # Save the model state dict (not the entire model for memory efficiency)\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            history['best_epoch'] = epoch\n",
    "            print(f\"* Validation loss improved to {best_val_loss:.6f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"* Validation loss did not improve. Patience: {patience_counter}/{patience}\")\n",
    "            \n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping triggered at epoch {epoch}\")\n",
    "                break\n",
    "        \n",
    "        # Step the learning rate scheduler if provided\n",
    "        if learning_rate_scheduler:\n",
    "            if isinstance(learning_rate_scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                learning_rate_scheduler.step(avg_val_loss)\n",
    "            else:\n",
    "                learning_rate_scheduler.step()\n",
    "\n",
    "    # Load the best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(f\"Loaded best model from epoch {history['best_epoch']}\")\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "def evaluate_model(model: nn.Module, \n",
    "                  test_loader: DataLoader, \n",
    "                  device: torch.device) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Evaluate the trained model on test data.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained LSTM model\n",
    "        test_loader: DataLoader for test data\n",
    "        device: Computation device\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of true and predicted values\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in test_loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            outputs = model(batch_x).cpu().numpy()\n",
    "            y_true.extend(batch_y.numpy())\n",
    "            y_pred.extend(outputs.flatten())\n",
    "\n",
    "    return np.array(y_true), np.array(y_pred)\n",
    "\n",
    "def backtest_trades(signals: List[Dict], df: pd.DataFrame) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Execute trades based on generated signals and record trade details.\n",
    "    \n",
    "    Args:\n",
    "        signals: List of trading signals\n",
    "        df: DataFrame with market data\n",
    "        \n",
    "    Returns:\n",
    "        List of executed trades with details\n",
    "    \"\"\"\n",
    "    trades = []\n",
    "    entry_signal = None\n",
    "    \n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "\n",
    "    for signal in signals:\n",
    "        signal_time = pd.to_datetime(signal['timestamp'])\n",
    "        \n",
    "        if signal['action'] in ['enter_long', 'enter_short']:\n",
    "            if entry_signal is None:\n",
    "                entry_signal = {\n",
    "                    'timestamp': signal_time,\n",
    "                    'price': signal['price'],\n",
    "                    'action': signal['action']\n",
    "                }\n",
    "        elif signal['action'] in ['exit_long', 'exit_short'] and entry_signal is not None:\n",
    "            exit_reason = 'stop_loss' if signal['action'].startswith('exit_') else 'take_profit'\n",
    "            duration = (signal_time - entry_signal['timestamp']).total_seconds() / 60\n",
    "            \n",
    "            trade = {\n",
    "                'entry_time': entry_signal['timestamp'],\n",
    "                'entry_price': entry_signal['price'],\n",
    "                'exit_time': signal_time,\n",
    "                'exit_price': signal['price'],\n",
    "                'position': entry_signal['action'],\n",
    "                'exit_reason': exit_reason,\n",
    "                'duration': duration\n",
    "            }\n",
    "            trades.append(trade)\n",
    "            entry_signal = None\n",
    "\n",
    "    # Close any open positions at the end\n",
    "    if entry_signal is not None:\n",
    "        last_time = df.index[-1]\n",
    "        last_price = df['close'].iloc[-1]\n",
    "        duration = (last_time - entry_signal['timestamp']).total_seconds() / 60\n",
    "        \n",
    "        trade = {\n",
    "            'entry_time': entry_signal['timestamp'],\n",
    "            'entry_price': entry_signal['price'],\n",
    "            'exit_time': last_time,\n",
    "            'exit_price': last_price,\n",
    "            'position': entry_signal['action'],\n",
    "            'exit_reason': 'end_of_data',\n",
    "            'duration': duration\n",
    "        }\n",
    "        trades.append(trade)\n",
    "\n",
    "    return trades\n",
    "\n",
    "def plot_candlestick_analysis(df: pd.DataFrame, signals: Optional[List[Dict]] = None, ticker: str = '') -> None:\n",
    "    \"\"\"\n",
    "    Plot candlestick chart with trade signals.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with OHLCV data\n",
    "        signals: Optional list of trading signals\n",
    "        ticker: Stock ticker symbol\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df_plot = df.copy()\n",
    "        df_plot = df_plot[['open', 'high', 'low', 'close', 'volume']]\n",
    "        df_plot.index.name = 'Date'\n",
    "\n",
    "        # Create subplots\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(16, 12), sharex=True)\n",
    "        fig.subplots_adjust(hspace=0)\n",
    "\n",
    "        # Plot candlesticks and volume\n",
    "        mc = mpf.make_marketcolors(up='g', down='r', inherit=True)\n",
    "        s = mpf.make_mpf_style(marketcolors=mc)\n",
    "        mpf.plot(df_plot, type='candle', style=s, ax=ax1, volume=ax2, show_nontrading=False)\n",
    "\n",
    "        # Add signals if provided\n",
    "        if signals:\n",
    "            for signal in signals:\n",
    "                if 'enter_long' in signal['action']:\n",
    "                    ax1.scatter(signal['timestamp'], signal['price'], \n",
    "                              marker='^', color='g', s=100, label='Buy Signal')\n",
    "                elif 'enter_short' in signal['action']:\n",
    "                    ax1.scatter(signal['timestamp'], signal['price'], \n",
    "                              marker='v', color='r', s=100, label='Sell Signal')\n",
    "                elif 'exit_long' in signal['action'] or 'exit_short' in signal['action']:\n",
    "                    ax1.scatter(signal['timestamp'], signal['price'], \n",
    "                              marker='o', color='k', s=100, label='Exit Signal')\n",
    "\n",
    "            handles, labels = ax1.get_legend_handles_labels()\n",
    "            by_label = dict(zip(labels, handles))\n",
    "            ax1.legend(by_label.values(), by_label.keys())\n",
    "\n",
    "        plt.title(f\"Candlestick Chart with Trade Signals for {ticker}\")\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Price')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in plot_candlestick_analysis: {e}\")\n",
    "        raise\n",
    "\n",
    "def plot_trading_metrics(metrics: Dict[str, float], ticker: str) -> None:\n",
    "    \"\"\"\n",
    "    Create comprehensive visualization of trading metrics.\n",
    "    \n",
    "    Args:\n",
    "        metrics: Dictionary of trading metrics\n",
    "        ticker: Stock ticker symbol\n",
    "    \"\"\"\n",
    "    plt.style.use('default')\n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    gs = gridspec.GridSpec(2, 2)\n",
    "\n",
    "    # Rates Comparison\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    rates = ['win_rate', 'loss_rate', 'net_win_rate', 'net_loss_rate']\n",
    "    values = [metrics.get(rate, 0) for rate in rates]\n",
    "    colors = ['green', 'red', 'lightgreen', 'lightcoral']\n",
    "    ax1.bar(rates, values, color=colors)\n",
    "    ax1.set_title('Trading Rates Comparison')\n",
    "    ax1.set_ylabel('Percentage (%)')\n",
    "    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "    # Profit Metrics\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    profit_metrics = ['average_profit_per_trade', 'daily_profit', 'profit_per_minute']\n",
    "    values = [metrics.get(metric, 0) for metric in profit_metrics]\n",
    "    ax2.bar(profit_metrics, values, color='blue')\n",
    "    ax2.set_title('Profit Metrics')\n",
    "    ax2.set_ylabel('Amount ($)')\n",
    "    plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "    # Risk Metrics\n",
    "    ax3 = fig.add_subplot(gs[1, 0])\n",
    "    risk_metrics = ['maximum_drawdown', 'sharpe_ratio', 'profit_factor']\n",
    "    values = [metrics.get(metric, 0) for metric in risk_metrics]\n",
    "    ax3.bar(risk_metrics, values, color='purple')\n",
    "    ax3.set_title('Risk Metrics')\n",
    "    plt.setp(ax3.xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "    # Trade Analysis\n",
    "    ax4 = fig.add_subplot(gs[1, 1])\n",
    "    trade_metrics = ['total_trades', 'average_trade_duration']\n",
    "    values = [metrics.get(metric, 0) for metric in trade_metrics]\n",
    "    ax4.bar(trade_metrics, values, color='orange')\n",
    "    ax4.set_title('Trade Analysis')\n",
    "    plt.setp(ax4.xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "    plt.suptitle(f'Trading Performance Metrics for {ticker}', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_learning_curves(training_history: TrainingHistory, ticker: str) -> None:\n",
    "    \"\"\"\n",
    "    Plot learning curves showing training and validation loss over epochs.\n",
    "    \n",
    "    Args:\n",
    "        training_history: Object containing training and validation loss history\n",
    "        ticker: Stock ticker symbol for plot title\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    epochs = range(1, len(training_history.loss_history) + 1)\n",
    "    \n",
    "    plt.plot(epochs, training_history.loss_history, 'b-', label='Training Loss')\n",
    "    plt.plot(epochs, training_history.validation_loss_history, 'r-', label='Validation Loss')\n",
    "    \n",
    "    plt.title(f'Learning Curves for {ticker}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.yscale('log')  # Use log scale for better visualization of loss changes\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_actual_vs_predicted(timestamps, y_true, y_pred, ticker: str) -> None:\n",
    "    \"\"\"\n",
    "    Plot actual vs predicted prices.\n",
    "    \n",
    "    Args:\n",
    "        timestamps: DatetimeIndex or array of timestamps\n",
    "        y_true: Array of actual prices\n",
    "        y_pred: Array of predicted prices\n",
    "        ticker: Stock ticker symbol for plot title\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.plot(timestamps, y_true, 'b-', label='Actual Price')\n",
    "    plt.plot(timestamps, y_pred, 'r-', label='Predicted Price')\n",
    "    \n",
    "    # Calculate RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    \n",
    "    plt.title(f'Actual vs Predicted Prices for {ticker} (RMSE: {rmse:.4f})')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Price')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_model_prediction_quality(y_true, y_pred, timestamps, ticker):\n",
    "    \"\"\"\n",
    "    Create a comprehensive visualization of model prediction quality.\n",
    "    \n",
    "    Args:\n",
    "        y_true: Array of actual values\n",
    "        y_pred: Array of predicted values\n",
    "        timestamps: Array of corresponding timestamps\n",
    "        ticker: Stock ticker symbol\n",
    "    \"\"\"\n",
    "    # Create figure with multiple subplots\n",
    "    fig = plt.figure(figsize=(20, 16))\n",
    "    gs = gridspec.GridSpec(3, 2, height_ratios=[2, 1, 1])\n",
    "    \n",
    "    # 1. Time Series Plot\n",
    "    ax1 = fig.add_subplot(gs[0, :])\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    correlation = np.corrcoef(y_true, y_pred)[0, 1]\n",
    "    \n",
    "    ax1.plot(timestamps, y_true, 'b-', label='Actual Price', linewidth=1.5)\n",
    "    ax1.plot(timestamps, y_pred, 'r-', label='Predicted Price', linewidth=1.5)\n",
    "    \n",
    "    ax1.set_title(f'Actual vs Predicted Prices for {ticker}\\nRMSE: {rmse:.4f}, MAE: {mae:.4f}, Correlation: {correlation:.4f}')\n",
    "    ax1.set_xlabel('Time')\n",
    "    ax1.set_ylabel('Price')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # 2. Prediction Error Over Time\n",
    "    ax2 = fig.add_subplot(gs[1, 0])\n",
    "    errors = y_pred - y_true\n",
    "    ax2.plot(timestamps, errors, 'g-')\n",
    "    ax2.axhline(y=0, color='r', linestyle='-')\n",
    "    ax2.set_title('Prediction Error Over Time')\n",
    "    ax2.set_xlabel('Time')\n",
    "    ax2.set_ylabel('Error')\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    # 3. Error Distribution\n",
    "    ax3 = fig.add_subplot(gs[1, 1])\n",
    "    ax3.hist(errors, bins=50, alpha=0.75)\n",
    "    ax3.axvline(x=0, color='r', linestyle='-')\n",
    "    ax3.set_title('Error Distribution')\n",
    "    ax3.set_xlabel('Error')\n",
    "    ax3.set_ylabel('Frequency')\n",
    "    \n",
    "    # 4. Actual vs Predicted Scatter Plot\n",
    "    ax4 = fig.add_subplot(gs[2, 0])\n",
    "    ax4.scatter(y_true, y_pred, alpha=0.5)\n",
    "    \n",
    "    # Add perfect prediction line\n",
    "    min_val = min(np.min(y_true), np.min(y_pred))\n",
    "    max_val = max(np.max(y_true), np.max(y_pred))\n",
    "    ax4.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
    "    \n",
    "    ax4.set_title('Actual vs Predicted (Scatter)')\n",
    "    ax4.set_xlabel('Actual Price')\n",
    "    ax4.set_ylabel('Predicted Price')\n",
    "    ax4.grid(True)\n",
    "    \n",
    "    # 5. Direction Accuracy\n",
    "    ax5 = fig.add_subplot(gs[2, 1])\n",
    "    \n",
    "    # Calculate price changes\n",
    "    true_changes = np.diff(y_true)\n",
    "    pred_changes = np.diff(y_pred)\n",
    "    \n",
    "    # Determine direction accuracy\n",
    "    true_direction = np.sign(true_changes)\n",
    "    pred_direction = np.sign(pred_changes)\n",
    "    correct_direction = (true_direction == pred_direction)\n",
    "    \n",
    "    direction_accuracy = np.mean(correct_direction) * 100\n",
    "    \n",
    "    # Create color-coded time series based on direction accuracy\n",
    "    reduced_timestamps = timestamps[1:]  # Adjust for diff operation\n",
    "    \n",
    "    # Plot lines for correct and incorrect predictions\n",
    "    accuracy_colors = ['green' if correct else 'red' for correct in correct_direction]\n",
    "    \n",
    "    for i in range(len(reduced_timestamps)-1):\n",
    "        ax5.plot(reduced_timestamps[i:i+2], y_true[i:i+2], color=accuracy_colors[i], linewidth=1.5)\n",
    "    \n",
    "    ax5.set_title(f'Direction Prediction Accuracy: {direction_accuracy:.2f}%')\n",
    "    ax5.set_xlabel('Time')\n",
    "    ax5.set_ylabel('Price')\n",
    "    \n",
    "    # Create custom legend\n",
    "    from matplotlib.lines import Line2D\n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], color='green', lw=2, label='Correct Direction'),\n",
    "        Line2D([0], [0], color='red', lw=2, label='Incorrect Direction')\n",
    "    ]\n",
    "    ax5.legend(handles=legend_elements)\n",
    "    ax5.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function for the high-frequency trading system.\"\"\"\n",
    "    \n",
    "    # Load environment variables from .env file\n",
    "    load_dotenv()\n",
    "    \n",
    "    # Configuration\n",
    "    TICKERS = ['MSFT', 'GOOGL', 'TSLA', 'NVDA', 'TQQQ', 'SQQQ', 'QLD', 'PSQ']\n",
    "    END_DATE = '2025-03-21'\n",
    "    \n",
    "    # Retrieve API key from environment variables\n",
    "    API_KEY = os.getenv(\"POLYGON_API_KEY\")\n",
    "    if not API_KEY:\n",
    "        logger.error(\"No API key found. Please check your .env file.\")\n",
    "        print(\"ERROR: Polygon API key not found. Please check your .env file.\")\n",
    "        return\n",
    "\n",
    "    # Enhanced model hyperparameters\n",
    "    MODEL_PARAMS = {\n",
    "        'sequence_length': 60,\n",
    "        'hidden_size': 128,\n",
    "        'num_layers': 2,\n",
    "        'batch_size': 64,\n",
    "        'num_epochs': 100,\n",
    "        'learning_rate': 0.0005,\n",
    "        'dropout': 0.3,\n",
    "        'weight_decay': 1e-5\n",
    "    }\n",
    "\n",
    "    # Initialize components\n",
    "    fetcher = PolygonDataFetcher(API_KEY)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "\n",
    "    for ticker in TICKERS:\n",
    "        try:\n",
    "            logger.info(f\"Processing {ticker}\")\n",
    "            print(f\"\\nProcessing {ticker}...\")\n",
    "\n",
    "            # Initialize stock-specific components\n",
    "            signal_generator = AdaptiveSignalGenerator(ticker)\n",
    "            monitor = MultiTickerMonitor(signal_generator)\n",
    "\n",
    "            # Define date range\n",
    "            end_date = datetime.strptime(END_DATE, '%Y-%m-%d')\n",
    "            start_date = end_date - timedelta(days=70)\n",
    "            start_date_str = start_date.strftime('%Y-%m-%d')\n",
    "            end_date_str = end_date.strftime('%Y-%m-%d')\n",
    "\n",
    "            # Fetch and prepare market data\n",
    "            logger.info(f\"Fetching market data from {start_date_str} to {end_date_str}\")\n",
    "            stock_df = fetcher.fetch_stock_data(ticker, start_date_str, end_date_str)\n",
    "\n",
    "            if stock_df.empty:\n",
    "                logger.warning(f\"No data fetched for {ticker}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Calculate technical indicators\n",
    "            stock_df['MA5'] = ta.trend.sma_indicator(stock_df['close'], window=5)\n",
    "            stock_df['SMA_20'] = ta.trend.sma_indicator(stock_df['close'], window=20)\n",
    "            stock_df['EMA_20'] = ta.trend.ema_indicator(stock_df['close'], window=20)\n",
    "            stock_df['RSI'] = ta.momentum.rsi(stock_df['close'], window=14)\n",
    "            \n",
    "            macd_indicator = MACD(close=stock_df['close'])\n",
    "            stock_df['MACD'] = macd_indicator.macd()\n",
    "            stock_df['MACD_Signal'] = macd_indicator.macd_signal()\n",
    "            \n",
    "            bb_indicator = BollingerBands(close=stock_df['close'])\n",
    "            stock_df['Bollinger_High'] = bb_indicator.bollinger_hband()\n",
    "            stock_df['Bollinger_Low'] = bb_indicator.bollinger_lband()\n",
    "\n",
    "            # Remove NaN values\n",
    "            stock_df.dropna(inplace=True)\n",
    "\n",
    "            # Prepare datasets using improved dataset class\n",
    "            dataset = ImprovedHFTDataset(\n",
    "                data=stock_df,\n",
    "                sequence_length=MODEL_PARAMS['sequence_length'],\n",
    "                predict_pct_change=True  # Predict percentage changes instead of absolute prices\n",
    "            )\n",
    "\n",
    "            # Split data\n",
    "            train_size = int(0.7 * len(dataset))\n",
    "            val_size = int(0.15 * len(dataset))\n",
    "            test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "            train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "                dataset, [train_size, val_size, test_size]\n",
    "            )\n",
    "\n",
    "            # Create data loaders\n",
    "            train_loader = DataLoader(\n",
    "                train_dataset,\n",
    "                batch_size=MODEL_PARAMS['batch_size'],\n",
    "                shuffle=True\n",
    "            )\n",
    "            val_loader = DataLoader(\n",
    "                val_dataset,\n",
    "                batch_size=MODEL_PARAMS['batch_size']\n",
    "            )\n",
    "            test_loader = DataLoader(\n",
    "                test_dataset,\n",
    "                batch_size=MODEL_PARAMS['batch_size']\n",
    "            )\n",
    "\n",
    "            # Initialize enhanced model\n",
    "            input_size = dataset.features.shape[1]  # Get input size from dataset\n",
    "            model = EnhancedLSTMModel(\n",
    "                input_size=input_size,\n",
    "                hidden_size=MODEL_PARAMS['hidden_size'],\n",
    "                num_layers=MODEL_PARAMS['num_layers'],\n",
    "                dropout=MODEL_PARAMS['dropout']\n",
    "            )\n",
    "\n",
    "            # Initialize training components\n",
    "            criterion = DirectionalMSELoss(direction_weight=2.0, magnitude_weight=1.0)\n",
    "            optimizer = torch.optim.Adam(\n",
    "                model.parameters(), \n",
    "                lr=MODEL_PARAMS['learning_rate'],\n",
    "                weight_decay=MODEL_PARAMS['weight_decay']\n",
    "            )\n",
    "            \n",
    "            # Learning rate scheduler\n",
    "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer, \n",
    "                mode='min', \n",
    "                factor=0.5, \n",
    "                patience=5, \n",
    "                min_lr=1e-6, \n",
    "                verbose=True\n",
    "            )\n",
    "\n",
    "            # Train enhanced model\n",
    "            logger.info(\"Starting model training...\")\n",
    "            model, training_history = train_enhanced_model(\n",
    "                model=model,\n",
    "                train_loader=train_loader,\n",
    "                val_loader=val_loader,\n",
    "                criterion=criterion,\n",
    "                optimizer=optimizer,\n",
    "                num_epochs=MODEL_PARAMS['num_epochs'],\n",
    "                device=device,\n",
    "                learning_rate_scheduler=scheduler\n",
    "            )\n",
    "\n",
    "            # Plot learning curves\n",
    "            logger.info(\"Plotting learning curves...\")\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plt.plot(range(1, len(training_history['train_loss'])+1), training_history['train_loss'], 'b-', label='Training Loss')\n",
    "            plt.plot(range(1, len(training_history['val_loss'])+1), training_history['val_loss'], 'r-', label='Validation Loss')\n",
    "            plt.axvline(x=training_history['best_epoch'], color='g', linestyle='--', label=f'Best Epoch ({training_history[\"best_epoch\"]})')\n",
    "            plt.title(f'Learning Curves for {ticker}')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.yscale('log')  # Use log scale for better visualization\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            # Evaluate model\n",
    "            logger.info(\"Evaluating model...\")\n",
    "            model.eval()\n",
    "            y_true = []\n",
    "            y_pred = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch_x, batch_y in test_loader:\n",
    "                    batch_x = batch_x.to(device)\n",
    "                    outputs = model(batch_x).cpu().numpy()\n",
    "                    y_pred.extend(outputs.flatten())\n",
    "                    y_true.extend(batch_y.numpy().flatten())\n",
    "            \n",
    "            # Convert predictions from percentage changes back to prices if necessary\n",
    "            test_indices = np.arange(MODEL_PARAMS['sequence_length'], len(stock_df))\n",
    "            test_timestamps = stock_df.index[test_indices[-len(y_pred):]]\n",
    "            \n",
    "            # If predicting percentage changes, convert back to prices for plotting\n",
    "            if dataset.predict_pct_change:\n",
    "                # Get the base prices from the test data\n",
    "                base_prices = stock_df['close'].iloc[test_indices[-len(y_pred):] - 1].values\n",
    "                \n",
    "                # Convert percentage changes to actual prices\n",
    "                y_pred_prices = base_prices * (1 + np.array(y_pred))\n",
    "                y_true_prices = base_prices * (1 + np.array(y_true))\n",
    "            else:\n",
    "                y_pred_prices = np.array(y_pred)\n",
    "                y_true_prices = np.array(y_true)\n",
    "            \n",
    "            # Ensure arrays are properly shaped and aligned\n",
    "            y_true_prices = np.array(y_true_prices).flatten()\n",
    "            y_pred_prices = np.array(y_pred_prices).flatten()\n",
    "            \n",
    "            # Ensure same length (take the smaller length)\n",
    "            min_length = min(len(y_true_prices), len(y_pred_prices), len(test_timestamps))\n",
    "            y_true_prices = y_true_prices[:min_length]\n",
    "            y_pred_prices = y_pred_prices[:min_length]\n",
    "            test_timestamps_aligned = test_timestamps[:min_length]\n",
    "            \n",
    "            # Calculate RMSE with aligned arrays\n",
    "            rmse = np.sqrt(mean_squared_error(y_true_prices, y_pred_prices))\n",
    "            \n",
    "            # Continue with plotting using the aligned arrays\n",
    "            plt.figure(figsize=(14, 7))\n",
    "            plt.plot(test_timestamps_aligned, y_true_prices, 'b-', label='Actual Price')\n",
    "            plt.plot(test_timestamps_aligned, y_pred_prices, 'r-', label='Predicted Price')\n",
    "            plt.title(f'Actual vs Predicted Prices for {ticker} (RMSE: {rmse:.4f})')\n",
    "            plt.xlabel('Time')\n",
    "            plt.ylabel('Price')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Pass aligned arrays to the prediction quality visualization\n",
    "            logger.info(\"Generating comprehensive prediction quality visualization...\")\n",
    "            plot_model_prediction_quality(y_true_prices, y_pred_prices, test_timestamps_aligned, ticker)\n",
    "\n",
    "            # Generate trading signals using the improved predictions\n",
    "            all_signals = []\n",
    "            \n",
    "            print(f\"Generating signals for {len(test_timestamps_aligned)} timestamps...\")\n",
    "\n",
    "            for idx, (timestamp, actual, predicted) in enumerate(zip(test_timestamps_aligned, y_true_prices, y_pred_prices)):\n",
    "                # Find the original index in the stock_df that corresponds to this timestamp\n",
    "                try:\n",
    "                    orig_idx = stock_df.index.get_loc(timestamp)\n",
    "                    market_data_window = stock_df.iloc[max(0, orig_idx-20):orig_idx+1].copy()\n",
    "                    \n",
    "                    if len(market_data_window) == 0:\n",
    "                        continue\n",
    "\n",
    "                    # Debug information for monitoring\n",
    "                    if idx % 1000 == 0:\n",
    "                        print(f\"Processing index {idx}: Current price = {actual:.2f}, Predicted price = {predicted:.2f}\")\n",
    "\n",
    "                    # Generate signal\n",
    "                    signal = monitor.update_ticker(\n",
    "                        ticker=ticker,\n",
    "                        current_price=actual,\n",
    "                        predicted_price=predicted,\n",
    "                        timestamp=timestamp,\n",
    "                        market_data=market_data_window\n",
    "                    )\n",
    "\n",
    "                    if signal and signal['action']:\n",
    "                        print(f\"Generated signal at {timestamp}: {signal['action']}\")\n",
    "                        all_signals.append(signal)\n",
    "                        \n",
    "                except KeyError:\n",
    "                    # Skip if timestamp is not found in the index\n",
    "                    continue\n",
    "\n",
    "            print(f\"Generated {len(all_signals)} signals\")\n",
    "\n",
    "            # Perform backtesting\n",
    "            trades = backtest_trades(all_signals, stock_df)\n",
    "            print(f\"Generated {len(trades)} trades\")\n",
    "\n",
    "            # Calculate performance metrics\n",
    "            model_evaluator = ModelEvaluator()\n",
    "            trade_metrics = model_evaluator.calculate_trade_performance_metrics(\n",
    "                trades=trades,\n",
    "                initial_capital=10000.0\n",
    "            )\n",
    "\n",
    "            # Print performance summary\n",
    "            print(\"\\nTrading Performance Summary:\")\n",
    "            print(f\"Total Trades: {trade_metrics['total_trades']}\")\n",
    "            print(f\"Win Rate: {trade_metrics['win_rate']:.2f}%\")\n",
    "            print(f\"Total Profit: ${trade_metrics['total_profit']:.2f}\")\n",
    "            print(f\"Maximum Drawdown: ${trade_metrics['maximum_drawdown']:.2f}\")\n",
    "            print(f\"Sharpe Ratio: {trade_metrics['sharpe_ratio']:.2f}\")\n",
    "\n",
    "            # Generate visualizations\n",
    "            plot_trading_metrics(trade_metrics, ticker)\n",
    "            plot_candlestick_analysis(stock_df, signals=all_signals, ticker=ticker)\n",
    "            \n",
    "            # Save model and artifacts\n",
    "            save_path = f'structured_models/{ticker}/'\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "            \n",
    "            model_filename = f'{save_path}enhanced_lstm_model_{ticker}_{end_date_str}.pth'\n",
    "            scaler_filename = f'{save_path}enhanced_scaler_{ticker}_{end_date_str}.pkl'\n",
    "            history_filename = f'{save_path}enhanced_training_history_{ticker}_{end_date_str}.pkl'\n",
    "            \n",
    "            torch.save(model.state_dict(), model_filename)\n",
    "            \n",
    "            # Save price and feature scalers\n",
    "            with open(scaler_filename, 'wb') as f:\n",
    "                pickle.dump({\n",
    "                    'price_scaler': dataset.price_scaler,\n",
    "                    'feature_scaler': dataset.feature_scaler\n",
    "                }, f)\n",
    "                \n",
    "            with open(history_filename, 'wb') as f:\n",
    "                pickle.dump(training_history, f)\n",
    "                \n",
    "            print(f\"\\nSaved model to {model_filename}\")\n",
    "            print(f\"Saved scalers to {scaler_filename}\")\n",
    "            print(f\"Saved training history to {history_filename}\")\n",
    "\n",
    "            logger.info(f\"Completed processing for {ticker}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing {ticker}: {e}\")\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "            \n",
    "        finally:\n",
    "            # Clean up memory\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    logger.info(\"Completed processing all tickers\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
