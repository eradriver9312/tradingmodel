# -*- coding: utf-8 -*-
"""Untitled74.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1My9_sBVc5Th3RdpRLhhASQFFhkUOApHr
"""

# Imports and Configurations
import requests
import pandas as pd
import numpy as np
import time
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
import mplfinance as mpf
from tqdm import tqdm
import torch
import torch.nn as nn
import torch.optim as optim
import pickle
import ta
import logging
import gc
import traceback
from dataclasses import dataclass, field
from typing import List, Optional, Dict
from sklearn.metrics import mean_squared_error, mean_absolute_error
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    filename='stock_prediction_hft.log',
    filemode='a',
    format='%(asctime)s:%(levelname)s:%(message)s'
)
logger = logging.getLogger(__name__)

# Set random seeds for reproducibility
np.random.seed(42)
torch.manual_seed(42)
if torch.cuda.is_available():
    torch.cuda.manual_seed(42)

# API Key Configuration
API_KEY = os.environ.get("POLYGON_API_KEY")

# Suppress mplfinance warnings for too much data
import warnings
warnings.filterwarnings("ignore", category=UserWarning, module="mplfinance")


@dataclass
class TrainingHistory:
    """Tracks training metrics during model training."""
    loss_history: List[float] = field(default_factory=list)
    validation_loss_history: List[float] = field(default_factory=list)

    def update(self, epoch: int, loss: float, val_loss: float):
        self.loss_history.append(loss)
        self.validation_loss_history.append(val_loss)

class ModelEvaluator:
    """Handles model evaluation metrics and performance monitoring."""

    def __init__(self, prediction_threshold: float = 0.001):
        self.prediction_threshold = prediction_threshold
        self.mse_scores = []
        self.mae_scores = []
        self.seasonal_errors = []

    def calculate_metrics(self, y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:
        """Calculate various error metrics based on y_true and y_pred."""
        metrics = {}

        # Ensure y_true and y_pred are 1-D and have the same length
        y_true = y_true.flatten()
        y_pred = y_pred.flatten()

        if len(y_true) != len(y_pred):
            logger.error(f"Length mismatch: y_true ({len(y_true)}) vs y_pred ({len(y_pred)})")
            raise ValueError("y_true and y_pred must have the same length.")

        # Handle NaNs and Infs
        if np.isnan(y_true).any() or np.isnan(y_pred).any():
            logger.error("NaN values found in y_true or y_pred.")
            raise ValueError("y_true and y_pred must not contain NaN values.")
        if np.isinf(y_true).any() or np.isinf(y_pred).any():
            logger.error("Infinite values found in y_true or y_pred.")
            raise ValueError("y_true and y_pred must not contain infinite values.")

        try:
            # Direction Accuracy
            direction_true = np.sign(y_true[1:] - y_true[:-1])
            direction_pred = np.sign(y_pred[1:] - y_pred[:-1])
            direction_accuracy = np.mean(direction_true == direction_pred) * 100
            metrics['direction_accuracy'] = direction_accuracy

            # Magnitude Correlation
            magnitude_correlation = np.corrcoef(y_true, y_pred)[0, 1]
            metrics['magnitude_correlation'] = magnitude_correlation

            # Timing Accuracy (e.g., predicting trend changes)
            trend_changes_true = np.where(direction_true != 0)[0]
            trend_changes_pred = np.where(direction_pred != 0)[0]
            if len(trend_changes_true) > 0:
                timing_accuracy = len(set(trend_changes_true).intersection(set(trend_changes_pred))) / len(trend_changes_true) * 100
            else:
                timing_accuracy = 0.0
            metrics['timing_accuracy'] = timing_accuracy

        except Exception as e:
            logger.error(f"Error in calculate_metrics: {e}")
            raise

        return metrics

    def calculate_trade_performance_metrics(self, trades: List[Dict]) -> Dict[str, float]:
        """
        Calculate performance metrics based on executed trades.

        Args:
            trades (List[Dict]): List of executed trades with entry and exit details.

        Returns:
            Dict[str, float]: Calculated performance metrics.
        """
        if not trades:
            logger.warning("No trades to evaluate.")
            return {
                'total_trades': 0,
                'total_profit': 0.0,
                'win_rate': 0.0,
                'average_profit_per_trade': 0.0,
                'maximum_drawdown': 0.0
            }

        profits = []
        for trade in trades:
            if trade['position'] == 'enter_long':
                profit = trade['exit_price'] - trade['entry_price']
            elif trade['position'] == 'enter_short':
                profit = trade['entry_price'] - trade['exit_price']
            profits.append(profit)

        total_profit = sum(profits)
        win_rate = sum(1 for p in profits if p > 0) / len(profits) * 100 if profits else 0
        avg_profit = np.mean(profits) if profits else 0
        max_drawdown = min(profits) if profits else 0

        metrics = {
            'total_trades': len(trades),
            'total_profit': total_profit,
            'win_rate': win_rate,
            'average_profit_per_trade': avg_profit,
            'maximum_drawdown': max_drawdown
        }

        return metrics

class SignalGenerator:
    def __init__(self, buy_threshold=1.5, sell_threshold=0.66, stop_loss_threshold=0.005):
        """
        Initialize the SignalGenerator with specified thresholds.

        Args:
            buy_threshold (float): Percentage threshold to trigger a buy signal.
            sell_threshold (float): Percentage threshold to trigger a sell signal.
            stop_loss_threshold (float): Threshold to trigger a stop-loss.
        """
        self.buy_threshold = buy_threshold
        self.sell_threshold = sell_threshold
        self.stop_loss_threshold = stop_loss_threshold
        self.position = None
        self.entry_price = None

    def generate_signals(self, ticker, current_price, predicted_price, timestamp):
        """
        Generate trading signals based on current and predicted prices.

        Args:
            ticker (str): Stock ticker symbol.
            current_price (float): Current stock price.
            predicted_price (float): Predicted stock price.
            timestamp (pd.Timestamp): Current timestamp.

        Returns:
            Dict: Signal containing action and position details.
        """
        signal = {'ticker': ticker, 'timestamp': timestamp, 'price': current_price, 'action': None, 'position': None}

        if self.position is None:
            # No current position, decide whether to enter
            if predicted_price > current_price * (1 + self.buy_threshold/100):
                # Price is expected to rise significantly
                self.position = 'long'
                self.entry_price = current_price
                signal['action'] = 'enter_long'
                signal['position'] = 'long'
            elif predicted_price < current_price * (1 - self.sell_threshold/100):
                # Price is expected to fall significantly
                self.position = 'short'
                self.entry_price = current_price
                signal['action'] = 'enter_short'
                signal['position'] = 'short'
        else:
            # Existing position, check for exit conditions
            if self.position == 'long':
                # Stop-loss condition
                if current_price <= self.entry_price * (1 - self.stop_loss_threshold):
                    self.position = None
                    self.entry_price = None
                    signal['action'] = 'exit_long'
                    signal['position'] = 'long'
                # Take profit condition
                elif current_price >= self.entry_price * (1 + self.buy_threshold/100):
                    self.position = None
                    self.entry_price = None
                    signal['action'] = 'exit_long'
                    signal['position'] = 'long'
            elif self.position == 'short':
                # Stop-loss condition
                if current_price >= self.entry_price * (1 + self.stop_loss_threshold):
                    self.position = None
                    self.entry_price = None
                    signal['action'] = 'exit_short'
                    signal['position'] = 'short'
                # Take profit condition
                elif current_price <= self.entry_price * (1 - self.sell_threshold/100):
                    self.position = None
                    self.entry_price = None
                    signal['action'] = 'exit_short'
                    signal['position'] = 'short'

        return signal

class MultiTickerMonitor:
    """Monitors multiple tickers simultaneously."""

    def __init__(self, signal_generator: SignalGenerator):
        """
        Initialize the MultiTickerMonitor with a SignalGenerator.

        Args:
            signal_generator (SignalGenerator): Instance to generate trading signals.
        """
        self.tracked_tickers = {}
        self.signal_generator = signal_generator

    def add_ticker(self, ticker: str, initial_price: float):
        """Add a new ticker to monitor."""
        self.tracked_tickers[ticker] = {
            'current_price': initial_price,
            'signals': [],
            'last_update': datetime.now()
        }

    def update_ticker(self, ticker: str, current_price: float,
                      predicted_price: float, timestamp: pd.Timestamp) -> Optional[Dict]:
        """Update ticker information and generate signals."""
        if ticker not in self.tracked_tickers:
            self.add_ticker(ticker, current_price)

        self.tracked_tickers[ticker]['current_price'] = current_price
        self.tracked_tickers[ticker]['last_update'] = timestamp

        signal = self.signal_generator.generate_signals(
            ticker, current_price, predicted_price,
            timestamp
        )

        if signal['action']:
            self.tracked_tickers[ticker]['signals'].append(signal)
            return signal

        return None

    def get_signals(self, ticker: str) -> List[Dict]:
        """Retrieve all signals for a given ticker."""
        if ticker in self.tracked_tickers:
            return self.tracked_tickers[ticker]['signals']
        return []

class PolygonDataFetcher:
    BASE_URL = "https://api.polygon.io/v2"

    def __init__(self, api_key):
        """
        Initialize the PolygonDataFetcher with API key.

        Args:
            api_key (str): Polygon.io API key.
        """
        self.api_key = api_key
        self.session = requests.Session()
        self.session.headers.update({"Authorization": f"Bearer {self.api_key}"})
        self.validation_metrics = []

    def _make_request(self, endpoint, params=None, retries=3):
        """
        Make an API request with retry logic.

        Args:
            endpoint (str): API endpoint.
            params (dict, optional): Query parameters.
            retries (int): Number of retries.

        Returns:
            dict: JSON response.
        """
        url = f"{self.BASE_URL}/{endpoint}"
        for attempt in range(retries):
            try:
                response = self.session.get(url, params=params, timeout=30)
                if response.status_code == 429:
                    wait_time = min(60 * (attempt + 1), 300)  # Max 5 minutes
                    logger.warning(f"Rate limit exceeded. Waiting {wait_time} seconds...")
                    time.sleep(wait_time)
                    continue
                response.raise_for_status()
                data = response.json()

                # Log the keys of the response
                logger.debug(f"API response keys: {data.keys()}")

                # Optional: Log a sample of the results
                if 'results' in data and len(data['results']) > 0:
                    logger.debug(f"Sample result: {data['results'][0]}")

                return data
            except Exception as e:
                if attempt == retries - 1:
                    logger.error(f"Request failed after {retries} attempts: {e}")
                    raise
                logger.warning(f"Request failed (Attempt {attempt + 1}/{retries}): {e}. Retrying in 30 seconds...")
                time.sleep(30)
        raise Exception(f"Failed after {retries} attempts")

    def fetch_stock_data(self, symbol, start_date, end_date, timespan="minute", multiplier=1):
        """
        Fetch aggregate stock data (e.g., OHLC) for a specific ticker and date range.

        Args:
            symbol (str): Stock ticker symbol.
            start_date (str): Start date in YYYY-MM-DD format.
            end_date (str): End date in YYYY-MM-DD format.
            timespan (str, optional): Timespan of the aggregates ('minute', 'hour', 'day'). Defaults to "minute".
            multiplier (int, optional): The size of the timespan multiplier. Defaults to 1.

        Returns:
            pd.DataFrame: DataFrame containing aggregated stock data.
        """
        endpoint = f"aggs/ticker/{symbol}/range/{multiplier}/{timespan}/{start_date}/{end_date}"
        params = {"adjusted": "true", "sort": "asc", "limit": 50000}
        all_results = []

        with tqdm(desc=f"Fetching {symbol}", unit='rows') as pbar:
            while True:
                data = self._make_request(endpoint, params)
                if 'results' in data:
                    # Check if 'c' (close) is in first result
                    if len(data['results']) > 0 and 'c' not in data['results'][0]:
                        logger.error(f"Data for {symbol} does not contain 'c' (close) field.")
                        return pd.DataFrame()

                    all_results.extend(data['results'])
                    pbar.update(len(data['results']))
                if len(data.get('results', [])) < 50000:
                    break
                last_timestamp = data['results'][-1]['t']
                # Increment start_date to avoid infinite loop
                start_date = datetime.fromtimestamp(last_timestamp / 1000).strftime('%Y-%m-%d')
                params['from'] = start_date

        df = pd.DataFrame(all_results)

        # Remove duplicate timestamps
        if 't' in df.columns:
            duplicates_before = len(df) - df.drop_duplicates(subset=['t'], keep='last').shape[0]
            if duplicates_before > 0:
                logger.info(f"Removed {duplicates_before} duplicate timestamps for {symbol}.")
            df = df.drop_duplicates(subset=['t'], keep='last')
        else:
            logger.error("'t' column not found in fetched data.")
            return pd.DataFrame()

        # Rename columns for clarity
        df = df.rename(columns={
            't': 'timestamp',
            'o': 'open',
            'h': 'high',
            'l': 'low',
            'c': 'close',
            'v': 'volume',
            'n': 'transactions'
        })

        # Convert timestamp to datetime
        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
        df.set_index('timestamp', inplace=True)

        return df

class HFTDataset(Dataset):
    """Custom Dataset for High-Frequency Trading."""

    def __init__(self, data: pd.DataFrame, sequence_length: int, scaler: Optional[StandardScaler] = None):
        """
        Initialize the dataset.

        Args:
            data (pd.DataFrame): Processed DataFrame with features.
            sequence_length (int): Number of past timesteps to include.
            scaler (StandardScaler, optional): Scaler for feature normalization.
        """
        self.sequence_length = sequence_length
        self.scaler = scaler or StandardScaler()
        feature_cols = ['close', 'volume', 'SMA_20', 'EMA_20', 'RSI']

        # Ensure all required feature columns are present
        missing_cols = [col for col in feature_cols if col not in data.columns]
        if missing_cols:
            logger.error(f"Missing feature columns: {missing_cols}")
            raise ValueError(f"Missing feature columns: {missing_cols}")

        self.features = data[feature_cols].values
        self.labels = data['close'].shift(-1).values  # Predicting next close price

        # Remove the last row as it contains NaN label
        self.features = self.features[:-1]
        self.labels = self.labels[:-1]

        # Normalize features
        self.features = self.scaler.fit_transform(self.features)

    def __len__(self):
        return len(self.labels) - self.sequence_length

    def __getitem__(self, idx):
        x = self.features[idx:idx + self.sequence_length]
        y = self.labels[idx + self.sequence_length]
        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)

class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size=1):
        """
        Initialize the LSTM model.

        Args:
            input_size (int): Number of input features.
            hidden_size (int): Number of features in the hidden state.
            num_layers (int): Number of recurrent layers.
            output_size (int, optional): Size of the output. Defaults to 1.
        """
        super(LSTMModel, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers

        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        # Set initial hidden and cell states
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)

        # Forward propagate LSTM
        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch, seq_length, hidden_size)

        # Decode the hidden state of the last time step
        out = self.fc(out[:, -1, :])
        return out

def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, training_history: Optional[TrainingHistory] = None):
    """
    Train the LSTM model.

    Args:
        model (nn.Module): The LSTM model.
        train_loader (DataLoader): DataLoader for training data.
        val_loader (DataLoader): DataLoader for validation data.
        criterion: Loss function.
        optimizer: Optimizer.
        num_epochs (int): Number of training epochs.
        device: Computation device.
        training_history (TrainingHistory, optional): To track training progress.

    Returns:
        nn.Module: Trained model.
    """
    model.to(device)

    for epoch in range(1, num_epochs + 1):
        model.train()
        train_losses = []

        for batch_x, batch_y in train_loader:
            batch_x = batch_x.to(device)
            batch_y = batch_y.to(device).unsqueeze(1)

            optimizer.zero_grad()
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()

            train_losses.append(loss.item())

        avg_train_loss = np.mean(train_losses)

        # Validation
        model.eval()
        val_losses = []
        with torch.no_grad():
            for batch_x, batch_y in val_loader:
                batch_x = batch_x.to(device)
                batch_y = batch_y.to(device).unsqueeze(1)

                outputs = model(batch_x)
                loss = criterion(outputs, batch_y)
                val_losses.append(loss.item())

        avg_val_loss = np.mean(val_losses)

        if training_history:
            training_history.update(epoch, avg_train_loss, avg_val_loss)

        logger.info(f"Epoch [{epoch}/{num_epochs}], Train Loss: {avg_train_loss:.6f}, Val Loss: {avg_val_loss:.6f}")
        print(f"Epoch [{epoch}/{num_epochs}], Train Loss: {avg_train_loss:.6f}, Val Loss: {avg_val_loss:.6f}")

        # Early Stopping or other callbacks can be implemented here

    return model

def evaluate_model(model, test_loader, device):
    """
    Evaluate the trained model on test data.

    Args:
        model (nn.Module): Trained model.
        test_loader (DataLoader): DataLoader for test data.
        device: Computation device.

    Returns:
        Tuple[np.ndarray, np.ndarray]: True and predicted values.
    """
    model.eval()
    y_true = []
    y_pred = []

    with torch.no_grad():
        for batch_x, batch_y in test_loader:
            batch_x = batch_x.to(device)
            outputs = model(batch_x).cpu().numpy()
            y_true.extend(batch_y.numpy())
            y_pred.extend(outputs.flatten())

    y_true = np.array(y_true)
    y_pred = np.array(y_pred)

    return y_true, y_pred

def backtest_trades(signals: List[Dict], df: pd.DataFrame) -> List[Dict]:
    """
    Execute trades based on generated signals and record trade details.

    Args:
        signals (List[Dict]): List of generated signals.
        df (pd.DataFrame): DataFrame with actual prices.

    Returns:
        List[Dict]: List of executed trades with entry and exit details.
    """
    trades = []
    entry_signal = None

    for signal in signals:
        if signal['action'] in ['enter_long', 'enter_short']:
            if entry_signal is None:
                entry_signal = signal
        elif signal['action'] in ['exit_long', 'exit_short'] and entry_signal is not None:
            exit_reason = 'stop_loss' if signal['action'].startswith('exit_') else 'take_profit'
            trade = {
                'entry_time': entry_signal['timestamp'],
                'entry_price': entry_signal['price'],
                'exit_time': signal['timestamp'],
                'exit_price': signal['price'],
                'position': entry_signal['action'],
                'exit_reason': exit_reason
            }
            trades.append(trade)
            entry_signal = None

    # Close any open positions at the end of data
    if entry_signal is not None:
        last_price = df['close'].iloc[-1]
        trade = {
            'entry_time': entry_signal['timestamp'],
            'entry_price': entry_signal['price'],
            'exit_time': df.index[-1],
            'exit_price': last_price,
            'position': entry_signal['action'],
            'exit_reason': 'end_of_data'
        }
        trades.append(trade)

    return trades

def execute_trades(trades: List[Dict]):
    """
    Execute trades based on backtested signals.

    Args:
        trades (List[Dict]): List of executed trades.
    """
    for trade in trades:
        entry_time = trade['entry_time']
        entry_price = trade['entry_price']
        exit_time = trade['exit_time']
        exit_price = trade['exit_price']
        position = trade['position']
        exit_reason = trade['exit_reason']

        # Implement actual trade execution logic here
        # For example, interfacing with a brokerage API to place orders
        logger.info(f"Executed Trade - Entry: {position} at {entry_price} on {entry_time}, "
                    f"Exit: {exit_price} on {exit_time}, Reason: {exit_reason}")
        print(f"Executed Trade - Entry: {position} at {entry_price} on {entry_time}, "
              f"Exit: {exit_price} on {exit_time}, Reason: {exit_reason}")

def plot_candlestick_analysis(df, signals=None, ticker=''):
    """Plot candlestick chart with trade signals."""
    try:
        # Prepare the main candlestick data
        df_plot = df.copy()
        df_plot = df_plot[['open', 'high', 'low', 'close', 'volume']]
        df_plot.index.name = 'Date'

        # Create subplots: one for candlestick, one for volume
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(16, 12), sharex=True)
        fig.subplots_adjust(hspace=0)

        # Plot candlestick chart on ax1 and volume on ax2
        mc = mpf.make_marketcolors(up='g', down='r', inherit=True)
        s = mpf.make_mpf_style(marketcolors=mc)
        mpf.plot(df_plot, type='candle', style=s, ax=ax1, volume=ax2, show_nontrading=False)

        # Plot Signals on ax1
        if signals:
            for signal in signals:
                if 'enter_long' in signal['action']:
                    ax1.scatter(signal['timestamp'], signal['price'], marker='^', color='g', s=100, label='Buy Signal')
                elif 'enter_short' in signal['action']:
                    ax1.scatter(signal['timestamp'], signal['price'], marker='v', color='r', s=100, label='Sell Signal')
                elif 'exit_long' in signal['action'] or 'exit_short' in signal['action']:
                    ax1.scatter(signal['timestamp'], signal['price'], marker='o', color='k', s=100, label='Exit Signal')

            # To avoid duplicate labels in legend
            handles, labels = ax1.get_legend_handles_labels()
            by_label = dict(zip(labels, handles))
            ax1.legend(by_label.values(), by_label.keys())

        plt.title(f"Candlestick Chart with Trade Signals for {ticker}")
        plt.xlabel('Date')
        plt.ylabel('Price')
        plt.tight_layout()
        plt.show()

    except Exception as e:
        logger.error(f"Error in plot_candlestick_analysis: {e}")
        print(f"Error in plot_candlestick_analysis: {e}")
        raise

def main():
    # Configuration
    TICKERS = ['TSLA', 'NVDA',"GOOGL","MSFT"]
    DATE = '2023-10-27'  # Replace with a valid past date in YYYY-MM-DD format

    # Model parameters
    PARAMS = {
        'sequence_length': 60,  # Number of past minutes to consider
        'hidden_size': 128,
        'num_layers': 2,
        'batch_size': 64,
        'num_epochs': 50,
        'learning_rate': 1e-3
    }

    # Initialize Data Fetcher
    fetcher = PolygonDataFetcher(API_KEY)

    # Initialize Signal Generator
    signal_generator = SignalGenerator(buy_threshold=1.5, sell_threshold=0.66, stop_loss_threshold=0.005)
    monitor = MultiTickerMonitor(signal_generator)

    # Initialize Training History and Model Evaluator
    training_history = TrainingHistory()
    model_evaluator = ModelEvaluator()

    # Device Configuration
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"Using device: {device}")
    print(f"Using device: {device}")

    for ticker in TICKERS:
        try:
            logger.info(f"Processing {ticker}")
            print(f"\nProcessing {ticker}...")

            # Define date range (assuming end_date is the same as DATE)
            start_date = (datetime.strptime(DATE, '%Y-%m-%d') - timedelta(days=30)).strftime('%Y-%m-%d')  # 30 days before
            end_date = DATE

            # Fetch Aggregate Stock Data
            stock_df = fetcher.fetch_stock_data(ticker, start_date, end_date, timespan="minute", multiplier=1)

            if stock_df.empty:
                logger.warning(f"No data fetched for {ticker} from {start_date} to {end_date}. Skipping.")
                print(f"No data fetched for {ticker} from {start_date} to {end_date}. Skipping.")
                continue

            # Feature Engineering
            stock_df['SMA_20'] = ta.trend.sma_indicator(stock_df['close'], window=20)
            stock_df['EMA_20'] = ta.trend.ema_indicator(stock_df['close'], window=20)
            stock_df['RSI'] = ta.momentum.rsi(stock_df['close'], window=14)

            # Drop rows with NaN values resulting from indicators
            stock_df.dropna(inplace=True)

            # Initialize Dataset and DataLoaders
            scaler = StandardScaler()
            dataset = HFTDataset(stock_df, sequence_length=PARAMS['sequence_length'], scaler=scaler)
            train_size = int(0.7 * len(dataset))
            val_size = int(0.15 * len(dataset))
            test_size = len(dataset) - train_size - val_size

            train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])

            train_loader = DataLoader(train_dataset, batch_size=PARAMS['batch_size'], shuffle=True)
            val_loader = DataLoader(val_dataset, batch_size=PARAMS['batch_size'], shuffle=False)
            test_loader = DataLoader(test_dataset, batch_size=PARAMS['batch_size'], shuffle=False)

            # Initialize Model, Loss, Optimizer
            input_size = 5  # Number of features: ['close', 'volume', 'SMA_20', 'EMA_20', 'RSI']
            model = LSTMModel(input_size, PARAMS['hidden_size'], PARAMS['num_layers'])
            criterion = nn.MSELoss()
            optimizer = optim.Adam(model.parameters(), lr=PARAMS['learning_rate'])

            # Train Model
            logger.info("Starting model training...")
            print("Starting model training...")
            model = train_model(model, train_loader, val_loader, criterion, optimizer, PARAMS['num_epochs'], device, training_history)

            # Evaluate Model
            logger.info("Evaluating model...")
            print("Evaluating model...")
            y_true, y_pred = evaluate_model(model, test_loader, device)

            # Calculate Metrics
            metrics = model_evaluator.calculate_metrics(y_true, y_pred)
            logger.info(f"Model Metrics: {metrics}")
            print(f"Model Metrics: {metrics}")

            # Backtesting: Generate Predictions and Signals
            # Align test_loader indices with timestamps
            test_indices = np.arange(PARAMS['sequence_length'], len(stock_df))
            test_timestamps = stock_df.index[test_indices[-len(y_pred):]]

            all_signals = []

            for timestamp, actual, predicted in zip(test_timestamps, y_true, y_pred):
                current_price = actual
                predicted_price = predicted

                # Generate Signal
                signal = monitor.update_ticker(
                    ticker,
                    current_price,
                    predicted_price,
                    timestamp
                )

                if signal:
                    all_signals.append(signal)
                    logger.debug(f"Generated Signal: {signal}")

            # Backtest Trades
            trades = backtest_trades(all_signals, stock_df)
            trade_metrics = model_evaluator.calculate_trade_performance_metrics(trades)

            logger.info(f"Trade Performance Metrics: {trade_metrics}")
            print(f"Trade Performance Metrics: {trade_metrics}")

            # Execute Trades
            execute_trades(trades)

            # Visualization: Plot Trades
            plot_candlestick_analysis(stock_df, signals=all_signals, ticker=ticker)

            # Save Model and Scaler
            model_filename = f'lstm_model_{ticker}.pth'
            scaler_filename = f'scaler_{ticker}.pkl'
            torch.save(model.state_dict(), model_filename)
            with open(scaler_filename, 'wb') as f:
                pickle.dump(scaler, f)
            logger.info(f"Saved model to {model_filename} and scaler to {scaler_filename}")
            print(f"Saved model to {model_filename} and scaler to {scaler_filename}")

            logger.info(f"Completed processing for {ticker}")
            print(f"Completed processing for {ticker}")

        except Exception as e:
            logger.error(f"Error processing {ticker}: {e}")
            print(f"Error processing {ticker}: {e}")
            traceback.print_exc()
            continue
        finally:
            # Clean up memory
            try:
                del stock_df, dataset, train_dataset, val_dataset, test_dataset, train_loader, val_loader, test_loader, model
                gc.collect()
            except NameError:
                pass

if __name__ == "__main__":
    main()