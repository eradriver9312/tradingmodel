{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334401aa-2f7d-48ce-bcf9-0713e64f1d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter year:  2024\n",
      "Enter month:  10\n",
      "Enter day (optional):  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-15 20:15:36,522 [INFO] Listing files under prefix: us_stocks_sip/trades_v1/2024/10/\n",
      "2025-01-15 20:15:36,691 [INFO] Found 23 files to process.\n",
      "2025-01-15 20:15:36,692 [INFO] Processing file: us_stocks_sip/trades_v1/2024/10/2024-10-01.csv.gz\n",
      "2025-01-15 20:16:26,981 [INFO] wbout to trigger upload attempt, with bucket: ml-3bean-ts-raw-data\n",
      "2025-01-15 20:16:26,982 [INFO] retry with backoff\n",
      "2025-01-15 20:17:07,102 [INFO] Successfully processed and uploaded: us_stocks_sip/trades_v1/2024/10/2024-10-01.csv.gz\n",
      "2025-01-15 20:17:07,414 [INFO] Processing file: us_stocks_sip/trades_v1/2024/10/2024-10-02.csv.gz\n",
      "2025-01-15 20:17:51,577 [INFO] wbout to trigger upload attempt, with bucket: ml-3bean-ts-raw-data\n",
      "2025-01-15 20:17:51,578 [INFO] retry with backoff\n",
      "2025-01-15 20:18:26,375 [INFO] Successfully processed and uploaded: us_stocks_sip/trades_v1/2024/10/2024-10-02.csv.gz\n",
      "2025-01-15 20:18:26,654 [INFO] Processing file: us_stocks_sip/trades_v1/2024/10/2024-10-03.csv.gz\n",
      "2025-01-15 20:19:08,123 [INFO] wbout to trigger upload attempt, with bucket: ml-3bean-ts-raw-data\n",
      "2025-01-15 20:19:08,123 [INFO] retry with backoff\n",
      "2025-01-15 20:19:40,989 [INFO] Successfully processed and uploaded: us_stocks_sip/trades_v1/2024/10/2024-10-03.csv.gz\n",
      "2025-01-15 20:19:41,250 [INFO] Processing file: us_stocks_sip/trades_v1/2024/10/2024-10-04.csv.gz\n",
      "2025-01-15 20:20:25,386 [INFO] wbout to trigger upload attempt, with bucket: ml-3bean-ts-raw-data\n",
      "2025-01-15 20:20:25,387 [INFO] retry with backoff\n",
      "2025-01-15 20:20:59,655 [INFO] Successfully processed and uploaded: us_stocks_sip/trades_v1/2024/10/2024-10-04.csv.gz\n",
      "2025-01-15 20:20:59,925 [INFO] Processing file: us_stocks_sip/trades_v1/2024/10/2024-10-07.csv.gz\n",
      "2025-01-15 20:21:46,098 [INFO] wbout to trigger upload attempt, with bucket: ml-3bean-ts-raw-data\n",
      "2025-01-15 20:21:46,099 [INFO] retry with backoff\n",
      "2025-01-15 20:22:21,916 [INFO] Successfully processed and uploaded: us_stocks_sip/trades_v1/2024/10/2024-10-07.csv.gz\n",
      "2025-01-15 20:22:22,198 [INFO] Processing file: us_stocks_sip/trades_v1/2024/10/2024-10-08.csv.gz\n",
      "2025-01-15 20:23:04,805 [INFO] wbout to trigger upload attempt, with bucket: ml-3bean-ts-raw-data\n",
      "2025-01-15 20:23:04,806 [INFO] retry with backoff\n",
      "2025-01-15 20:23:38,596 [INFO] Successfully processed and uploaded: us_stocks_sip/trades_v1/2024/10/2024-10-08.csv.gz\n",
      "2025-01-15 20:23:38,865 [INFO] Processing file: us_stocks_sip/trades_v1/2024/10/2024-10-09.csv.gz\n",
      "2025-01-15 20:24:20,938 [INFO] wbout to trigger upload attempt, with bucket: ml-3bean-ts-raw-data\n",
      "2025-01-15 20:24:20,939 [INFO] retry with backoff\n",
      "2025-01-15 20:24:53,936 [INFO] Successfully processed and uploaded: us_stocks_sip/trades_v1/2024/10/2024-10-09.csv.gz\n",
      "2025-01-15 20:24:54,199 [INFO] Processing file: us_stocks_sip/trades_v1/2024/10/2024-10-10.csv.gz\n",
      "2025-01-15 20:25:35,261 [INFO] wbout to trigger upload attempt, with bucket: ml-3bean-ts-raw-data\n",
      "2025-01-15 20:25:35,261 [INFO] retry with backoff\n",
      "2025-01-15 20:26:07,732 [INFO] Successfully processed and uploaded: us_stocks_sip/trades_v1/2024/10/2024-10-10.csv.gz\n",
      "2025-01-15 20:26:07,989 [INFO] Processing file: us_stocks_sip/trades_v1/2024/10/2024-10-11.csv.gz\n",
      "2025-01-15 20:26:49,504 [INFO] wbout to trigger upload attempt, with bucket: ml-3bean-ts-raw-data\n",
      "2025-01-15 20:26:49,505 [INFO] retry with backoff\n",
      "2025-01-15 20:27:21,967 [INFO] Successfully processed and uploaded: us_stocks_sip/trades_v1/2024/10/2024-10-11.csv.gz\n",
      "2025-01-15 20:27:22,228 [INFO] Processing file: us_stocks_sip/trades_v1/2024/10/2024-10-14.csv.gz\n",
      "2025-01-15 20:28:02,710 [INFO] wbout to trigger upload attempt, with bucket: ml-3bean-ts-raw-data\n",
      "2025-01-15 20:28:02,711 [INFO] retry with backoff\n",
      "2025-01-15 20:28:35,096 [INFO] Successfully processed and uploaded: us_stocks_sip/trades_v1/2024/10/2024-10-14.csv.gz\n",
      "2025-01-15 20:28:35,348 [INFO] Processing file: us_stocks_sip/trades_v1/2024/10/2024-10-15.csv.gz\n",
      "2025-01-15 20:29:26,520 [INFO] wbout to trigger upload attempt, with bucket: ml-3bean-ts-raw-data\n",
      "2025-01-15 20:29:26,521 [INFO] retry with backoff\n",
      "2025-01-15 20:30:07,329 [INFO] Successfully processed and uploaded: us_stocks_sip/trades_v1/2024/10/2024-10-15.csv.gz\n",
      "2025-01-15 20:30:07,641 [INFO] Processing file: us_stocks_sip/trades_v1/2024/10/2024-10-16.csv.gz\n",
      "2025-01-15 20:30:51,950 [INFO] wbout to trigger upload attempt, with bucket: ml-3bean-ts-raw-data\n",
      "2025-01-15 20:30:51,951 [INFO] retry with backoff\n",
      "2025-01-15 20:31:26,589 [INFO] Successfully processed and uploaded: us_stocks_sip/trades_v1/2024/10/2024-10-16.csv.gz\n",
      "2025-01-15 20:31:26,864 [INFO] Processing file: us_stocks_sip/trades_v1/2024/10/2024-10-17.csv.gz\n",
      "2025-01-15 20:32:11,907 [INFO] wbout to trigger upload attempt, with bucket: ml-3bean-ts-raw-data\n",
      "2025-01-15 20:32:11,907 [INFO] retry with backoff\n",
      "2025-01-15 20:32:46,666 [INFO] Successfully processed and uploaded: us_stocks_sip/trades_v1/2024/10/2024-10-17.csv.gz\n",
      "2025-01-15 20:32:46,944 [INFO] Processing file: us_stocks_sip/trades_v1/2024/10/2024-10-18.csv.gz\n",
      "2025-01-15 20:33:28,445 [INFO] wbout to trigger upload attempt, with bucket: ml-3bean-ts-raw-data\n",
      "2025-01-15 20:33:28,446 [INFO] retry with backoff\n",
      "2025-01-15 20:34:02,563 [INFO] Successfully processed and uploaded: us_stocks_sip/trades_v1/2024/10/2024-10-18.csv.gz\n",
      "2025-01-15 20:34:02,823 [INFO] Processing file: us_stocks_sip/trades_v1/2024/10/2024-10-21.csv.gz\n",
      "2025-01-15 20:34:46,424 [INFO] wbout to trigger upload attempt, with bucket: ml-3bean-ts-raw-data\n",
      "2025-01-15 20:34:46,424 [INFO] retry with backoff\n",
      "2025-01-15 20:35:20,914 [INFO] Successfully processed and uploaded: us_stocks_sip/trades_v1/2024/10/2024-10-21.csv.gz\n",
      "2025-01-15 20:35:21,181 [INFO] Processing file: us_stocks_sip/trades_v1/2024/10/2024-10-22.csv.gz\n",
      "2025-01-15 20:36:03,256 [INFO] wbout to trigger upload attempt, with bucket: ml-3bean-ts-raw-data\n",
      "2025-01-15 20:36:03,257 [INFO] retry with backoff\n",
      "2025-01-15 20:36:35,134 [INFO] Successfully processed and uploaded: us_stocks_sip/trades_v1/2024/10/2024-10-22.csv.gz\n",
      "2025-01-15 20:36:35,398 [INFO] Processing file: us_stocks_sip/trades_v1/2024/10/2024-10-23.csv.gz\n",
      "2025-01-15 20:37:21,265 [INFO] wbout to trigger upload attempt, with bucket: ml-3bean-ts-raw-data\n",
      "2025-01-15 20:37:21,265 [INFO] retry with backoff\n",
      "2025-01-15 20:37:56,478 [INFO] Successfully processed and uploaded: us_stocks_sip/trades_v1/2024/10/2024-10-23.csv.gz\n",
      "2025-01-15 20:37:56,762 [INFO] Processing file: us_stocks_sip/trades_v1/2024/10/2024-10-24.csv.gz\n",
      "2025-01-15 20:38:41,570 [INFO] wbout to trigger upload attempt, with bucket: ml-3bean-ts-raw-data\n",
      "2025-01-15 20:38:41,570 [INFO] retry with backoff\n",
      "2025-01-15 20:39:15,224 [INFO] Successfully processed and uploaded: us_stocks_sip/trades_v1/2024/10/2024-10-24.csv.gz\n",
      "2025-01-15 20:39:15,499 [INFO] Processing file: us_stocks_sip/trades_v1/2024/10/2024-10-25.csv.gz\n",
      "2025-01-15 20:40:00,563 [INFO] wbout to trigger upload attempt, with bucket: ml-3bean-ts-raw-data\n",
      "2025-01-15 20:40:00,563 [INFO] retry with backoff\n",
      "2025-01-15 20:40:34,944 [INFO] Successfully processed and uploaded: us_stocks_sip/trades_v1/2024/10/2024-10-25.csv.gz\n",
      "2025-01-15 20:40:35,223 [INFO] Processing file: us_stocks_sip/trades_v1/2024/10/2024-10-28.csv.gz\n",
      "2025-01-15 20:41:21,157 [INFO] wbout to trigger upload attempt, with bucket: ml-3bean-ts-raw-data\n",
      "2025-01-15 20:41:21,158 [INFO] retry with backoff\n",
      "2025-01-15 20:41:56,382 [INFO] Successfully processed and uploaded: us_stocks_sip/trades_v1/2024/10/2024-10-28.csv.gz\n",
      "2025-01-15 20:41:56,665 [INFO] Processing file: us_stocks_sip/trades_v1/2024/10/2024-10-29.csv.gz\n",
      "2025-01-15 20:42:44,628 [INFO] wbout to trigger upload attempt, with bucket: ml-3bean-ts-raw-data\n",
      "2025-01-15 20:42:44,629 [INFO] retry with backoff\n",
      "2025-01-15 20:43:23,364 [INFO] Successfully processed and uploaded: us_stocks_sip/trades_v1/2024/10/2024-10-29.csv.gz\n",
      "2025-01-15 20:43:23,660 [INFO] Processing file: us_stocks_sip/trades_v1/2024/10/2024-10-30.csv.gz\n",
      "2025-01-15 20:44:13,842 [INFO] wbout to trigger upload attempt, with bucket: ml-3bean-ts-raw-data\n",
      "2025-01-15 20:44:13,843 [INFO] retry with backoff\n",
      "2025-01-15 20:44:54,570 [INFO] Successfully processed and uploaded: us_stocks_sip/trades_v1/2024/10/2024-10-30.csv.gz\n",
      "2025-01-15 20:44:54,875 [INFO] Processing file: us_stocks_sip/trades_v1/2024/10/2024-10-31.csv.gz\n",
      "2025-01-15 20:45:50,301 [INFO] wbout to trigger upload attempt, with bucket: ml-3bean-ts-raw-data\n",
      "2025-01-15 20:45:50,301 [INFO] retry with backoff\n",
      "2025-01-15 20:46:32,524 [INFO] Successfully processed and uploaded: us_stocks_sip/trades_v1/2024/10/2024-10-31.csv.gz\n",
      "2025-01-15 20:46:32,856 [INFO] All files processed successfully.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import gzip\n",
    "import io\n",
    "import logging\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from botocore.exceptions import ClientError\n",
    "from botocore.config import Config\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time\n",
    " \n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# AWS credentials and bucket information from .env\n",
    "SOURCE_BUCKET = 'flatfiles'\n",
    "DEST_BUCKET = os.getenv(\"DEST_BUCKET\")\n",
    "SOURCE_REGION = os.getenv(\"SOURCE_REGION\")\n",
    "DEST_REGION = os.getenv(\"DEST_REGION\")\n",
    "SOURCE_ACCESS_KEY = os.getenv(\"SOURCE_ACCESS_KEY\")\n",
    "SOURCE_SECRET_KEY = os.getenv(\"SOURCE_SECRET_KEY\")\n",
    "DEST_ACCESS_KEY = os.getenv(\"DEST_ACCESS_KEY\")\n",
    "DEST_SECRET_KEY = os.getenv(\"DEST_SECRET_KEY\")\n",
    "\n",
    "# Initialize S3 clients\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=SOURCE_ACCESS_KEY,\n",
    "    aws_secret_access_key=SOURCE_SECRET_KEY\n",
    ")\n",
    "\n",
    "\n",
    "source_s3_client = session.client(\n",
    "    's3',\n",
    "    aws_access_key_id=SOURCE_ACCESS_KEY,\n",
    "    aws_secret_access_key=SOURCE_SECRET_KEY,\n",
    "    endpoint_url='https://files.polygon.io',\n",
    "    config=Config(signature_version='s3v4')\n",
    ")\n",
    "\n",
    "destination_s3_client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=DEST_ACCESS_KEY,\n",
    "    aws_secret_access_key=DEST_SECRET_KEY,\n",
    "    region_name=DEST_REGION\n",
    ")\n",
    "\n",
    "# Logger setup\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')\n",
    "\n",
    "\n",
    "def retry_with_backoff(func, retries=3, backoff_factor=2, **kwargs):\n",
    "    \"\"\"Retry logic with exponential backoff.\"\"\"\n",
    "    logging.info(\"retry with backoff\")\n",
    "    for attempt in range(1, retries + 1):\n",
    "        try:\n",
    "            return func(**kwargs)\n",
    "        except Exception as e:\n",
    "            if attempt == retries:\n",
    "                logging.error(f\"Failed after {retries} attempts: {e}\")\n",
    "                raise\n",
    "            wait_time = backoff_factor ** attempt\n",
    "            logging.warning(f\"Retrying in {wait_time}s due to: {e}\")\n",
    "            time.sleep(wait_time)\n",
    "\n",
    "\n",
    "def transfer_and_decompress(file_key):\n",
    "    try:\n",
    "        logging.info(f\"Processing file: {file_key}\")\n",
    "\n",
    "        # Stream download from source bucket\n",
    "        response = source_s3_client.get_object(Bucket=SOURCE_BUCKET, Key=file_key)\n",
    "        compressed_stream = response['Body']\n",
    "\n",
    "        # Decompress in chunks and stream to destination bucket\n",
    "        with gzip.GzipFile(fileobj=compressed_stream) as gz:\n",
    "            uncompressed_stream = io.BytesIO()\n",
    "            while chunk := gz.read(8192):  # Read in 8 KB chunks\n",
    "                uncompressed_stream.write(chunk)\n",
    "            uncompressed_stream.seek(0)  # Reset pointer for uploading\n",
    "\n",
    "            # Extract year and month from the file key\n",
    "            parts = file_key.split('/')  # Example: 'stocks/trades/2025/01/2025-01-13.csv.gz'\n",
    "            year = parts[2]  # Third segment\n",
    "            month = parts[3]  # Fourth segment\n",
    "            file_name = parts[-1].replace('.gz', '')  # Remove .gz extension\n",
    "\n",
    "            # Construct the destination key\n",
    "            destination_key = f\"{year}/{month}/{file_name}\"\n",
    "            logging.info(f\"wbout to trigger upload attempt, with bucket: {DEST_BUCKET}\")\n",
    "            retry_with_backoff(\n",
    "                destination_s3_client.upload_fileobj,\n",
    "                retries=3,\n",
    "                backoff_factor=2,\n",
    "                Fileobj=uncompressed_stream,\n",
    "                Bucket=DEST_BUCKET,\n",
    "                Key=destination_key\n",
    "            )\n",
    "            logging.info(f\"Successfully processed and uploaded: {file_key}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing {file_key}: {e}\")\n",
    "\n",
    "\n",
    "def process_files_concurrently(files):\n",
    "    \"\"\"Process files in parallel.\"\"\"\n",
    "    with ThreadPoolExecutor(max_workers=1) as executor:\n",
    "        futures = {executor.submit(transfer_and_decompress, file): file for file in files}\n",
    "        for future in as_completed(futures):\n",
    "            try:\n",
    "                future.result()\n",
    "            except Exception:\n",
    "                logging.error(f\"Failed processing file: {futures[future]}\")\n",
    "\n",
    "\n",
    "def main(year, month, day=None):\n",
    "    # If day is provided, directly construct the file name and process that file\n",
    "    if day:\n",
    "        file_name = f\"us_stocks_sip/trades_v1/{year}/{month:02d}/{year}-{month:02d}-{day:02d}.csv.gz\"\n",
    "        logging.info(f\"Processing file: {file_name}\")\n",
    "        try:\n",
    "            transfer_and_decompress(file_name)\n",
    "            logging.info(\"File processed successfully.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to process file: {file_name}, Error: {e}\")\n",
    "        return\n",
    "\n",
    "*\n",
    "    prefix = f\"us_stocks_sip/trades_v1/{year}/{month:02d}/\"\n",
    "    logging.info(f\"Listing files under prefix: {prefix}\")\n",
    "    files = []\n",
    "    try:\n",
    "        paginator = source_s3_client.get_paginator('list_objects_v2')\n",
    "        for page in paginator.paginate(Bucket=SOURCE_BUCKET, Prefix=prefix):\n",
    "            if 'Contents' in page:\n",
    "                files.extend(obj['Key'] for obj in page['Contents'])\n",
    "    except ClientError as e:\n",
    "        logging.error(f\"Failed to list objects: {e}\")\n",
    "        return\n",
    "\n",
    "    if files:\n",
    "        logging.info(f\"Found {len(files)} files to process.\")\n",
    "        process_files_concurrently(files)\n",
    "        logging.info(\"All files processed successfully.\")\n",
    "    else:\n",
    "        logging.info(\"No files found for the specified period.\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    year = int(input(\"Enter year: \"))\n",
    "    month = int(input(\"Enter month: \"))\n",
    "    day = input(\"Enter day (optional): \")\n",
    "    day = int(day) if day else None\n",
    "    main(year, month, day)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee864b03-35e6-458c-a3ef-3a7c426d660f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
